"""
Sentiment Analyzer
æƒ…ç»ªåˆ†æå™¨ï¼Œç”¨äºåˆ†æå¸‚åœºæƒ…ç»ªå’Œç¤¾äº¤åª’ä½“æ•°æ®
æ”¯æŒå¤šç§æ•°æ®æºå’Œæƒ…ç»ªè®¡ç®—æ–¹æ³•
"""

import re
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from collections import deque, defaultdict, Counter
from dataclasses import dataclass, field
from enum import Enum
import sys
from pathlib import Path
import json
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger
from config.settings import config

logger = setup_logger(__name__)

class SentimentPolarity(Enum):
    """æƒ…ç»ªææ€§"""
    VERY_NEGATIVE = -2
    NEGATIVE = -1
    NEUTRAL = 0
    POSITIVE = 1
    VERY_POSITIVE = 2

class SourceType(Enum):
    """æ•°æ®æºç±»å‹"""
    TWITTER = "twitter"
    REDDIT = "reddit"
    NEWS = "news"
    FINANCIAL_BLOG = "financial_blog"
    FORUM = "forum"
    TELEGRAM = "telegram"
    DISCORD = "discord"

class DataFrequency(Enum):
    """æ•°æ®é¢‘ç‡"""
    REALTIME = "realtime"
    MINUTE = "minute"
    HOURLY = "hourly"
    DAILY = "daily"

@dataclass

class SentimentData:
    """æƒ…ç»ªæ•°æ®"""
    data_id: str
    content: str
    source: SourceType
    author: str
    published_at: datetime

    # æƒ…ç»ªåˆ†æç»“æœ
    sentiment_score: Optional[float] = None  # -1 åˆ° 1
    sentiment_polarity: Optional[SentimentPolarity] = None
    confidence: Optional[float] = None

    # å½±å“åŠ›æŒ‡æ ‡
    likes: int = 0
    shares: int = 0
    replies: int = 0
    views: int = 0
    influence_score: float = 0.0

    # æå–çš„ä¿¡æ¯
    mentioned_symbols: List[str] = field(default_factory=list)
    mentioned_keywords: List[str] = field(default_factory=list)
    hashtags: List[str] = field(default_factory=list)

    # è¯­è¨€ç‰¹å¾
    word_count: int = 0
    emoji_count: int = 0
    exclamation_count: int = 0

    def to_dict(self) -> dict:
        return {
            'data_id': self.data_id,
            'content': self.content[:200] + '...' if len(self.content) > 200 else self.content,
            'source': self.source.value,
            'author': self.author,
            'published_at': self.published_at.isoformat(),
            'sentiment_score': self.sentiment_score,
            'sentiment_polarity': self.sentiment_polarity.value if self.sentiment_polarity else None,
            'confidence': self.confidence,
            'likes': self.likes,
            'shares': self.shares,
            'replies': self.replies,
            'views': self.views,
            'influence_score': self.influence_score,
            'mentioned_symbols': self.mentioned_symbols,
            'mentioned_keywords': self.mentioned_keywords,
            'hashtags': self.hashtags,
            'word_count': self.word_count,
            'emoji_count': self.emoji_count,
            'exclamation_count': self.exclamation_count
        }

@dataclass

class AggregatedSentiment:
    """èšåˆæƒ…ç»ªæ•°æ®"""
    symbol: str
    time_window: datetime
    frequency: DataFrequency

    # èšåˆæŒ‡æ ‡
    total_mentions: int = 0
    average_sentiment: float = 0.0
    sentiment_std: float = 0.0
    bullish_count: int = 0
    bearish_count: int = 0
    neutral_count: int = 0

    # å½±å“åŠ›åŠ æƒæŒ‡æ ‡
    weighted_sentiment: float = 0.0
    total_influence: float = 0.0

    # æ¥æºåˆ†å¸ƒ
    source_distribution: Dict[str, int] = field(default_factory=dict)

    # æƒ…ç»ªå¼ºåº¦
    sentiment_intensity: float = 0.0  # æƒ…ç»ªæ³¢åŠ¨ç¨‹åº¦
    sentiment_momentum: float = 0.0   # æƒ…ç»ªå˜åŒ–è¶‹åŠ¿

    def to_dict(self) -> dict:
        return {
            'symbol': self.symbol,
            'time_window': self.time_window.isoformat(),
            'frequency': self.frequency.value,
            'total_mentions': self.total_mentions,
            'average_sentiment': self.average_sentiment,
            'sentiment_std': self.sentiment_std,
            'bullish_count': self.bullish_count,
            'bearish_count': self.bearish_count,
            'neutral_count': self.neutral_count,
            'weighted_sentiment': self.weighted_sentiment,
            'total_influence': self.total_influence,
            'source_distribution': self.source_distribution,
            'sentiment_intensity': self.sentiment_intensity,
            'sentiment_momentum': self.sentiment_momentum
        }

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""

    def __init__(self):
        # è‚¡ç¥¨ç¬¦å·æ¨¡å¼
        self.stock_pattern = re.compile(r'\$([A-Z]{1,5})\b')

        # æƒ…ç»ªè¯å…¸
        self.positive_words = {
            'excellent', 'amazing', 'fantastic', 'great', 'good', 'positive', 'up', 'rise', 'bull', 'bullish',
            'gain', 'profit', 'win', 'success', 'strong', 'buy', 'long', 'moon', 'rocket', 'ğŸš€', 'ğŸ“ˆ',
            'love', 'like', 'awesome', 'incredible', 'outstanding', 'boom', 'surge', 'soar', 'climb'
        }

        self.negative_words = {
            'terrible', 'awful', 'bad', 'negative', 'down', 'fall', 'bear', 'bearish', 'loss', 'lose',
            'fail', 'failure', 'weak', 'sell', 'short', 'crash', 'dump', 'drop', 'ğŸ“‰', 'ğŸ’©',
            'hate', 'dislike', 'horrible', 'disappointing', 'decline', 'plunge', 'collapse', 'tank'
        }

        # å¼ºåŒ–è¯
        self.intensifiers = {
            'very', 'extremely', 'really', 'super', 'absolutely', 'completely', 'totally',
            'incredibly', 'amazingly', 'highly', 'massively', 'hugely'
        }

        # è¡¨æƒ…ç¬¦å·æ˜ å°„
        self.emoji_sentiment = {
            'ğŸ˜€': 1, 'ğŸ˜ƒ': 1, 'ğŸ˜„': 1, 'ğŸ˜': 1, 'ğŸ˜†': 1, 'ğŸ˜Š': 1, 'â˜ºï¸': 1, 'ğŸ™‚': 0.5,
            'ğŸ˜': 2, 'ğŸ¤©': 2, 'ğŸ˜': 1, 'ğŸ¥³': 2, 'ğŸ‰': 1, 'ğŸ‘': 1, 'ğŸ’ª': 1, 'ğŸ”¥': 1,
            'ğŸ˜¢': -1, 'ğŸ˜­': -2, 'ğŸ˜ ': -2, 'ğŸ˜¡': -2, 'ğŸ¤¬': -2, 'ğŸ˜¤': -1, 'ğŸ‘': -1,
            'ğŸ’€': -1, 'ğŸ’©': -2, 'ğŸ¤¡': -1, 'ğŸ˜´': -0.5
        }

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # ç§»é™¤@mentions (ä¿ç•™å†…å®¹ç”¨äºåˆ†æä½†ä¸å½±å“æƒ…ç»ª)
        text = re.sub(r'@\w+', '', text)

        # æ ‡å‡†åŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def extract_features(self, text: str) -> Dict[str, Any]:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        text_lower = text.lower()

        features = {
            'word_count': len(text.split()),
            'char_count': len(text),
            'emoji_count': len([c for c in text if c in self.emoji_sentiment]),
            'exclamation_count': text.count('!'),
            'question_count': text.count('?'),
            'capital_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),
            'mentioned_symbols': self.extract_stock_symbols(text),
            'hashtags': self.extract_hashtags(text)
        }

        return features

    def extract_stock_symbols(self, text: str) -> List[str]:
        """æå–è‚¡ç¥¨ä»£ç """
        matches = self.stock_pattern.findall(text.upper())
        return list(set(matches))  # å»é‡

    def extract_hashtags(self, text: str) -> List[str]:
        """æå–hashtags"""
        hashtag_pattern = re.compile(r'  # (\w+)')
        matches = hashtag_pattern.findall(text.lower())
        return list(set(matches))  # å»é‡

    def calculate_lexicon_sentiment(self, text: str) -> Tuple[float, float]:
        """åŸºäºè¯å…¸è®¡ç®—æƒ…ç»ª"""
        words = re.findall(r'\b\w+\b', text.lower())

        positive_score = 0
        negative_score = 0
        intensifier_multiplier = 1.0

        for i, word in enumerate(words):
            # æ£€æŸ¥å¼ºåŒ–è¯
            if word in self.intensifiers:
                intensifier_multiplier = 1.5
                continue

            # è®¡ç®—æƒ…ç»ªå¾—åˆ†
            if word in self.positive_words:
                positive_score += 1 * intensifier_multiplier
            elif word in self.negative_words:
                negative_score += 1 * intensifier_multiplier

            # é‡ç½®å¼ºåŒ–å€æ•°
            intensifier_multiplier = 1.0

        # è®¡ç®—è¡¨æƒ…ç¬¦å·æƒ…ç»ª
        emoji_score = sum(self.emoji_sentiment.get(c, 0) for c in text)

        # ç»¼åˆå¾—åˆ†
        total_positive = positive_score + max(emoji_score, 0)
        total_negative = negative_score + abs(min(emoji_score, 0))

        if total_positive + total_negative == 0:
            return 0.0, 0.0  # ä¸­æ€§

        # å½’ä¸€åŒ–åˆ°[-1, 1]
        sentiment = (total_positive - total_negative) / (total_positive + total_negative)
        confidence = min((total_positive + total_negative) / 10.0, 1.0)  # åŸºäºè¯æ±‡å¯†åº¦çš„ç½®ä¿¡åº¦

        return sentiment, confidence

class MockDataSource:
    """æ¨¡æ‹Ÿæ•°æ®æº"""

    def __init__(self, source_type: SourceType):
        self.source_type = source_type

        # æ¨¡æ‹Ÿæ•°æ®æ¨¡æ¿
        self.sample_tweets = [
            ("BullishTrader", "ğŸš€ $AAPL is going to the moon! Amazing earnings report!  # bullish  # apple"),
            ("MarketAnalyst", "$TSLA looks weak today. Might be time to take profits. ğŸ“‰"),
            ("InvestorJoe", "Just bought more $MSFT. Love this dip! ğŸ’ª  # longterm"),
            ("TechFan2023", "$NVDA crushing it with AI revolution! This is just the beginning ğŸ”¥"),
            ("BearishBob", "Market crash incoming? Everything looks overvalued right now ğŸ˜°"),
            ("CryptoKing", "Bitcoin to 100k! All altcoins following. Bull run confirmed! ğŸŒ™"),
            ("StockGuru", "Dividend stocks are the way to go in this market. $KO $PEP steady gains"),
            ("RetailTrader", "Lost money on $GME again... When will I learn? ğŸ˜­"),
            ("OptionsPlayer", "Calls on $SPY printing money! Easy 200% gains today ğŸ“ˆ"),
            ("ValueInvestor", "$BRK.B undervalued as always. Buffett knows what he's doing")
        ]

    async def fetch_data(self, symbols: List[str] = None, limit: int = 50) -> List[SentimentData]:
        """è·å–æ¨¡æ‹Ÿæ•°æ®"""
        data = []

        for i in range(min(limit, len(self.sample_tweets))):
            author, content = self.sample_tweets[i % len(self.sample_tweets)]

            # æ·»åŠ æ—¶é—´æˆ³å˜åŒ–
            timestamp = datetime.now() - timedelta(minutes=np.random.randint(1, 1440))

            sentiment_data = SentimentData(
                data_id=f"{self.source_type.value}_{i}_{int(timestamp.timestamp())}",
                content=content,
                source=self.source_type,
                author=f"{author}_{np.random.randint(1000, 9999)}",
                published_at=timestamp,
                likes=np.random.randint(0, 1000),
                shares=np.random.randint(0, 100),
                replies=np.random.randint(0, 50),
                views=np.random.randint(100, 10000)
            )

            data.append(sentiment_data)

        return data

class SentimentCalculator:
    """æƒ…ç»ªè®¡ç®—å™¨"""

    def __init__(self):
        self.preprocessor = TextPreprocessor()

    def analyze_sentiment(self, data: SentimentData) -> SentimentData:
        """åˆ†æå•æ¡æ•°æ®çš„æƒ…ç»ª"""
        # æ¸…ç†æ–‡æœ¬
        clean_content = self.preprocessor.clean_text(data.content)

        # æå–ç‰¹å¾
        features = self.preprocessor.extract_features(data.content)

        # æ›´æ–°æ•°æ®ç‰¹å¾
        data.word_count = features['word_count']
        data.emoji_count = features['emoji_count']
        data.exclamation_count = features['exclamation_count']
        data.mentioned_symbols = features['mentioned_symbols']
        data.hashtags = features['hashtags']

        # è®¡ç®—æƒ…ç»ª
        sentiment_score, confidence = self.preprocessor.calculate_lexicon_sentiment(clean_content)

        data.sentiment_score = sentiment_score
        data.confidence = confidence

        # è®¾ç½®æƒ…ç»ªææ€§
        if sentiment_score >= 0.5:
            data.sentiment_polarity = SentimentPolarity.VERY_POSITIVE
        elif sentiment_score >= 0.1:
            data.sentiment_polarity = SentimentPolarity.POSITIVE
        elif sentiment_score <= -0.5:
            data.sentiment_polarity = SentimentPolarity.VERY_NEGATIVE
        elif sentiment_score <= -0.1:
            data.sentiment_polarity = SentimentPolarity.NEGATIVE
        else:
            data.sentiment_polarity = SentimentPolarity.NEUTRAL

        # è®¡ç®—å½±å“åŠ›å¾—åˆ†
        data.influence_score = self._calculate_influence_score(data)

        return data

    def _calculate_influence_score(self, data: SentimentData) -> float:
        """è®¡ç®—å½±å“åŠ›å¾—åˆ†"""
        # åŸºäºäº’åŠ¨æŒ‡æ ‡è®¡ç®—å½±å“åŠ›
        interaction_score = (
            data.likes * 1.0 +
            data.shares * 2.0 +  # åˆ†äº«æƒé‡æ›´é«˜
            data.replies * 1.5 +
            data.views * 0.01    # æµè§ˆé‡æƒé‡è¾ƒä½
        )

        # åŸºäºå†…å®¹è´¨é‡è°ƒæ•´
        content_quality = min(data.word_count / 20.0, 2.0)  # é€‚åº¦çš„å†…å®¹é•¿åº¦

        # åŸºäºæƒ…ç»ªå¼ºåº¦è°ƒæ•´
        sentiment_intensity = abs(data.sentiment_score or 0) * 2

        # ç»¼åˆå½±å“åŠ›å¾—åˆ†
        influence_score = (interaction_score * 0.7 +
                          content_quality * 0.2 +
                          sentiment_intensity * 0.1)

        # å½’ä¸€åŒ–
        return min(influence_score / 1000.0, 10.0)

    def aggregate_sentiment(self, data_list: List[SentimentData],
                          symbol: str, time_window: datetime,
                          frequency: DataFrequency) -> AggregatedSentiment:
        """èšåˆæƒ…ç»ªæ•°æ®"""
        if not data_list:
            return AggregatedSentiment(symbol, time_window, frequency)

        # è¿‡æ»¤ç›¸å…³æ•°æ®
        relevant_data = [
            d for d in data_list
            if symbol.upper() in [s.upper() for s in d.mentioned_symbols]
        ]

        if not relevant_data:
            return AggregatedSentiment(symbol, time_window, frequency)

        # åŸºç¡€ç»Ÿè®¡
        sentiments = [d.sentiment_score for d in relevant_data if d.sentiment_score is not None]
        influences = [d.influence_score for d in relevant_data]

        aggregated = AggregatedSentiment(symbol, time_window, frequency)
        aggregated.total_mentions = len(relevant_data)

        if sentiments:
            aggregated.average_sentiment = np.mean(sentiments)
            aggregated.sentiment_std = np.std(sentiments)

            # å½±å“åŠ›åŠ æƒæƒ…ç»ª
            if influences and sum(influences) > 0:
                weighted_sentiment = sum(s * i for s, i in zip(sentiments, influences))
                aggregated.weighted_sentiment = weighted_sentiment / sum(influences)
                aggregated.total_influence = sum(influences)
            else:
                aggregated.weighted_sentiment = aggregated.average_sentiment

        # æƒ…ç»ªåˆ†å¸ƒç»Ÿè®¡
        for data in relevant_data:
            if data.sentiment_polarity:
                if data.sentiment_polarity in [SentimentPolarity.POSITIVE, SentimentPolarity.VERY_POSITIVE]:
                    aggregated.bullish_count += 1
                elif data.sentiment_polarity in [SentimentPolarity.NEGATIVE, SentimentPolarity.VERY_NEGATIVE]:
                    aggregated.bearish_count += 1
                else:
                    aggregated.neutral_count += 1

        # æ¥æºåˆ†å¸ƒ
        source_counter = Counter(d.source.value for d in relevant_data)
        aggregated.source_distribution = dict(source_counter)

        # æƒ…ç»ªå¼ºåº¦ (æ ‡å‡†å·®ä½œä¸ºæ³¢åŠ¨æ€§æŒ‡æ ‡)
        aggregated.sentiment_intensity = aggregated.sentiment_std

        return aggregated

class SentimentAnalyzer:
    """æƒ…ç»ªåˆ†æå™¨ä¸»ç±»"""

    def __init__(self):
        self.data_sources = {}
        self.sentiment_calculator = SentimentCalculator()

        # æ·»åŠ é»˜è®¤æ•°æ®æº
        self.add_data_source("twitter", MockDataSource(SourceType.TWITTER))
        self.add_data_source("reddit", MockDataSource(SourceType.REDDIT))

        # æ•°æ®ç¼“å­˜
        self.raw_data_cache = deque(maxlen=10000)
        self.aggregated_cache = defaultdict(lambda: deque(maxlen=1000))

        # å®æ—¶æµ
        self.subscribers = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_data_processed': 0,
            'total_symbols_tracked': 0,
            'average_sentiment': 0.0,
            'data_by_source': defaultdict(int),
            'sentiment_distribution': defaultdict(int),
            'processing_errors': 0
        }

        # ä»»åŠ¡ç®¡ç†
        self.is_running = False
        self.processing_task = None

        logger.info("æƒ…ç»ªåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def add_data_source(self, name: str, source):
        """æ·»åŠ æ•°æ®æº"""
        self.data_sources[name] = source
        logger.info(f"æ·»åŠ æ•°æ®æº: {name}")

    async def start(self):
        """å¯åŠ¨åˆ†æå™¨"""
        if self.is_running:
            return

        self.is_running = True

        # å¯åŠ¨å®šæœŸå¤„ç†ä»»åŠ¡
        self.processing_task = asyncio.create_task(self._processing_loop())

        logger.info("æƒ…ç»ªåˆ†æå™¨å¯åŠ¨å®Œæˆ")

    async def stop(self):
        """åœæ­¢åˆ†æå™¨"""
        if not self.is_running:
            return

        logger.info("æ­£åœ¨åœæ­¢æƒ…ç»ªåˆ†æå™¨...")
        self.is_running = False

        if self.processing_task:
            self.processing_task.cancel()

        logger.info("æƒ…ç»ªåˆ†æå™¨å·²åœæ­¢")

    async def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.is_running:
            try:
                await self.collect_and_analyze_data()
                await asyncio.sleep(300)  # 5åˆ†é’Ÿå¤„ç†ä¸€æ¬¡
            except Exception as e:
                logger.error(f"å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(60)

    async def collect_and_analyze_data(self, symbols: List[str] = None,
                                     limit: int = 100) -> List[SentimentData]:
        """æ”¶é›†å¹¶åˆ†ææ•°æ®"""
        all_data = []

        for source_name, source in self.data_sources.items():
            try:
                # è·å–æ•°æ®
                raw_data = await source.fetch_data(symbols, limit)

                # åˆ†ææƒ…ç»ª
                analyzed_data = []
                for data in raw_data:
                    try:
                        analyzed = self.sentiment_calculator.analyze_sentiment(data)
                        analyzed_data.append(analyzed)

                        # ç¼“å­˜æ•°æ®
                        self.raw_data_cache.append(analyzed)

                    except Exception as e:
                        logger.error(f"åˆ†ææ•°æ®å¤±è´¥ {data.data_id}: {e}")
                        self.stats['processing_errors'] += 1

                all_data.extend(analyzed_data)

                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_data_processed'] += len(analyzed_data)
                self.stats['data_by_source'][source_name] += len(analyzed_data)

                logger.info(f"ä» {source_name} å¤„ç†äº† {len(analyzed_data)} æ¡æ•°æ®")

            except Exception as e:
                logger.error(f"ä»æ•°æ®æº {source_name} è·å–æ•°æ®å¤±è´¥: {e}")

        # é€šçŸ¥è®¢é˜…è€…
        if all_data:
            await self._notify_subscribers(all_data)

        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self._update_global_stats(all_data)

        return all_data

    async def _notify_subscribers(self, data_list: List[SentimentData]):
        """é€šçŸ¥è®¢é˜…è€…"""
        for callback in self.subscribers:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(data_list)
                else:
                    callback(data_list)
            except Exception as e:
                logger.error(f"é€šçŸ¥è®¢é˜…è€…å¤±è´¥: {e}")

    def _update_global_stats(self, data_list: List[SentimentData]):
        """æ›´æ–°å…¨å±€ç»Ÿè®¡"""
        if not data_list:
            return

        sentiments = [d.sentiment_score for d in data_list if d.sentiment_score is not None]

        if sentiments:
            # æ›´æ–°å¹³å‡æƒ…ç»ª
            current_avg = self.stats['average_sentiment']
            current_count = self.stats['total_data_processed'] - len(sentiments)

            total_sentiment = current_avg * current_count + sum(sentiments)
            self.stats['average_sentiment'] = total_sentiment / self.stats['total_data_processed']

        # æ›´æ–°æƒ…ç»ªåˆ†å¸ƒ
        for data in data_list:
            if data.sentiment_polarity:
                self.stats['sentiment_distribution'][data.sentiment_polarity.value] += 1

        # æ›´æ–°è¿½è¸ªçš„è‚¡ç¥¨æ•°é‡
        all_symbols = set()
        for data in data_list:
            all_symbols.update(data.mentioned_symbols)

        self.stats['total_symbols_tracked'] = len(all_symbols)

    def get_sentiment_summary(self, symbol: str, hours_back: int = 24) -> Dict[str, Any]:
        """è·å–æƒ…ç»ªæ‘˜è¦"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        # è¿‡æ»¤ç›¸å…³æ•°æ®
        relevant_data = []
        for data in self.raw_data_cache:
            if (data.published_at >= cutoff_time and
                symbol.upper() in [s.upper() for s in data.mentioned_symbols]):
                relevant_data.append(data)

        if not relevant_data:
            return {
                'symbol': symbol,
                'total_mentions': 0,
                'average_sentiment': 0.0,
                'bullish_ratio': 0.0,
                'bearish_ratio': 0.0,
                'neutral_ratio': 0.0,
                'sentiment_trend': 'neutral',
                'hours_back': hours_back
            }

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        sentiments = [d.sentiment_score for d in relevant_data if d.sentiment_score is not None]

        bullish_count = sum(1 for d in relevant_data
                           if d.sentiment_polarity in [SentimentPolarity.POSITIVE,
                                                      SentimentPolarity.VERY_POSITIVE])
        bearish_count = sum(1 for d in relevant_data
                           if d.sentiment_polarity in [SentimentPolarity.NEGATIVE,
                                                      SentimentPolarity.VERY_NEGATIVE])
        neutral_count = len(relevant_data) - bullish_count - bearish_count

        total_count = len(relevant_data)

        # è®¡ç®—è¶‹åŠ¿
        recent_data = sorted(relevant_data, key=lambda x: x.published_at)[-10:]  # æœ€è¿‘10æ¡
        recent_sentiments = [d.sentiment_score for d in recent_data if d.sentiment_score is not None]

        if len(recent_sentiments) >= 2:
            trend_slope = np.polyfit(range(len(recent_sentiments)), recent_sentiments, 1)[0]
            if trend_slope > 0.1:
                sentiment_trend = 'improving'
            elif trend_slope < -0.1:
                sentiment_trend = 'declining'
            else:
                sentiment_trend = 'stable'
        else:
            sentiment_trend = 'neutral'

        return {
            'symbol': symbol,
            'total_mentions': total_count,
            'average_sentiment': np.mean(sentiments) if sentiments else 0.0,
            'sentiment_std': np.std(sentiments) if sentiments else 0.0,
            'bullish_ratio': bullish_count / total_count,
            'bearish_ratio': bearish_count / total_count,
            'neutral_ratio': neutral_count / total_count,
            'sentiment_trend': sentiment_trend,
            'top_sources': dict(Counter(d.source.value for d in relevant_data).most_common(3)),
            'hours_back': hours_back,
            'data_points': len(relevant_data)
        }

    def get_trending_symbols(self, limit: int = 10,
                           hours_back: int = 24) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨è‚¡ç¥¨"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        # ç»Ÿè®¡è‚¡ç¥¨æåŠæ¬¡æ•°
        symbol_mentions = defaultdict(list)

        for data in self.raw_data_cache:
            if data.published_at >= cutoff_time:
                for symbol in data.mentioned_symbols:
                    symbol_mentions[symbol.upper()].append(data)

        # è®¡ç®—çƒ­é—¨è‚¡ç¥¨
        trending_symbols = []

        for symbol, data_list in symbol_mentions.items():
            if len(data_list) >= 5:  # è‡³å°‘5æ¬¡æåŠ
                sentiments = [d.sentiment_score for d in data_list if d.sentiment_score is not None]
                influences = [d.influence_score for d in data_list]

                trending_score = (
                    len(data_list) * 1.0 +  # æåŠé¢‘ç‡
                    sum(influences) * 2.0 +  # å½±å“åŠ›æ€»å’Œ
                    abs(np.mean(sentiments)) * 50 if sentiments else 0  # æƒ…ç»ªå¼ºåº¦
                )

                trending_symbols.append({
                    'symbol': symbol,
                    'mentions': len(data_list),
                    'average_sentiment': np.mean(sentiments) if sentiments else 0.0,
                    'total_influence': sum(influences),
                    'trending_score': trending_score,
                    'sentiment_polarity': 'bullish' if np.mean(sentiments) > 0.1 else
                                        'bearish' if np.mean(sentiments) < -0.1 else 'neutral'
                })

        # æŒ‰çƒ­é—¨å¾—åˆ†æ’åº
        trending_symbols.sort(key=lambda x: x['trending_score'], reverse=True)

        return trending_symbols[:limit]

    def subscribe_to_sentiment(self, callback: Callable):
        """è®¢é˜…æƒ…ç»ªæ•°æ®"""
        self.subscribers.append(callback)
        logger.info("æ–°å¢æƒ…ç»ªæ•°æ®è®¢é˜…è€…")

    def unsubscribe_from_sentiment(self, callback: Callable):
        """å–æ¶ˆè®¢é˜…æƒ…ç»ªæ•°æ®"""
        if callback in self.subscribers:
            self.subscribers.remove(callback)
            logger.info("ç§»é™¤æƒ…ç»ªæ•°æ®è®¢é˜…è€…")

    async def analyze_custom_text(self, text: str, author: str = "custom") -> SentimentData:
        """åˆ†æè‡ªå®šä¹‰æ–‡æœ¬"""
        # åˆ›å»ºä¸´æ—¶æ•°æ®å¯¹è±¡
        data = SentimentData(
            data_id=f"custom_{int(datetime.now().timestamp())}",
            content=text,
            source=SourceType.NEWS,  # é»˜è®¤æ¥æº
            author=author,
            published_at=datetime.now()
        )

        return self.sentiment_calculator.analyze_sentiment(data)

    def get_historical_aggregation(self, symbol: str,
                                 frequency: DataFrequency = DataFrequency.HOURLY,
                                 days_back: int = 7) -> List[AggregatedSentiment]:
        """è·å–å†å²èšåˆæ•°æ®"""
        cutoff_time = datetime.now() - timedelta(days=days_back)

        # è¿‡æ»¤æ•°æ®
        relevant_data = []
        for data in self.raw_data_cache:
            if (data.published_at >= cutoff_time and
                symbol.upper() in [s.upper() for s in data.mentioned_symbols]):
                relevant_data.append(data)

        if not relevant_data:
            return []

        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
        if frequency == DataFrequency.HOURLY:
            window_size = timedelta(hours=1)
        elif frequency == DataFrequency.DAILY:
            window_size = timedelta(days=1)
        else:
            window_size = timedelta(hours=1)  # é»˜è®¤

        # åˆ›å»ºæ—¶é—´çª—å£
        start_time = cutoff_time
        end_time = datetime.now()

        aggregated_data = []
        current_time = start_time

        while current_time < end_time:
            window_end = current_time + window_size

            # è·å–çª—å£å†…çš„æ•°æ®
            window_data = [
                d for d in relevant_data
                if current_time <= d.published_at < window_end
            ]

            if window_data:
                aggregated = self.sentiment_calculator.aggregate_sentiment(
                    window_data, symbol, current_time, frequency
                )
                aggregated_data.append(aggregated)

            current_time = window_end

        return aggregated_data

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'is_running': self.is_running,
            'data_sources_count': len(self.data_sources),
            'cached_data_points': len(self.raw_data_cache),
            'active_subscribers': len(self.subscribers),
            'stats': self.stats,
            'cache_stats': {
                'raw_data_cache_size': len(self.raw_data_cache),
                'aggregated_cache_keys': len(self.aggregated_cache)
            }
        }

# å…¨å±€å®ä¾‹
_sentiment_analyzer_instance = None

def get_sentiment_analyzer() -> SentimentAnalyzer:
    """è·å–æƒ…ç»ªåˆ†æå™¨å®ä¾‹"""
    global _sentiment_analyzer_instance
    if _sentiment_analyzer_instance is None:
        _sentiment_analyzer_instance = SentimentAnalyzer()
    return _sentiment_analyzer_instance
