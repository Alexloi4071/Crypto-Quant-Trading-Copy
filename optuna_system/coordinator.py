# -*- coding: utf-8 -*-
"""
Optunaç³»çµ±ä¸»å”èª¿å™¨
çµ±ä¸€ç®¡ç†Layer0+9å±¤å®Œæ•´å„ªåŒ–æ¶æ§‹çš„åŸ·è¡Œå’Œçµæœç®¡ç†
æ”¯æŒ: Layer0æ•¸æ“šæ¸…æ´— + Layer1-4æ ¸å¿ƒå±¤ + Layer5-8å°ˆé …å±¤
"""
import json
import logging
import sys
import os
import time
import traceback
from hashlib import md5
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from optuna_system.utils.io_utils import write_dataframe, read_dataframe, compute_file_md5, atomic_write_json
from optuna_system.utils.logging_utils import setup_logging

# ğŸ†• é˜¶æ®µ6ï¼šç”Ÿå­˜è€…åå·®æ ¡æ­£ï¼ˆå¯é€‰ï¼‰
try:
    from optuna_system.utils.survivorship_bias import apply_survivorship_correction
    HAS_SURVIVORSHIP_CORRECTION = True
except ImportError:
    HAS_SURVIVORSHIP_CORRECTION = False
    print("âš ï¸ survivorship_biasæ¨¡å—ä¸å¯ç”¨ï¼Œç”Ÿå­˜è€…åå·®æ ¡æ­£å°†è·³è¿‡")

# ğŸ†• é˜¶æ®µ7ï¼šç³»ç»Ÿæ€§åå·®+å¯è§£é‡Šæ€§ï¼ˆå¯é€‰ï¼‰
try:
    from optuna_system.utils.adversarial_validation import quick_adversarial_check
    HAS_ADVERSARIAL_VALIDATION = True
except ImportError:
    HAS_ADVERSARIAL_VALIDATION = False
    print("â„¹ï¸ adversarial_validationæ¨¡å—æœªå¯¼å…¥")

try:
    from optuna_system.utils.random_benchmark import RandomBenchmarkTester
    HAS_RANDOM_BENCHMARK = True
except ImportError:
    HAS_RANDOM_BENCHMARK = False
    print("â„¹ï¸ random_benchmarkæ¨¡å—æœªå¯¼å…¥")

try:
    from optuna_system.utils.model_interpretability import ModelInterpreter
    HAS_MODEL_INTERPRETABILITY = True
except ImportError:
    HAS_MODEL_INTERPRETABILITY = False
    print("â„¹ï¸ model_interpretabilityæ¨¡å—æœªå¯¼å…¥")

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘
current_dir = Path(__file__).parent
project_root = current_dir.parent  # é¡¹ç›®æ ¹ç›®å½•
sys.path.append(str(current_dir))
sys.path.append(str(current_dir / "optimizers"))
sys.path.append(str(project_root))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•ï¼Œä»¥ä¾¿å¯¼å…¥srcæ¨¡å—

try:
    # Layer0-2: æ ¸å¿ƒå±¤ï¼ˆå¿…éœ€ï¼‰
    from optimizers.optuna_cleaning import DataCleaningOptimizer
    from optimizers.optuna_feature import FeatureOptimizer
    from optimizers.optuna_meta_label import MetaLabelOptimizer  # Meta-Labeling é›™å±¤æ¶æ§‹
    from config.timeframe_scaler import TimeFrameScaler
    from version_manager import OptunaVersionManager

    # Layer3-8: å…¶ä»–å±¤ï¼ˆå¯é¸ï¼Œå¦‚æœå­˜åœ¨å‰‡å°å…¥ï¼‰
    try:
        from optimizers.optuna_model import ModelOptimizer
        HAS_MODEL_OPTIMIZER = True
    except ImportError:
        HAS_MODEL_OPTIMIZER = False
        print("âš ï¸ optuna_modelä¸å¯ç”¨ï¼ŒLayer3å°‡è·³é")

    try:
        from optimizers.optuna_cv_risk import CVRiskOptimizer
        HAS_CV_RISK_OPTIMIZER = True
    except ImportError:
        HAS_CV_RISK_OPTIMIZER = False
        print("âš ï¸ optuna_cv_riskä¸å¯ç”¨ï¼ŒLayer4å°‡è·³é")
    
    try:
        from optimizers.kelly_optimizer import KellyOptimizer
        HAS_KELLY_OPTIMIZER = True
    except ImportError:
        HAS_KELLY_OPTIMIZER = False
        print("âš ï¸ kelly_optimizerä¸å¯ç”¨ï¼ŒLayer5å°‡è·³é")

    try:
        from optimizers.ensemble_optimizer import EnsembleOptimizer
        HAS_ENSEMBLE_OPTIMIZER = True
    except ImportError:
        HAS_ENSEMBLE_OPTIMIZER = False
        print("âš ï¸ ensemble_optimizerä¸å¯ç”¨ï¼ŒLayer6å°‡è·³é")

    try:
        from optimizers.polynomial_optimizer import PolynomialOptimizer
        HAS_POLYNOMIAL_OPTIMIZER = True
    except ImportError:
        HAS_POLYNOMIAL_OPTIMIZER = False
        print("âš ï¸ polynomial_optimizerä¸å¯ç”¨ï¼ŒLayer7å°‡è·³é")

    try:
        from optimizers.confidence_optimizer import ConfidenceOptimizer
        HAS_CONFIDENCE_OPTIMIZER = True
    except ImportError:
        HAS_CONFIDENCE_OPTIMIZER = False
        print("âš ï¸ confidence_optimizerä¸å¯ç”¨ï¼ŒLayer8å°‡è·³é")

except ImportError as e:
    print(f"âš ï¸ æ ¸å¿ƒæ¨¡å¡Šå°å…¥å¤±æ•—: {e}")
    print("è«‹æª¢æŸ¥version_manager.pyå’Œæ ¸å¿ƒå„ªåŒ–å™¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨")


class OptunaCoordinator:
    """Layer0+9å±¤å®Œæ•´å„ªåŒ–ç³»çµ±å”èª¿å™¨ï¼ˆæ”¯æŒå¤šæ™‚é–“æ¡†æ¶è‡ªå‹•ç¸®æ”¾ï¼‰"""
    
    def __init__(self, symbol: str = "BTCUSDT", timeframe: str = "15m", 
                 data_path: str = "data", scaled_config: Dict = None, version: str = None):
        self.symbol = symbol
        self.timeframe = timeframe
        self.data_path = Path(data_path)

        # çµ±ä¸€æ—¥èªŒåˆå§‹åŒ–
        self.logger = setup_logging(name=__name__)

        # æ™‚æ¡†ç¸®æ”¾é…ç½®ï¼ˆå¯ç”±MultiTimeframeCoordinatoræä¾›ï¼‰
        self.scaled_config = scaled_config or {}
        self.scaler = TimeFrameScaler(self.logger)
        
        # è¨­ç½®è·¯å¾‘
        self.optuna_root = Path(__file__).parent
        self.configs_path = self.optuna_root / "configs"
        self.results_path = self.optuna_root / "results"
        # ç‰©åŒ–è¼¸å‡ºæ”¹ç‚º data/processed
        self.processed_root = self.data_path / "processed"
        self.materialized_path = self.processed_root  # å…¼å®¹èˆŠå‘½å
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.configs_path.mkdir(exist_ok=True)
        self.results_path.mkdir(exist_ok=True)
        self.processed_root.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç‰ˆæœ¬ç®¡ç†å™¨
        self.version_manager = OptunaVersionManager(str(self.results_path))
        
        # ç•¶å‰ç‰ˆæœ¬
        self.current_version = version or self.version_manager.create_new_version()
        
        # å„ªåŒ–çµæœå­˜å„²
        self.layer_results: Dict[str, Any] = {}  # ç”¨æ–¼å­˜å„²å„å±¤çµæœ
        
        self.logger.info("ğŸš€ ä¿®å¾©ç‰ˆLayer0+9å±¤å„ªåŒ–ç³»çµ±å”èª¿å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   äº¤æ˜“å°: {symbol}")
        self.logger.info(f"   æ™‚é–“æ¡†: {timeframe}")
        self.logger.info(f"   ç‰ˆæœ¬è™Ÿ: {self.current_version}")

        # åˆå§‹åŒ– meta study ä»¥ç²å–å¤šæ™‚æ¡†æ³¢å‹•è³‡è¨Š
        self._initialize_meta_study()

    def _initialize_meta_study(self) -> None:
        """æ§‹å»ºå¤šæ™‚æ¡†æ³¢å‹•è³‡è¨Šï¼ˆmeta_volã€global_volï¼‰ä¸¦æ³¨å…¥ scaled_config"""
        timeframes = self.scaled_config.get('meta_timeframes') or ['15m', '1h', '4h', '1d']
        global_vol = {}

        for tf in timeframes:
            try:
                ohlcv_file = self.data_path / "raw" / self.symbol / f"{self.symbol}_{tf}_ohlcv.parquet"
                if not ohlcv_file.exists():
                    # å›é€€ï¼šå¾ 15m é‡æ¡æ¨£
                    base_15m = self.data_path / "raw" / self.symbol / f"{self.symbol}_15m_ohlcv.parquet"
                    if base_15m.exists() and tf in {"1h", "4h", "1d"}:
                        try:
                            df15 = read_dataframe(base_15m)
                            rule_map = {"1h": "1H", "4h": "4H", "1d": "1D"}
                            rule = rule_map[tf]
                            
                            # ğŸ”§ P0ä¿®å¤ï¼šä¸¥æ ¼åŒé‡shifté˜²æ­¢æ—¶é—´æ³„æ¼
                            # é—®é¢˜ï¼šåŸä»£ç ç›´æ¥resampleï¼ŒåŒ…å«æœªæ¥æ•°æ®
                            # ä¿®å¤ï¼šä½¿ç”¨MultiTimeframeAlignerçš„ä¸¥æ ¼æ–¹æ³•
                            from optuna_system.utils.timeframe_alignment import MultiTimeframeAligner
                            
                            aligner = MultiTimeframeAligner('15m', [tf], strict_mode=True)
                            df = aligner.resample_ohlcv(df15, tf, method='double_shift')
                            
                            self.logger.info(
                                f"âœ… {tf} å›é€€é‡é‡‡æ ·è‡ª15mï¼ˆåŒé‡shifté˜²æ³„æ¼ï¼‰: {len(df)} è¡Œ"
                            )
                        except Exception as ie:
                            self.logger.warning(f"{tf} ç„¡æ³•é‡æ¡æ¨£: {ie}")
                            global_vol[tf] = 0.02
                            continue
                    else:
                        self.logger.warning(f"âš ï¸ {tf} è³‡æ–™ä¸å­˜åœ¨ä¸”ç„¡æ³•å›é€€: {ohlcv_file}")
                        global_vol[tf] = 0.02
                        continue
                else:
                    df = read_dataframe(ohlcv_file)

                returns = df['close'].pct_change().dropna()
                rolling_std = returns.rolling(window=100, min_periods=50).std()
                global_vol[tf] = float(rolling_std.mean()) if len(rolling_std) > 0 else 0.02
                self.logger.info(f"{tf} æ³¢å‹•è¨ˆç®—: std_mean={global_vol[tf]:.4f}")
            except Exception as e:
                # é™ç´šç‚º Warningï¼Œä¸¦å˜—è©¦ä»¥ 15m é‡æ¡æ¨£å›é€€
                self.logger.warning(f"âš ï¸ {tf} æ³¢å‹•è¨ˆç®—è®€å–å¤±æ•—ï¼Œå˜—è©¦å›é€€: {e}")
                try:
                    base_15m = self.data_path / "raw" / self.symbol / f"{self.symbol}_15m_ohlcv.parquet"
                    if base_15m.exists() and tf in {"1h", "4h", "1d"}:
                        df15 = read_dataframe(base_15m)
                        rule_map = {"1h": "1H", "4h": "4H", "1d": "1D"}
                        rule = rule_map[tf]
                        df = df15.resample(rule).agg({
                            'open': 'first',
                            'high': 'max',
                            'low': 'min',
                            'close': 'last',
                            'volume': 'sum'
                        }).dropna()
                        returns = df['close'].pct_change().dropna()
                        rolling_std = returns.rolling(window=100, min_periods=50).std()
                        global_vol[tf] = float(rolling_std.mean()) if len(rolling_std) > 0 else 0.02
                        self.logger.info(f"{tf} å›é€€é‡æ¡æ¨£è‡ª15m: std_mean={global_vol[tf]:.4f}")
                        continue
                except Exception as ie:
                    self.logger.warning(f"âš ï¸ {tf} å›é€€é‡æ¡æ¨£ä»å¤±æ•—: {ie}")
                global_vol[tf] = 0.02

        if not global_vol:
            meta_vol = 0.02
        else:
            meta_vol = float(np.mean(list(global_vol.values())))

        self.scaled_config.setdefault('global_vol', global_vol)
        self.scaled_config.setdefault('meta_vol', meta_vol)
        self.scaled_config.setdefault('meta_timeframes', timeframes)

        self.logger.info(
            f"Meta å¤§è…¦æº–å‚™å®Œæˆ: å¹³å‡æ³¢å‹•={meta_vol:.4f}, å„æ™‚æ¡†={ {tf: round(vol, 4) for tf, vol in global_vol.items()} }"
        )

    def _get_layer_json_path(self, layer_name: str) -> Path:
        mapping = {
            "layer0_cleaning": "cleaning_params",
            "layer1_labels": "label_params",
            "layer2_features": "feature_params",
        }
        base = mapping.get(layer_name)
        if not base:
            return self.configs_path
        tf_file = self.configs_path / f"{base}_{self.timeframe}.json"
        if tf_file.exists():
            return tf_file
        return self.configs_path / f"{base}.json"

    def get_cleaned_file_path(self, timeframe: str) -> Path:
        # å„ªå…ˆ processedï¼Œå…¶æ¬¡å›é€€åˆ°èˆŠä½ç½®
        processed = self.processed_root / "cleaned" / f"{self.symbol}_{timeframe}" / "cleaned_ohlcv.parquet"
        if processed.exists():
            return processed
        legacy = self.configs_path / f"cleaned_ohlcv_{timeframe}.parquet"
        return legacy

    def get_label_config_path(self, timeframe: str) -> Path:
        return self.configs_path / f"label_params_{timeframe}.json"

    def get_feature_config_path(self, timeframe: str) -> Path:
        return self.configs_path / f"feature_params_{timeframe}.json"
    
    def load_default_configs(self) -> Dict[str, Dict]:
        """åŠ è¼‰é»˜èªé…ç½®"""
        configs = {}
        
        config_files = {
            # Layer0-4: æ ¸å¿ƒå±¤é…ç½®
            'layer0_cleaning': 'cleaning_params.json',
            'layer1_label': 'label_params.json',
            'layer2_feature': 'feature_params.json',
            'layer3_model': 'model_params.json',
            'layer4_cv_risk': 'cv_risk_params.json',
            
            # Layer5-8: å°ˆé …å±¤é…ç½®
            'layer5_kelly': 'kelly_params.json',
            'layer6_ensemble': 'ensemble_params.json',
            'layer7_polynomial': 'polynomial_params.json',
            'layer8_confidence': 'confidence_params.json'
        }
        
        for module_name, filename in config_files.items():
            config_file = self.configs_path / filename
            if not config_file.exists():
                continue
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                if isinstance(config_data, dict):
                    # è¼•é‡æª¢æŸ¥ï¼šå¿…å‚™éµï¼ˆå¦‚ best_params / best_scoreï¼‰ç¼ºå¤±æ™‚ï¼Œè·³éé¿å…æ±¡æŸ“
                    if any(key in filename for key in ("label_params", "feature_params", "cleaning_params")):
                        if not ("best_params" in config_data and "best_score" in config_data):
                            self.logger.warning("âš ï¸ é…ç½®ç¼ºå°‘å¿…å‚™éµï¼Œè·³é: %s", filename)
                            continue
                    configs[module_name] = config_data
                    self.logger.info("âœ… åŠ è¼‰é…ç½®: %s", filename)
                else:
                    self.logger.warning("âš ï¸ é…ç½®å…§å®¹éå­—å…¸: %s", filename)
            except Exception as e:
                self.logger.warning("âš ï¸ åŠ è¼‰é…ç½®å¤±æ•— %s: %s", filename, e)
        
        return configs
    
    # ============================================================
    # Layer0ï¼šæ•¸æ“šæ¸…æ´—åŸºç¤å±¤
    # ============================================================
    
    # -------------------------------------------------
    # è³‡æ–™ç‰©åŒ–èˆ‡å¿«å–æ©Ÿåˆ¶
    # -------------------------------------------------

    def _generate_params_hash(self, params: Dict[str, Any]) -> str:
        """æ ¹æ“šæ’åºå¾Œçš„åƒæ•¸ç”ŸæˆMD5å“ˆå¸Œ"""
        serialized = json.dumps(params, sort_keys=True, ensure_ascii=False)
        return md5(serialized.encode("utf-8")).hexdigest()

    def _materialize_dir(self, layer_name: str) -> Path:
        """ä¾ layer åˆ†é…ç‰©åŒ–å­è³‡æ–™å¤¾ã€‚"""
        layer_to_sub = {
            "layer0_cleaning": "cleaned",
            "layer1_labels": "labels",
            "layer2_features": "features",
        }
        sub = layer_to_sub.get(layer_name, "misc")
        # å°‡è¼¸å‡ºå¯«å…¥ç‰ˆæœ¬å­ç›®éŒ„ï¼ˆVnï¼‰
        out_dir = self.processed_root / sub / f"{self.symbol}_{self.timeframe}" / str(self.current_version)
        out_dir.mkdir(parents=True, exist_ok=True)
        return out_dir

    def _materialize_stem(self, layer_name: str, params_hash: str) -> str:
        prefix = {
            "layer0_cleaning": "cleaned_ohlcv",
            "layer1_labels": "labels",
            "layer2_features": "features",
        }.get(layer_name, layer_name)
        return f"{prefix}_{self.symbol}_{self.timeframe}_{params_hash}"

    def _materialize_cache_path(self, layer_name: str, params_hash: str) -> Path:
        """å–å¾—ç‰©åŒ–ç›®æ¨™æª”æ¡ˆï¼ˆå„ªå…ˆ parquetï¼Œå¯¦éš›å¯«å…¥å¯èƒ½ç‚º pklï¼‰ã€‚"""
        out_dir = self._materialize_dir(layer_name)
        stem = self._materialize_stem(layer_name, params_hash)
        return out_dir / f"{stem}.parquet"

    def _materialize_metadata_path(self, layer_name: str, params_hash: str) -> Path:
        out_dir = self._materialize_dir(layer_name)
        stem = self._materialize_stem(layer_name, params_hash)
        return out_dir / f"{stem}.json"

    def _is_cache_valid(self, cache_file: Path, ttl_hours: int = 24) -> bool:
        if not cache_file.exists():
            return False
        try:
            modified_delta = time.time() - cache_file.stat().st_mtime
            return modified_delta <= ttl_hours * 3600
        except OSError:
            return False

    def _load_materialized_metadata(self, metadata_file: Path) -> Dict[str, Any]:
        if not metadata_file.exists():
            return {}
        try:
            with open(metadata_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def _save_materialized_metadata(self, metadata_file: Path, metadata: Dict[str, Any]) -> None:
        metadata_file.parent.mkdir(parents=True, exist_ok=True)
        # åŸå­æ–¹å¼å¯«å…¥ metadata JSON
        atomic_write_json(metadata_file, metadata)

    def _ensure_layer_input(self, layer_name: str) -> Optional[Path]:
        """è¿”å›ä¸Šä¸€å±¤çš„ç‰©åŒ–è³‡æ–™è·¯å¾‘"""
        order = [
            "layer0_cleaning",
            "layer1_labels",
            "layer2_features",
        ]
        idx = order.index(layer_name) if layer_name in order else -1
        if idx <= 0:
            return None
        prev_layer = order[idx - 1]
        prev_result = self.layer_results.get(prev_layer, {})
        materialized_path = prev_result.get("materialized_path")
        if materialized_path:
            p = Path(materialized_path)
            if p.exists():
                return p

        # å›é€€ï¼šåœ¨ data/processed å°‹æ‰¾ä¸Šä¸€å±¤è¼¸å‡º
        try:
            if prev_layer == "layer0_cleaning":
                # å°‹æ‰¾ cleanedï¼ˆéè¿´æœç´¢ç‰ˆæœ¬å­ç›®éŒ„ï¼‰
                base = self.processed_root / "cleaned" / f"{self.symbol}_{self.timeframe}"
                candidates = []
                if base.exists():
                    for pattern in ["**/cleaned_ohlcv.parquet", "**/cleaned_ohlcv_*.parquet", "**/cleaned_ohlcv.pkl", "**/cleaned_ohlcv_*.pkl"]:
                        candidates.extend(sorted(base.rglob(pattern), key=lambda x: x.stat().st_mtime, reverse=True))
                # Return first readable candidate to avoid corrupted files
                for c in candidates:
                    try:
                        _ = read_dataframe(c)
                        return c
                    except Exception:
                        continue

                # æœ€å¾Œå›é€€åˆ°èˆŠä½ç½®
                legacy = self.configs_path / f"cleaned_ohlcv_{self.timeframe}.parquet"
                if legacy.exists():
                    return legacy

            if prev_layer == "layer1_labels":
                # å°‹æ‰¾ labelsï¼ˆéè¿´æœç´¢ç‰ˆæœ¬å­ç›®éŒ„ï¼‰
                base = self.processed_root / "labels" / f"{self.symbol}_{self.timeframe}"
                candidates = []
                if base.exists():
                    for pattern in ["**/labels_*.parquet", "**/labels_*.pkl"]:
                        candidates.extend(sorted(base.rglob(pattern), key=lambda x: x.stat().st_mtime, reverse=True))
                # Return first readable candidate to avoid corrupted files
                for c in candidates:
                    try:
                        _ = read_dataframe(c)
                        return c
                    except Exception:
                        continue

                # å›é€€ï¼šè‹¥ç„¡å¯è®€å–ä¹‹ä¸Šä¸€å±¤æ¨™ç±¤ï¼Œå˜—è©¦å¾æ¸…æ´—è³‡æ–™èˆ‡Layer1é…ç½®é‡å»º
                try:
                    label_cfg_path = self.get_label_config_path(self.timeframe)
                    if label_cfg_path.exists():
                        with open(label_cfg_path, 'r', encoding='utf-8') as f:
                            label_cfg = json.load(f)
                        label_params = dict(label_cfg.get('best_params', {}))
                        if label_params:
                            cleaned_path = self.get_cleaned_file_path(self.timeframe)
                            df_cleaned = read_dataframe(cleaned_path)
                            # Meta-Labeling è¿”å› DataFrameï¼ˆåŒ…å« primary_signal, meta_quality, labelï¼‰
                            labels_df = MetaLabelOptimizer(
                                data_path=str(self.data_path),
                                config_path=str(self.configs_path),
                                symbol=self.symbol,
                                timeframe=self.timeframe,
                                scaled_config=self.scaled_config,
                            ).apply_labels(df_cleaned, label_params)
                            # æå– label åˆ—ï¼ˆ0/1/2 ä¸‰åˆ†é¡ï¼Œå‘å¾Œå…¼å®¹ï¼‰
                            labels = labels_df['label']
                            aligned = df_cleaned.loc[labels.index].copy()
                            aligned['label'] = labels
                            base_cols = [c for c in df_cleaned.columns if c != 'label']
                            processed = aligned[base_cols + ['label']]
                            params_hash = self._generate_params_hash(label_params)
                            target = self._materialize_cache_path('layer1_labels', params_hash)
                            actual_file, _fmt = write_dataframe(processed, target)
                            self.logger.info(f"ğŸ§© å·²å›é€€é‡å»ºLayer1æ¨™ç±¤ä¸¦ç‰©åŒ–: {actual_file}")
                            return actual_file
                except Exception as _e:
                    # éœé»˜å¤±æ•—ï¼Œäº¤ç”±ä¸Šå±¤è™•ç†
                    self.logger.warning(f"âš ï¸ æ¨™ç±¤å›é€€é‡å»ºå¤±æ•—: {_e}")

        except Exception:
            pass

        return None

    def _get_optimizer(self, layer_name: str):
        if layer_name == "layer0_cleaning":
            return DataCleaningOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe,
                scaled_config=self.scaled_config,
            )
        if layer_name == "layer1_labels":
            # ğŸ¯ ä½¿ç”¨ Meta-Labeling é›™å±¤æ¶æ§‹ï¼ˆPrimary + Metaï¼‰
            self.logger.info("âœ¨ ä½¿ç”¨ Meta-Labeling é›™å±¤æ¶æ§‹ï¼ˆPrimary + Metaï¼‰")
            return MetaLabelOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe,
                scaled_config=self.scaled_config
            )
        if layer_name == "layer2_features":
            return FeatureOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe,
                scaled_config=self.scaled_config,
            )
        raise ValueError(f"Unsupported layer for materialization: {layer_name}")

    def materialize_layer_data(
        self,
        layer_name: str,
        params: Dict[str, Any],
        input_data: Optional[pd.DataFrame] = None,
        optimization_result: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Path, Dict[str, Any]]:
        """ç‰©åŒ–å±¤ç´šè™•ç†çµæœï¼Œè¿”å›è³‡æ–™è·¯å¾‘èˆ‡metadata"""
        params_hash = self._generate_params_hash(params)
        cache_file = self._materialize_cache_path(layer_name, params_hash)
        metadata_file = self._materialize_metadata_path(layer_name, params_hash)

        if self._is_cache_valid(cache_file):
            metadata = self._load_materialized_metadata(metadata_file)
            metadata.setdefault("cached", True)
            metadata.setdefault("cache_path", str(cache_file))
            self.logger.info("âœ… ä½¿ç”¨ç·©å­˜è³‡æ–™: %s", cache_file.name)
            return cache_file, metadata

        optimizer = self._get_optimizer(layer_name)

        if layer_name == "layer0_cleaning":
            if input_data is None and optimization_result:
                cleaned_path = optimization_result.get("cleaned_file")
                if cleaned_path and Path(cleaned_path).exists():
                    input_data = read_dataframe(Path(cleaned_path))
            if input_data is None or input_data.empty:
                raise ValueError("Layer0ç¼ºå°‘æ¸…æ´—å¾Œè³‡æ–™ï¼Œç„¡æ³•ç‰©åŒ–")
            processed_data = input_data
            # æ¨™æº–åŒ–æ¬„ä½é †åºï¼šOHLCVå„ªå…ˆ
            ohlcv_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in processed_data.columns]
            other_cols = [c for c in processed_data.columns if c not in ohlcv_cols]
            processed_data = processed_data[ohlcv_cols + other_cols]

        elif layer_name == "layer1_labels":
            if input_data is None or "close" not in input_data.columns:
                raise ValueError("Layer1éœ€è¦åŒ…å«closeæ¬„ä½çš„ä¸Šä¸€å±¤è³‡æ–™")
            # Meta-Labeling è¿”å› DataFrameï¼ˆåŒ…å« primary_signal, meta_quality, labelï¼‰
            labeled_df = optimizer.apply_labels(input_data, params)
            labels = labeled_df['label']
            # Consistency æ ¡é©—
            if not set(labels.index).issubset(set(input_data.index)):
                raise ValueError("Layer1æ¨™ç±¤ç´¢å¼•ä¸åœ¨æ¸…æ´—è³‡æ–™ç´¢å¼•å…§")
            # ä»åµŒå¥—å‚æ•°ä¸­æå– lag
            lag_v = params.get('primary', {}).get("lag", 0) if isinstance(params, dict) and 'primary' in params else int(params.get("lag", 0))
            aligned = input_data.loc[labels.index].copy()
            if lag_v >= 0:
                expected = max(len(input_data) - lag_v, 0)
                if len(aligned) != expected:
                    raise ValueError(f"Layer1é•·åº¦ä¸ä¸€è‡´: aligned={len(aligned)}, expected={expected}")
            aligned["label"] = labels
            # æ¨™æº–åŒ–æ¬„ä½é †åºï¼šä¿ç•™åŸæ¸…æ´—æ¬„ä½é †åºï¼Œlabelç½®å°¾
            base_cols = [c for c in input_data.columns if c != "label"]
            processed_data = aligned[base_cols + ["label"]]

        elif layer_name == "layer2_features":
            if input_data is None or "label" not in input_data.columns:
                raise ValueError("Layer2éœ€è¦åŒ…å«labelæ¬„ä½çš„ä¸Šä¸€å±¤è³‡æ–™")
            # èˆ‡å„ªåŒ–ä¸€è‡´ï¼šé‡å»ºç‰¹å¾µçŸ©é™£ï¼Œå†æŒ‰æœ€ä½³å­é›†ç¯©é¸
            ohlcv_cols = [c for c in ["open", "high", "low", "close", "volume"] if c in input_data.columns]
            if not ohlcv_cols:
                raise ValueError("Layer2ç‰©åŒ–ç¼ºå°‘OHLCVæ¬„ä½ï¼Œç„¡æ³•é‡å»ºç‰¹å¾µ")
            ohlcv_df = input_data[ohlcv_cols].copy()
            try:
                X_all = optimizer.build_features_for_materialization(ohlcv_df, params)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•é‡å»ºç‰¹å¾µï¼Œå›é€€è‡³ç©ºç‰¹å¾µ: {e}")
                X_all = pd.DataFrame(index=ohlcv_df.index)

            # å°é½Šç´¢å¼•ä¸¦é¸æ“‡æœ€ä½³ç‰¹å¾µå­é›†
            selected = params.get("selected_features") or []
            available = [c for c in selected if c in X_all.columns]
            missing = [c for c in selected if c not in X_all.columns]
            if missing:
                self.logger.warning("âš ï¸ Layer2ç‰©åŒ–ç¼ºå¤±ç‰¹å¾µåˆ—: %s", missing)
            X_sel = X_all.reindex(input_data.index).fillna(0)
            if available:
                X_sel = X_sel[available]
            # åˆä½µæ¨™ç±¤
            processed_data = pd.concat([X_sel, input_data[["label"]]], axis=1)
        else:
            raise ValueError(f"Unsupported layer: {layer_name}")

        if processed_data is None or processed_data.empty:
            raise ValueError(f"{layer_name} processed data is empty, ç„¡æ³•ç‰©åŒ–")

        cache_file.parent.mkdir(parents=True, exist_ok=True)

        # æ‰“å°JSONè·¯å¾‘èˆ‡åƒæ•¸é›œæ¹Šï¼Œå¹«åŠ©CLUæ—¥èªŒç¢ºèªã€Œç”±JSONåƒæ•¸ç”ŸæˆçœŸå¯¦æª”æ¡ˆã€
        json_path = self._get_layer_json_path(layer_name)
        try:
            params_hash = self._generate_params_hash(params)
            self.logger.info(
                f"ğŸ“„ {layer_name} è¶…åƒæ•¸JSON: {json_path} | params_hash={params_hash}"
            )
        except Exception:
            pass

        actual_file, storage_format = write_dataframe(processed_data, cache_file)
        # æª”æ¡ˆå…§å®¹æ ¡é©—ç¢¼
        try:
            file_md5 = compute_file_md5(actual_file)
        except Exception:
            file_md5 = None
        self.logger.info(
            f"ğŸ’¾ {layer_name} ç‰©åŒ–è¼¸å‡º: {actual_file} | format={storage_format} | MD5={file_md5}"
        )

        # è£œå……metadataï¼ˆå«best_paramsã€jsonè·¯å¾‘ã€scoreç­‰ï¼‰
        metadata = {
            "created_at": pd.Timestamp.utcnow().isoformat(),
            "params_hash": params_hash,
            "data_shape": processed_data.shape,
            "data_columns": list(processed_data.columns),
            "cached": False,
            "storage_format": storage_format,
            "json_path": str(json_path),
            "best_params": params,
            "file_path": str(actual_file),
            "file_md5": file_md5,
        }
        if isinstance(optimization_result, dict):
            if "best_score" in optimization_result:
                metadata["best_score"] = optimization_result["best_score"]
            if "n_trials" in optimization_result:
                metadata["n_trials"] = optimization_result["n_trials"]

        self._save_materialized_metadata(metadata_file, metadata)
        return actual_file, metadata

    def run_layer_with_materialization(
        self,
        layer_name: str,
        n_trials: int,
        prev_data_path: Optional[Path] = None,
    ) -> Dict[str, Any]:
        optimizer = self._get_optimizer(layer_name)
        if hasattr(optimizer, "set_input_data_path") and prev_data_path:
            optimizer.set_input_data_path(str(prev_data_path))

        optimization_result = optimizer.optimize(n_trials=n_trials)
        best_params = dict(optimization_result.get("best_params", {}))
        best_params["n_trials"] = n_trials

        # ğŸ”— å±¤é–“ç´„æŸï¼šLayer2 çš„ lag éœ€å°é½Š Layer1 çš„æœ€ä½³ lag
        if layer_name == "layer2_features":
            try:
                l1_params = self.layer_results.get("layer1_labels", {}).get("best_params", {})
                if isinstance(l1_params, dict) and "lag" in l1_params:
                    best_params["lag"] = int(l1_params.get("lag"))
                    self.logger.info(f"ğŸ”— å°é½Šè¨­å®š: Layer2 lag = Layer1 lag = {best_params['lag']}")
            except Exception as _e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•å°é½Š Layer2 lag èˆ‡ Layer1: {_e}")

        if layer_name == "layer0_cleaning":
            cleaned_file = optimization_result.get("cleaned_file")
            if cleaned_file and Path(cleaned_file).exists():
                input_df = read_dataframe(Path(cleaned_file))
            else:
                input_df = None
        else:
            if not prev_data_path or not prev_data_path.exists():
                prev_data_path = self._ensure_layer_input(layer_name)  # å†å˜—è©¦ä¸€æ¬¡ï¼ˆå¸¶å›é€€ï¼‰
            input_df = read_dataframe(prev_data_path) if prev_data_path and prev_data_path.exists() else None
            if input_df is None:
                raise ValueError(f"{layer_name} ç¼ºå°‘ä¸Šä¸€å±¤è³‡æ–™")

        # åœ¨ç‰©åŒ–ä¹‹å‰æ‰“å°ä¾†æºJSONèˆ‡é›œæ¹Šï¼Œä»¥ä¾¿å¯©è¨ˆ
        try:
            json_path = self._get_layer_json_path(layer_name)
            params_hash = self._generate_params_hash(best_params)
            self.logger.info(
                f"ğŸ§ª {layer_name} materialize å‰æª¢æŸ¥: JSON={json_path} | params_hash={params_hash}"
            )
        except Exception:
            pass

        materialized_path, metadata = self.materialize_layer_data(
            layer_name,
            best_params,
            input_df,
            optimization_result,
        )

        version_id = self.version_manager.create_data_version(
            layer_name,
            best_params,
            metadata.get("data_shape", (0, 0)),
        )
        try:
            self.version_manager.save_materialized_data_info(
                version_id,
                layer_name,
                str(materialized_path),
                metadata,
            )
        except Exception as _e:
            self.logger.warning(f"âš ï¸ ä¿å­˜è¡€çµ±è³‡è¨Šå¤±æ•—: {getattr(_e, 'message', _e)}")

        final_result = {
            **optimization_result,
            "best_params": best_params,
            "materialized_path": str(materialized_path),
            "metadata": metadata,
            "data_version": version_id,
        }

        self._save_layer_params(layer_name, final_result)
        return final_result

    def _save_layer_params(self, layer_name: str, result: Dict[str, Any]) -> None:
        # å°‡çµæœè½‰ç‚ºå¯åºåˆ—åŒ–ï¼Œéæ¿¾ DataFrame/Series ç­‰å¤§å‹ç‰©ä»¶
        import numpy as _np  # å±€éƒ¨å¼•ç”¨ï¼Œé¿å…å¾ªç’°å¼•ç”¨é¢¨éšª
        try:
            import pandas as _pd  # å¯é¸
        except Exception:  # pragma: no cover
            _pd = None

        def _is_pandas_obj(obj: Any) -> bool:
            if _pd is None:
                return False
            return isinstance(obj, (_pd.DataFrame, _pd.Series, _pd.Index))

        def _to_json_safe(obj: Any) -> Any:
            if _is_pandas_obj(obj):
                return None  # ä¸Ÿæ£„å¤§å‹è³‡æ–™æœ¬é«”ï¼Œé¿å…JSONè†¨è„¹ï¼›ç‰©åŒ–è·¯å¾‘å·²åœ¨ result ä¸­
            if isinstance(obj, (str, int, float, bool)) or obj is None:
                return obj
            if isinstance(obj, Path):
                return str(obj)
            if isinstance(obj, _np.integer):
                return int(obj)
            if isinstance(obj, _np.floating):
                return float(obj)
            if isinstance(obj, _np.ndarray):
                return obj.tolist()
            if isinstance(obj, dict):
                safe_dict = {}
                for k, v in obj.items():
                    if _is_pandas_obj(v):
                        continue
                    safe_v = _to_json_safe(v)
                    if safe_v is not None:
                        safe_dict[k] = safe_v
                return safe_dict
            if isinstance(obj, (list, tuple, set)):
                safe_list = []
                for v in obj:
                    if _is_pandas_obj(v):
                        continue
                    safe_v = _to_json_safe(v)
                    if safe_v is not None:
                        safe_list.append(safe_v)
                return safe_list
            # å…¶å®ƒæœªçŸ¥é¡å‹ï¼Œé€€å›å­—ä¸²
            return str(obj)

        safe_result = _to_json_safe(result)
        if isinstance(safe_result, dict):
            # é¡å¤–ä¿éšªï¼šä¸»å‹•ç§»é™¤å¯èƒ½æ®˜ç•™çš„å¤§å‹éµ
            safe_result.pop('labeled_data', None)
            safe_result.pop('materialized_features', None)

        layer_to_filename = {
            "layer0_cleaning": "cleaning_params",
            "layer1_labels": "label_params",
            "layer2_features": "feature_params",
        }
        base_name = layer_to_filename.get(layer_name)
        if not base_name:
            return
        file_path = self.configs_path / f"{base_name}.json"
        atomic_write_json(file_path, safe_result)

        tf_file_path = self.configs_path / f"{base_name}_{self.timeframe}.json"
        atomic_write_json(tf_file_path, safe_result)

    def run_layer0_data_cleaning(self, n_trials: int = 25) -> Dict[str, Any]:
        """Layer0ï¼šè³‡æ–™æ¸…æ´— + ç‰©åŒ–"""
        self.logger.info("ğŸ”§ Layer0ï¼šæ•¸æ“šæ¸…æ´—èˆ‡ç‰©åŒ–...")

        try:
            result = self.run_layer_with_materialization("layer0_cleaning", n_trials)
            self.layer_results["layer0_cleaning"] = result
            
            # âœ… æ·»åŠ çµæœé©—è­‰
            if 'best_score' in result and result['best_score'] < 0.3:
                self.logger.warning(f"âš ï¸ Layer0åˆ†æ•¸éä½: {result['best_score']:.4f}ï¼Œå»ºè­°æª¢æŸ¥æ•¸æ“šè³ªé‡")
            if 'error' in result:
                self.logger.error(f"âŒ Layer0å‡ºç¾éŒ¯èª¤: {result['error']}")
            
            cleaned_path = result.get("materialized_path")
            if cleaned_path:
                self.scaled_config["cleaned_file"] = cleaned_path
            return result
        except Exception as e:
            self.logger.error("âŒ Layer0æ•¸æ“šæ¸…æ´—å¤±æ•—: %s", e)
            error_result = {"error": str(e), "layer": 0}
            self.layer_results["layer0_cleaning"] = error_result
            return error_result
    
    # ============================================================
    # Layer1-4ï¼šæ ¸å¿ƒå„ªåŒ–å±¤
    # ============================================================
    
    def run_layer1_label_optimization(self, n_trials: int = 50) -> Dict[str, Any]:
        """
        Layer1ï¼šæ¨™ç±¤ç”Ÿæˆ + ç‰©åŒ–
        
        ğŸ”§ P0ä¿®å¤ï¼šé™ä½trialæ•°é‡75% (200â†’50)
        é—®é¢˜ï¼šè¿‡å¤štrialså¯¼è‡´æ•°æ®çª¥æ¢åå·®
        - åŸ150Ã—250=37,500æ¬¡éšæ€§æµ‹è¯•ï¼Œé¢„æœŸ1,875ä¸ªå‡é˜³æ€§ï¼ˆ5% FDRï¼‰
        
        è§£å†³ï¼šåº”ç”¨Romano-Wolfå¤šé‡æ£€éªŒæ ¡æ­£
        å­¦æœ¯ä¾æ®ï¼šRomano & Wolf (2005), White (2000)
        """
        self.logger.info("ğŸ¯ Layer1ï¼šæ¨™ç±¤ç”Ÿæˆèˆ‡ç‰©åŒ–...")

        try:
            prev_path = self._ensure_layer_input("layer1_labels")
            result = self.run_layer_with_materialization("layer1_labels", n_trials, prev_path)
            self.layer_results["layer1_labels"] = result
            
            # âœ… æ·»åŠ çµæœé©—è­‰
            if 'best_score' in result:
                if result['best_score'] < 0.45:
                    self.logger.warning(f"âš ï¸ Layer1åˆ†æ•¸éä½: {result['best_score']:.4f}ï¼Œå»ºè­°æª¢æŸ¥æ¨™ç±¤å¹³è¡¡æ€§")
                # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ
                if 'metadata' in result and 'label_distribution' in result['metadata']:
                    dist = result['metadata']['label_distribution']
                    if len(dist) == 3:
                        ratios = list(dist.values())
                        imbalance = max(ratios) / min(ratios) if min(ratios) > 0 else 99
                        if imbalance > 5.0:
                            self.logger.warning(f"âš ï¸ æ¨™ç±¤ä¸å¹³è¡¡æ¯”éé«˜: {imbalance:.2f}")
            
            return result
        except Exception as e:
            self.logger.error("âŒ Layer1æ¨™ç±¤å„ªåŒ–å¤±æ•—: %s", e)
            error_result = {"error": str(e), "layer": 1}
            self.layer_results["layer1_labels"] = error_result
            return error_result
    
    def run_layer2_feature_optimization(
        self, n_trials: int = 30, multi_timeframes: List[str] = None
    ) -> Dict[str, Any]:
        """Layer2ï¼šç‰¹å¾µå·¥ç¨‹åƒæ•¸å„ªåŒ– - æ”¯æŒå–®ä¸€æ™‚æ¡†æˆ–å¤šæ™‚æ¡†å„ªåŒ–"""

        if multi_timeframes:
            # ğŸš€ å¤šæ™‚æ¡†å„ªåŒ–æ¨¡å¼
            self.logger.info(f"ğŸ¯ Layer2å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–: {multi_timeframes}...")
            return self._run_multi_timeframe_feature_optimization(multi_timeframes, n_trials)
        else:
            # åŸæœ‰çš„å–®ä¸€æ™‚æ¡†å„ªåŒ–æ¨¡å¼
            self.logger.info(f"ğŸ¯ Layer2å–®ä¸€æ™‚æ¡†ç‰¹å¾µå„ªåŒ–: {self.timeframe}...")
        
        try:
            # å¯é¸ï¼šä½¿ç”¨å¢å¼·ç·¨æ’å™¨ï¼ˆä¸æ”¹å‹•æ—¢æœ‰çµæ§‹ï¼Œé€éç’°å¢ƒè®Šé‡é–‹é—œï¼‰
            if os.getenv('OPTUNA_ENHANCED_L2', '0') == '1':
                self.logger.info("ğŸš€ ä½¿ç”¨ EnhancedLayer2Optimizerï¼ˆPhase 5 ç·¨æ’å™¨ï¼‰")
                prev_path = self._ensure_layer_input("layer2_features")
                if not prev_path or not prev_path.exists():
                    raise ValueError("Layer2ç¼ºå°‘ä¸Šä¸€å±¤è³‡æ–™")

                # æ§‹å»ºåŸºç¤å„ªåŒ–å™¨ä¸¦äº¤ç”±ç·¨æ’å™¨åŸ·è¡Œå¢å¼·å„ªåŒ–
                base_optimizer = FeatureOptimizer(
                    data_path=str(self.data_path),
                    config_path=str(self.configs_path),
                    symbol=self.symbol,
                    timeframe=self.timeframe,
                    scaled_config=self.scaled_config,
                )

                # Phase 5 å…§åµŒç·¨æ’ï¼šæ¨™ç±¤è³ªæª¢â†’ï¼ˆå¿…è¦æ™‚ï¼‰é‡è¨“ç·´â†’è‡ªé©æ‡‰trialså„ªåŒ–â†’é©—è­‰
                analysis = base_optimizer.analyze_label_quality()
                self.logger.info(f"Label åˆ†ä½ˆ/ç©©å®šæ€§: {analysis}")
                if not analysis.get('pass', True):
                    retrain = base_optimizer.auto_retrain_labels_if_needed()
                    if retrain:
                        self.logger.info(f"Layer1 é‡æ–°å„ªåŒ–å®Œæˆ: F1={retrain.get('best_score', 0):.4f}")

                trials_completed = 0
                target_trials = n_trials
                best_score = -1.0
                best_params: Dict[str, Any] = {}

                def _next_target(current_best: float, done: int) -> int:
                    if current_best < 0.30:
                        return min(done + 100, 500)
                    elif current_best > 0.60:
                        return done + 20
                    else:
                        return done + 50

                while trials_completed < target_trials:
                    remaining = target_trials - trials_completed
                    n_batch = min(remaining, 50)
                    self.logger.info(
                        f"åŸ·è¡Œæ‰¹æ¬¡: trials {trials_completed}->{trials_completed + n_batch} / target {target_trials}"
                    )
                    result_batch = base_optimizer.optimize(n_trials=n_batch)
                    trials_completed += n_batch

                    cur_best = float(result_batch.get('best_score', 0.0))
                    if cur_best > best_score:
                        best_score = cur_best
                        best_params = dict(result_batch.get('best_params', {}))

                    target_trials = _next_target(best_score, trials_completed)
                    self.logger.info(
                        f"ç´¯è¨ˆ {trials_completed} trials, ç•¶å‰æœ€ä½³={best_score:.4f}, æ–°ç›®æ¨™={target_trials}"
                    )
                    if trials_completed >= 500 or (trials_completed >= n_trials and best_score >= 0.70):
                        break

                # ğŸ”— å±¤é–“ç´„æŸï¼šLayer2 çš„ lag éœ€å°é½Š Layer1 çš„æœ€ä½³ lagï¼ˆEnhanced è·¯å¾‘ï¼‰
                try:
                    l1_params = self.layer_results.get("layer1_labels", {}).get("best_params", {})
                    if isinstance(l1_params, dict) and "lag" in l1_params:
                        best_params["lag"] = int(l1_params.get("lag"))
                        self.logger.info(f"ğŸ”— å°é½Šè¨­å®š(Enhanced): Layer2 lag = Layer1 lag = {best_params['lag']}")
                except Exception as _e:
                    self.logger.warning(f"âš ï¸ ç„¡æ³•å°é½Š Layer2 lag èˆ‡ Layer1(Enhanced): {_e}")

                optimization_result = {
                    'best_params': best_params,
                    'best_score': best_score,
                    'n_trials': n_trials,
                }

                input_df = read_dataframe(prev_path)
                materialized_path, metadata = self.materialize_layer_data(
                    "layer2_features",
                    best_params,
                    input_df,
                    optimization_result,
                )

                version_id = self.version_manager.create_data_version(
                    "layer2_features",
                    best_params,
                    metadata.get("data_shape", (0, 0)),
                )
                try:
                    self.version_manager.save_materialized_data_info(
                        version_id,
                        "layer2_features",
                        str(materialized_path),
                        metadata,
                    )
                except Exception as _e:
                    self.logger.warning(f"âš ï¸ ä¿å­˜è¡€çµ±è³‡è¨Šå¤±æ•—: {getattr(_e, 'message', _e)}")

                final_result = {
                    **optimization_result,
                    "materialized_path": str(materialized_path),
                    "metadata": metadata,
                    "data_version": version_id,
                }
                self.layer_results["layer2_features"] = final_result

                # çµæœé©—è­‰èˆ‡æç¤º
                if 'best_score' in final_result and final_result['best_score'] < 0.50:
                    self.logger.warning(
                        f"âš ï¸ Layer2åˆ†æ•¸éä½: {final_result['best_score']:.4f}ï¼Œå»ºè­°å¢åŠ è©¦é©—æ¬¡æ•¸æˆ–æª¢æŸ¥ç‰¹å¾µè³ªé‡"
                    )
                if 'best_params' in final_result and 'selected_features' in final_result['best_params']:
                    selected = final_result['best_params']['selected_features']
                    strategy_features = [f for f in selected if any(p in f for p in ['wyk_', 'td_', 'micro_'])]
                    self.logger.info(f"âœ… ä¿ç•™ç­–ç•¥ç‰¹å¾µ: {len(strategy_features)}å€‹")
                    if len(strategy_features) == 0:
                        self.logger.warning("âš ï¸ è­¦å‘Š: æœªé¸ä¸­ä»»ä½•ç­–ç•¥ç‰¹å¾µï¼ˆWyckoff/TD/Microï¼‰")

                self.logger.info("âœ… Layer2ç‰¹å¾µå„ªåŒ–å®Œæˆ (Enhanced)")
                return final_result

            # é»˜èªï¼šä½¿ç”¨æ—¢æœ‰æµç¨‹
            prev_path = self._ensure_layer_input("layer2_features")
            result = self.run_layer_with_materialization("layer2_features", n_trials, prev_path)
            self.layer_results["layer2_features"] = result

            # âœ… æ·»åŠ çµæœé©—è­‰
            if 'best_score' in result:
                if result['best_score'] < 0.50:
                    self.logger.warning(f"âš ï¸ Layer2åˆ†æ•¸éä½: {result['best_score']:.4f}ï¼Œå»ºè­°å¢åŠ è©¦é©—æ¬¡æ•¸æˆ–æª¢æŸ¥ç‰¹å¾µè³ªé‡")
                # æª¢æŸ¥ç­–ç•¥ç‰¹å¾µæ˜¯å¦ä¿ç•™
                if 'best_params' in result and 'selected_features' in result['best_params']:
                    selected = result['best_params']['selected_features']
                    strategy_features = [f for f in selected if any(p in f for p in ['wyk_', 'td_', 'micro_'])]
                    self.logger.info(f"âœ… ä¿ç•™ç­–ç•¥ç‰¹å¾µ: {len(strategy_features)}å€‹")
                    if len(strategy_features) == 0:
                        self.logger.warning("âš ï¸ è­¦å‘Š: æœªé¸ä¸­ä»»ä½•ç­–ç•¥ç‰¹å¾µï¼ˆWyckoff/TD/Microï¼‰")

            self.logger.info("âœ… Layer2ç‰¹å¾µå„ªåŒ–å®Œæˆ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ Layer2ç‰¹å¾µå„ªåŒ–å¤±æ•—: {e}")
            self.logger.debug("Layer2 stacktrace:\n%s", traceback.format_exc())
            error_result = {'error': str(e), 'layer': 2}
            self.layer_results['layer2_features'] = error_result
            return error_result

    def _run_multi_timeframe_feature_optimization(self, timeframes: List[str], n_trials: int = 50) -> Dict[str, Any]:
        """ğŸš€ å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–ï¼šç‚ºæ¯å€‹æ™‚æ¡†æ‰¾åˆ°æœ€ä½³140-203ç²—é¸â†’10-25ç²¾é¸ç‰¹å¾µçµ„åˆï¼ˆå…¨é‡203ç‰¹å¾µæ± ï¼‰"""
        self.logger.info(f"ğŸš€ é–‹å§‹å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–: {timeframes}")

        try:
            # å‰µå»ºåŸºç¤å„ªåŒ–å™¨ç”¨æ–¼å”èª¿
            base_optimizer = FeatureOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe  # åŸºç¤æ™‚æ¡†
            )

            # åŸ·è¡Œå¤šæ™‚æ¡†å„ªåŒ–
            multi_result = base_optimizer.optimize_multi_timeframes(
                timeframes=timeframes,
                n_trials=n_trials
            )

            # å­˜å„²çµæœ
            self.layer_results['layer2_features'] = multi_result

            # è¼¸å‡ºæ‘˜è¦
            successful_count = multi_result['summary']['successful_optimizations']
            total_count = len(timeframes)
            best_tf = multi_result['summary']['best_performing_timeframe']

            self.logger.info("ğŸ¯ å¤šæ™‚æ¡†å„ªåŒ–å®Œæˆæ‘˜è¦:")
            self.logger.info(f"   æˆåŠŸ: {successful_count}/{total_count} å€‹æ™‚æ¡†")
            self.logger.info(f"   æœ€ä½³æ™‚æ¡†: {best_tf}")

            for tf, cfg in multi_result['best_configs'].items():
                if 'error' not in cfg:
                    self.logger.info(f"   {tf}: F1={cfg['best_score']:.4f}, {cfg['feature_range']}")
                else:
                    self.logger.error(f"   {tf}: å¤±æ•— - {cfg['error']}")

            self.logger.info("âœ… Layer2å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–å®Œæˆ")
            return multi_result

        except Exception as e:
            self.logger.error(f"âŒ Layer2å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 2, 'type': 'multi_timeframe'}
            self.layer_results['layer2_features'] = error_result
            return error_result
    
    def run_layer3_model_optimization(self, n_trials: int = 25) -> Dict[str, Any]:
        """
        Layer3ï¼šæ¨¡å‹è¶…åƒæ•¸å„ªåŒ–
        
        ğŸ”§ P0ä¿®å¤ï¼šé™ä½trialæ•°é‡75% (100â†’25)
        å‡å°‘æ•°æ®çª¥æ¢åå·®ï¼Œæ§åˆ¶FWER
        """
        self.logger.info("ğŸ¯ Layer3ï¼šæ¨¡å‹è¶…åƒæ•¸å„ªåŒ–...")
        
        if not globals().get('HAS_MODEL_OPTIMIZER', False):
            self.logger.warning("âš ï¸ Layer3è·³é: ModelOptimizerä¸å¯ç”¨")
            error_result = {'error': 'module_not_available', 'layer': 3}
            self.layer_results['layer3_models'] = error_result
            return error_result

        try:
            optimizer = ModelOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe,
                results_path=str(self.results_path / f"{self.symbol}_{self.timeframe}" / str(self.current_version))
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer3_models'] = result
            
            self.logger.info("âœ… Layer3æ¨¡å‹å„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer3æ¨¡å‹å„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 3}
            self.layer_results['layer3_models'] = error_result
            return error_result
    
    def run_layer4_cv_risk_optimization(self, n_trials: int = 100) -> Dict[str, Any]:
        """Layer4ï¼šäº¤å‰é©—è­‰èˆ‡é¢¨æ§åƒæ•¸å„ªåŒ–"""
        self.logger.info("ğŸ¯ Layer4ï¼šäº¤å‰é©—è­‰èˆ‡é¢¨æ§åƒæ•¸å„ªåŒ–...")
        
        if not globals().get('HAS_CV_RISK_OPTIMIZER', False):
            self.logger.warning("âš ï¸ Layer4è·³é: CVRiskOptimizerä¸å¯ç”¨")
            error_result = {'error': 'module_not_available', 'layer': 4}
            self.layer_results['layer4_cv_risk'] = error_result
            return error_result

        try:
            optimizer = CVRiskOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path),
                symbol=self.symbol,
                timeframe=self.timeframe
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer4_cv_risk'] = result
            
            self.logger.info("âœ… Layer4é¢¨æ§å„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer4é¢¨æ§å„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 4}
            self.layer_results['layer4_cv_risk'] = error_result
            return error_result
    
    # ============================================================
    # Layer5-8ï¼šå°ˆé …å„ªåŒ–å±¤
    # ============================================================
    
    def run_layer5_kelly_optimization(self, n_trials: int = 50) -> Dict[str, Any]:
        """Layer5ï¼šKellyå…¬å¼è³‡é‡‘ç®¡ç†å„ªåŒ–"""
        self.logger.info("ğŸ¯ Layer5ï¼šKellyå…¬å¼è³‡é‡‘ç®¡ç†å„ªåŒ–...")
        
        try:
            optimizer = KellyOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path)
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer5_kelly'] = result
            
            self.logger.info("âœ… Layer5 Kellyå„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer5 Kellyå„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 5}
            self.layer_results['layer5_kelly'] = error_result
            return error_result
    
    def run_layer6_ensemble_optimization(self, n_trials: int = 50) -> Dict[str, Any]:
        """Layer6ï¼šå¤šæ¨¡å‹èåˆæ¬Šé‡å„ªåŒ–"""
        self.logger.info("ğŸ¯ Layer6ï¼šå¤šæ¨¡å‹èåˆæ¬Šé‡å„ªåŒ–...")
        
        try:
            optimizer = EnsembleOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path)
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer6_ensemble'] = result
            
            self.logger.info("âœ… Layer6å¤šæ¨¡å‹èåˆå„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer6å¤šæ¨¡å‹èåˆå„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 6}
            self.layer_results['layer6_ensemble'] = error_result
            return error_result
    
    def run_layer7_polynomial_optimization(self, n_trials: int = 30) -> Dict[str, Any]:
        """Layer7ï¼šå¤šé …å¼ç‰¹å¾µç”Ÿæˆå„ªåŒ–"""
        self.logger.info("ğŸ¯ Layer7ï¼šå¤šé …å¼ç‰¹å¾µç”Ÿæˆå„ªåŒ–...")
        
        try:
            optimizer = PolynomialOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path)
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer7_polynomial'] = result
            
            self.logger.info("âœ… Layer7å¤šé …å¼ç‰¹å¾µå„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer7å¤šé …å¼ç‰¹å¾µå„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 7}
            self.layer_results['layer7_polynomial'] = error_result
            return error_result
    
    def run_layer8_confidence_optimization(self, n_trials: int = 40) -> Dict[str, Any]:
        """Layer8ï¼šå‹•æ…‹ç½®ä¿¡åº¦é–¾å€¼å„ªåŒ–"""
        self.logger.info("ğŸ¯ Layer8ï¼šå‹•æ…‹ç½®ä¿¡åº¦é–¾å€¼å„ªåŒ–...")
        
        try:
            optimizer = ConfidenceOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.configs_path)
            )
            
            result = optimizer.optimize(n_trials=n_trials)
            self.layer_results['layer8_confidence'] = result
            
            self.logger.info("âœ… Layer8å‹•æ…‹ç½®ä¿¡åº¦å„ªåŒ–å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Layer8å‹•æ…‹ç½®ä¿¡åº¦å„ªåŒ–å¤±æ•—: {e}")
            error_result = {'error': str(e), 'layer': 8}
            self.layer_results['layer8_confidence'] = error_result
            return error_result
    
    # ============================================================
    # å®Œæ•´9å±¤å„ªåŒ–æµç¨‹
    # ============================================================
    
    def run_complete_layered_optimization(self, trial_config: Dict[str, int] = None) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„Layer0+9å±¤å„ªåŒ–æµç¨‹"""
        if trial_config is None:
            trial_config = {
                # Layer0: æ•¸æ“šæ¸…æ´—åŸºç¤å±¤ï¼ˆå¿…é ˆåŸ·è¡Œï¼‰
                'layer0': 25,   # æ•¸æ“šæ¸…æ´—
                # Layer1-4: æ ¸å¿ƒå„ªåŒ–å±¤
                'layer1': 100,  # æ¨™ç±¤ç”Ÿæˆ
                'layer2': 200,  # ç‰¹å¾µå·¥ç¨‹
                'layer3': 100,  # æ¨¡å‹è¶…åƒæ•¸
                'layer4': 100,  # CVé¢¨æ§
                # Layer5-8: å°ˆé …å„ªåŒ–å±¤
                'layer5': 50,   # Kellyè³‡é‡‘ç®¡ç†
                'layer6': 50,   # å¤šæ¨¡å‹èåˆ
                'layer7': 30,   # å¤šé …å¼ç‰¹å¾µ
                'layer8': 40,   # å‹•æ…‹ç½®ä¿¡åº¦
            }
 
        self.logger.info("ğŸš€ é–‹å§‹å®Œæ•´Layer0+9å±¤å„ªåŒ–æµç¨‹...")
        overall_start_time = time.time()
 
        # Layer0: æ•¸æ“šæ¸…æ´—åŸºç¤å±¤ï¼ˆå¿…é ˆåŸ·è¡Œï¼‰
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“‹ Layer0: æ•¸æ“šæ¸…æ´—åŸºç¤å±¤ï¼ˆå¿…é ˆåŸ·è¡Œï¼‰")
        self.run_layer0_data_cleaning(trial_config.get('layer0', 25))
        time.sleep(1)
 
        # Layer1-4: æ ¸å¿ƒå„ªåŒ–å±¤ï¼ˆæŒ‰é †åºåŸ·è¡Œï¼‰
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“‹ Layer1-4: æ ¸å¿ƒå„ªåŒ–å±¤")
 
        self.run_layer1_label_optimization(trial_config.get('layer1', 200))
        time.sleep(1)
 
        self.run_layer2_feature_optimization(trial_config.get('layer2', 100))
        time.sleep(1)
 
        self.run_layer3_model_optimization(trial_config.get('layer3', 100))
        time.sleep(1)
 
        self.run_layer4_cv_risk_optimization(trial_config.get('layer4', 100))
        time.sleep(1)

        # Layer5-8: å°ˆé …å„ªåŒ–å±¤ï¼ˆå¯ä¸¦è¡ŒåŸ·è¡Œï¼‰
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“‹ Layer5-8: å°ˆé …å„ªåŒ–å±¤")

        self.run_layer5_kelly_optimization(trial_config.get('layer5', 50))
        time.sleep(1)

        self.run_layer6_ensemble_optimization(trial_config.get('layer6', 50))
        time.sleep(1)

        self.run_layer7_polynomial_optimization(trial_config.get('layer7', 30))
        time.sleep(1)

        self.run_layer8_confidence_optimization(trial_config.get('layer8', 40))

        total_time = time.time() - overall_start_time

        # çµ±è¨ˆå„ªåŒ–çµæœ
        successful_modules = sum(1 for result in self.layer_results.values() if 'error' not in result)
        total_modules = len(self.layer_results)
        success_rate = successful_modules / total_modules if total_modules > 0 else 0

        # æ”¶é›†æœ€ä½³åˆ†æ•¸
        best_scores = {}
        for layer_name, result in self.layer_results.items():
            if 'best_score' in result:
                best_scores[layer_name] = result['best_score']

        # ä¿å­˜çµæœ
        self.version_manager.save_results(self.current_version, self.layer_results)
        self.version_manager.set_latest(self.current_version)

        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ å®Œæ•´Layer0+9å±¤å„ªåŒ–æµç¨‹åŸ·è¡Œå®Œç•¢!")
        self.logger.info(f"âœ… æˆåŠŸæ¨¡å¡Š: {successful_modules}/{total_modules} ({success_rate:.1%})")
        self.logger.info(f"â±ï¸ ç¸½è€—æ™‚: {total_time:.1f}ç§’")

        return {
            'version': self.current_version,
            'layer_results': self.layer_results,
            'total_time': total_time,
            'optimization_summary': {
                'total_modules': total_modules,
                'successful_modules': successful_modules,
                'failed_modules': total_modules - successful_modules,
                'success_rate': success_rate,
                'best_scores': best_scores
            }
        }
    
    def quick_complete_optimization(self) -> Dict[str, Any]:
        self.logger.info("ğŸš€ é–‹å§‹åˆ†éšæ®µä¸²è¯å„ªåŒ–æµç¨‹ï¼ˆLayer0â†’Layer1â†’Layer2è¯å‹•ï¼‰...")
        overall_start_time = time.time()

        self.logger.info("ğŸ“Š é˜¶æ®µä¸€ï¼šLayer0æ•¸æ“šæ¸…æ´—èˆ‡çµ±è¨ˆåˆ†æ...")
        self.run_layer0_data_cleaning(max(15, 10))

        self.logger.info("ğŸ·ï¸ é˜¶æ®µäºŒï¼šLayer1æ¨™ç±¤å„ªåŒ–...")
        self.run_layer1_label_optimization(75)

        self.logger.info("ğŸ”§ é˜¶æ®µä¸‰ï¼šLayer2ç‰¹å¾µå„ªåŒ–...")
        self.run_layer2_feature_optimization(75)

        self.logger.info("ğŸ“˜ é©—è­‰èˆ‡é‡è·‘...")
        self.rerun_layers_if_needed(min_score=0.45, retries=1)

        if globals().get('HAS_MODEL_OPTIMIZER', False):
            self.run_layer3_model_optimization(50)
        if globals().get('HAS_CV_RISK_OPTIMIZER', False):
            self.run_layer4_cv_risk_optimization(50)

        total_time = time.time() - overall_start_time
        successful_modules = sum(
            1 for result in self.layer_results.values() if 'error' not in result
        )
        total_modules = len(self.layer_results)
        success_rate = successful_modules / total_modules if total_modules > 0 else 0

        best_scores = {}
        for layer_name, result in self.layer_results.items():
            if 'best_score' in result:
                best_scores[layer_name] = result['best_score']

        # ä¿å­˜çµæœä¸¦æ›´æ–°æœ€æ–°ç‰ˆæœ¬æ¨™è¨˜
        try:
            self.version_manager.save_results(self.current_version, self.layer_results)
            self.version_manager.set_latest(self.current_version)
        except Exception as _e:
            self.logger.warning(f"âš ï¸ quick æµç¨‹ä¿å­˜ç‰ˆæœ¬æˆ–è¨­ç½® latest å¤±æ•—: {getattr(_e, 'message', _e)}")

        return {
            'version': self.current_version,
            'layer_results': self.layer_results,
            'total_time': total_time,
            'optimization_summary': {
                'total_modules': total_modules,
                'successful_modules': successful_modules,
                'failed_modules': total_modules - successful_modules,
                'success_rate': success_rate,
                'best_scores': best_scores
            }
        }
    
    # ============================================================
    # å‘å¾Œå…¼å®¹æ–¹æ³•ï¼ˆèˆŠæ¥å£ï¼‰
    # ============================================================
    
    def run_complete_optimization(self, trial_config: Dict[str, int] = None) -> Dict[str, Any]:
        """å‘å¾Œå…¼å®¹ï¼šåŸ·è¡Œå®Œæ•´å„ªåŒ–ï¼ˆé‡æ–°æ˜ å°„åˆ°9å±¤æ¶æ§‹ï¼‰"""
        self.logger.warning("âš ï¸ ä½¿ç”¨èˆŠæ¥å£ï¼Œæ¨è–¦ä½¿ç”¨ run_complete_layered_optimization")
        return self.run_complete_layered_optimization(trial_config)
    
    def quick_optimization(self) -> Dict[str, Any]:
        """å‘å¾Œå…¼å®¹ï¼šå¿«é€Ÿå„ªåŒ–"""
        self.logger.warning("âš ï¸ ä½¿ç”¨èˆŠæ¥å£ï¼Œæ¨è–¦ä½¿ç”¨ quick_complete_optimization")
        return self.quick_complete_optimization()

    # -------------------------------------------------
    # ç›£æ§èˆ‡é©—è­‰å·¥å…·
    # -------------------------------------------------

    def validate_layer_outputs(self, min_score: float = 0.45) -> Dict[str, Dict[str, Any]]:
        """æª¢æŸ¥å„æ™‚æ¡†çš„ Layer1/Layer2 çµæœæ˜¯å¦æ»¿è¶³åˆ†æ•¸èˆ‡ lag ç¯„åœ."""
        issues = {}
        timeframes = self.scaled_config.get('meta_timeframes', [self.timeframe])
        meta_vol = self.scaled_config.get('meta_vol', 0.02)

        for tf in timeframes:
            label_path = self.get_label_config_path(tf)
            feature_path = self.get_feature_config_path(tf)

            if not label_path.exists() or not feature_path.exists():
                continue

            with open(label_path, 'r', encoding='utf-8') as f:
                label_data = json.load(f)
            with open(feature_path, 'r', encoding='utf-8') as f:
                feature_data = json.load(f)

            label_score = label_data.get('best_score', 0)
            feature_score = feature_data.get('best_score', 0)
            lag = label_data.get('best_params', {}).get('lag', 0)

            base_min, base_max = self.scaler.get_base_lag_range(tf)
            _, allowed_max = self.scaler.adjust_lag_range_with_meta(tf, (base_min, base_max), meta_vol)

            tf_issues = {}
            if label_score < min_score:
                tf_issues['label_score'] = label_score
            if feature_score < min_score:
                tf_issues['feature_score'] = feature_score
            if lag > allowed_max:
                tf_issues['lag'] = lag

            if tf_issues:
                tf_issues['allowed_max_lag'] = allowed_max
                issues[tf] = tf_issues

        return issues

    def rerun_layers_if_needed(self, min_score: float = 0.45, retries: int = 1) -> Dict[str, Dict[str, Any]]:
        """è‹¥ score æˆ– lag è¶…é™ï¼Œé‡è·‘ Layer1/Layer2."""
        issues = self.validate_layer_outputs(min_score=min_score)
        if not issues:
            self.logger.info("âœ… æ‰€æœ‰æ™‚æ¡†çµæœç¬¦åˆè¦æ±‚ï¼Œç„¡éœ€é‡è·‘")
            return {}

        self.logger.warning(f"âš ï¸ ç™¼ç¾æ™‚æ¡†çµæœä¸ç¬¦åˆè¦æ±‚: {issues}")
        fixes = {}

        for tf, detail in issues.items():
            self.logger.info(f"ğŸ”„ é‡è·‘ {tf} Layer1/Layer2â€¦")
            for retry in range(retries):
                self.logger.info(f"  â¤ ç¬¬ {retry+1}/{retries} æ¬¡é‡è·‘")
                self.symbol = self.symbol
                self.timeframe = tf
                self.run_layer1_label_optimization(n_trials=75)
                self.run_layer2_feature_optimization(n_trials=75)
                new_issues = self.validate_layer_outputs(min_score=min_score)
                if tf not in new_issues:
                    self.logger.info(f"  âœ… {tf} é‡è·‘å¾Œå·²ç¬¦åˆè¦æ±‚")
                    break
            fixes[tf] = {'original_issues': detail}

        return fixes
    
    # ============================================================
    # é˜¶æ®µ6+7ï¼šç”Ÿå­˜è€…åå·®+ç³»ç»Ÿæ€§åå·®é›†æˆæ–¹æ³•
    # ============================================================
    
    def apply_survivorship_correction_to_results(self, results: Dict) -> Dict:
        """
        åº”ç”¨ç”Ÿå­˜è€…åå·®æ ¡æ­£åˆ°ä¼˜åŒ–ç»“æœï¼ˆé˜¶æ®µ6é›†æˆï¼‰
        
        ä½¿ç”¨æ–¹æ³•ï¼š
        results = coordinator.run_complete_layered_optimization()
        results = coordinator.apply_survivorship_correction_to_results(results)
        
        Args:
            results: ä¼˜åŒ–ç»“æœå­—å…¸
            
        Returns:
            æ·»åŠ äº†ç”Ÿå­˜è€…åå·®æ ¡æ­£çš„ç»“æœå­—å…¸
        """
        if not HAS_SURVIVORSHIP_CORRECTION:
            self.logger.info("â„¹ï¸ ç”Ÿå­˜è€…åå·®æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ ¡æ­£")
            return results
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ”§ åº”ç”¨ç”Ÿå­˜è€…åå·®æ ¡æ­£...")
        
        try:
            # ä»layer_resultsä¸­æå–æ”¶ç›Šç‡åºåˆ—
            layer_results = results.get('layer_results', {})
            
            # å°è¯•ä»ä¸åŒå±‚çº§æå–æ”¶ç›Šç‡
            returns_series = None
            for layer_name in ['layer3_model', 'layer2_features', 'layer1_labels']:
                layer_data = layer_results.get(layer_name, {})
                if 'returns_series' in layer_data:
                    returns_series = layer_data['returns_series']
                    break
            
            if returns_series is None or (isinstance(returns_series, pd.Series) and returns_series.empty):
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æ”¶ç›Šç‡åºåˆ—ï¼Œè·³è¿‡ç”Ÿå­˜è€…åå·®æ ¡æ­£")
                return results
            
            # åº”ç”¨æ ¡æ­£
            correction_result = apply_survivorship_correction(
                returns=returns_series,
                symbol=self.pair,
                timeframe=self.timeframe
            )
            
            # æ·»åŠ æ ¡æ­£ç»“æœ
            results['survivorship_bias_correction'] = {
                'raw_sharpe': correction_result['raw_sharpe'],
                'corrected_sharpe': correction_result['corrected_sharpe'],
                'bias_estimate': correction_result['bias_estimate'],
                'confidence_interval_95': correction_result['confidence_interval'],
                'bootstrap_iterations': correction_result['n_iterations'],
                'failure_events_used': correction_result['n_failure_events']
            }
            
            self.logger.info("âœ… ç”Ÿå­˜è€…åå·®æ ¡æ­£å®Œæˆ")
            self.logger.info(f"   åŸå§‹Sharpe: {correction_result['raw_sharpe']:.4f}")
            self.logger.info(f"   æ ¡æ­£åSharpe: {correction_result['corrected_sharpe']:.4f}")
            self.logger.info(f"   åå·®ä¼°è®¡: {correction_result['bias_estimate']:.4f}")
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿå­˜è€…åå·®æ ¡æ­£å¤±è´¥: {e}")
            self.logger.debug(traceback.format_exc())
        
        return results
    
    def check_distribution_shift_optional(self, train_data: pd.DataFrame, test_data: pd.DataFrame):
        """
        æ£€æŸ¥è®­ç»ƒ/æµ‹è¯•åˆ†å¸ƒåç§»ï¼ˆé˜¶æ®µ7é›†æˆ - å¯é€‰ï¼‰
        
        ä½¿ç”¨å¯¹æŠ—æ€§éªŒè¯æ£€æµ‹æ•°æ®åˆ†å¸ƒå·®å¼‚ã€‚
        
        ä½¿ç”¨æ–¹æ³•ï¼ˆåœ¨Layer0æ•°æ®æ¸…æ´—åï¼‰ï¼š
        coordinator.check_distribution_shift_optional(train_features, test_features)
        
        Args:
            train_data: è®­ç»ƒé›†ç‰¹å¾
            test_data: æµ‹è¯•é›†ç‰¹å¾
        """
        if not HAS_ADVERSARIAL_VALIDATION:
            self.logger.debug("â„¹ï¸ å¯¹æŠ—æ€§éªŒè¯æ¨¡å—ä¸å¯ç”¨")
            return None
        
        self.logger.info("ğŸ” æ‰§è¡Œå¯¹æŠ—æ€§éªŒè¯æ£€æŸ¥...")
        
        try:
            result = quick_adversarial_check(train_data, test_data)
            
            auc = result['cv_auc_mean']
            shift = result['distribution_shift']
            
            self.logger.info(f"   AUC: {auc:.4f}, åˆ†å¸ƒåç§»: {shift}")
            
            if auc > 0.70:
                self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ˜¾è‘—çš„åˆ†å¸ƒåç§»ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åˆ’åˆ†")
            elif auc > 0.60:
                self.logger.info("â„¹ï¸ æ£€æµ‹åˆ°è½»åº¦åˆ†å¸ƒåç§»")
            else:
                self.logger.info("âœ… è®­ç»ƒ/æµ‹è¯•åˆ†å¸ƒç›¸ä¼¼")
            
            return result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ å¯¹æŠ—æ€§éªŒè¯å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•¸"""
    coordinator = OptunaCoordinator()
    result = coordinator.quick_complete_optimization()
    print(f"Layer0+9å±¤å„ªåŒ–å®Œæˆ: {result['version']}")


if __name__ == "__main__":
    main()
