# -*- coding: utf-8 -*-
"""
Layer0: åŠ å¯†è²¨å¹£æ•¸æ“šæ¸…æ´—åƒæ•¸å„ªåŒ–å™¨ (åŸºç¤å±¤)
å°ˆé–€é‡å° OHLCV æ•¸æ“šçš„åŸºç¤æ¸…æ´—å’Œé©—è­‰
å¿…é ˆåœ¨æ‰€æœ‰å…¶ä»–å„ªåŒ–å±¤ä¹‹å‰åŸ·è¡Œ
"""
import json
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import optuna
import pandas as pd
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

from optuna_system.utils.io_utils import write_dataframe, read_dataframe

warnings.filterwarnings('ignore')


class DataCleaningOptimizer:
    """Layer0: åŠ å¯†è²¨å¹£æ•¸æ“šæ¸…æ´—åƒæ•¸å„ªåŒ–å™¨ - åŸºç¤å±¤å„ªåŒ–"""

    def __init__(self, data_path: str, config_path: str = "configs/",
                 symbol: str = "BTCUSDT", timeframe: str = "15m",
                 scaled_config: Dict = None):
        self.data_path = Path(data_path)
        self.config_path = Path(config_path)
        self.config_path.mkdir(exist_ok=True)
        self.symbol = symbol
        self.timeframe = timeframe
        self.scaled_config = scaled_config or {}

        # ä½¿ç”¨é›†ä¸­æ—¥èªŒ (ç”±ä¸Šå±¤/å…¥å£åˆå§‹åŒ–)ï¼Œé¿å…é‡è¤‡ basicConfig
        self.logger = logging.getLogger(__name__)

    def apply_cleaning(self, original_data: pd.DataFrame, **params: Any) -> pd.DataFrame:
        """çµ„è£ä¸¦åŸ·è¡Œå®Œæ•´æ¸…æ´—æµç¨‹ï¼Œå›å‚³æ¸…æ´—å¾Œ DataFrameã€‚"""
        if original_data is None or len(original_data) == 0:
            return pd.DataFrame()
        step1_data = self.check_price_anomalies(original_data, params)
        step2_data = self.check_ohlc_logic(step1_data, params)
        step3_data = self.check_timestamp_continuity(step2_data, params)
        step4_data = self.enhanced_price_cleaning(step3_data, params)
        step5_data = self.check_volume_anomalies(step4_data, params)
        cleaned_data = self.apply_missing_value_treatment(step5_data, params)
        cleaned_data = self.volume_void_detection(cleaned_data, params)
        return cleaned_data

    def apply_transform(self, original_data: pd.DataFrame, **params: Any) -> pd.DataFrame:
        """ç‰©åŒ–ä»‹é¢åˆ¥åï¼Œèˆ‡å…¶ä»–å±¤å°é½Šã€‚"""
        return self.apply_cleaning(original_data, **params)

    def load_raw_ohlcv_data(self) -> pd.DataFrame:
        """åŠ è¼‰åŸå§‹ OHLCV æ•¸æ“š"""
        try:
            cleaned_candidate = self.config_path / f"cleaned_ohlcv_{self.timeframe}.parquet"
            if cleaned_candidate.exists():
                data_df = read_dataframe(cleaned_candidate)
                self.logger.info(f"âœ… ä½¿ç”¨Layer0æ¸…æ´—æ•¸æ“š: {cleaned_candidate}")
                self.logger.info(f"åŸå§‹OHLCVæ•¸æ“š: {data_df.shape}")
                return data_df

            ohlcv_file = self.data_path / "raw" / self.symbol / f"{self.symbol}_{self.timeframe}_ohlcv.parquet"
            self.logger.info(f"ğŸ” æŸ¥æ‰¾OHLCVæ–‡ä»¶: {ohlcv_file.absolute()}")

            if ohlcv_file.exists():
                data_df = read_dataframe(ohlcv_file)
                self.logger.info(f"âœ… åŠ è¼‰åŸå§‹OHLCVæ•¸æ“š: {ohlcv_file}")
            else:
                # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                alternative_paths = [
                    f"data/raw/{self.symbol}/{self.symbol}_{self.timeframe}_ohlcv.parquet",
                    f"../data/raw/{self.symbol}/{self.symbol}_{self.timeframe}_ohlcv.parquet",
                    f"./{self.symbol}_{self.timeframe}_ohlcv.parquet"
                ]
                
                data_df = None
                for alt_path in alternative_paths:
                    if Path(alt_path).exists():
                        self.logger.info(f"ğŸ” æ‰¾åˆ°æ›¿ä»£è·¯å¾„: {alt_path}")
                        data_df = read_dataframe(Path(alt_path))
                        break
                
                if data_df is None:
                    # ç”Ÿæˆæ¨¡æ“¬ OHLCV æ•¸æ“šç”¨æ–¼æ¸¬è©¦
                    self.logger.warning(f"âŒ æœªæ‰¾åˆ°OHLCVæ•¸æ“šæ–‡ä»¶: {ohlcv_file.absolute()}")
                    self.logger.warning("ğŸ”„ ç”Ÿæˆæ¨¡æ“¬æ•¸æ“šç”¨æ–¼æ¸¬è©¦")
                    data_df = self._generate_mock_ohlcv_data()
                else:
                    self.logger.info(f"âœ… æˆåŠŸåŠ è¼‰OHLCVæ•¸æ“š: {data_df.shape}")

            self.logger.info(f"åŸå§‹OHLCVæ•¸æ“š: {data_df.shape}")
            return data_df

        except Exception as e:
            self.logger.error(f"OHLCVæ•¸æ“šåŠ è¼‰å¤±æ•—: {e}")
            return pd.DataFrame()

    def _generate_mock_ohlcv_data(self) -> pd.DataFrame:
        """ç”ŸæˆåŒ…å«ç•°å¸¸å€¼çš„æ¨¡æ“¬OHLCVæ•¸æ“š"""
        np.random.seed(42)
        n_samples = 2000

        # ç”ŸæˆåŸºç¤åƒ¹æ ¼åºåˆ—
        base_price = 50000  # BTCåŸºç¤åƒ¹æ ¼
        price_changes = np.random.normal(0, 0.02, n_samples)  # 2%æ¨™æº–å·®
        prices = [base_price]

        for change in price_changes:
            new_price = prices[-1] * (1 + change)
            prices.append(max(new_price, 100))  # ç¢ºä¿åƒ¹æ ¼ä¸æœƒå¤ªä½

        prices = np.array(prices[1:])

        # ç”Ÿæˆ OHLCV æ•¸æ“š
        data = []
        for i, close in enumerate(prices):
            # æ­£å¸¸æƒ…æ³ä¸‹çš„ OHLC é—œä¿‚
            volatility = np.random.uniform(0.005, 0.03)  # 0.5%-3%æ³¢å‹•

            high = close * (1 + np.random.uniform(0, volatility))
            low = close * (1 - np.random.uniform(0, volatility))
            open_price = low + (high - low) * np.random.uniform(0.2, 0.8)

            # ç¢ºä¿ OHLC é‚è¼¯æ­£ç¢º
            high = max(high, open_price, close)
            low = min(low, open_price, close)

            # æ­£å¸¸æˆäº¤é‡
            volume = np.random.uniform(100, 10000)

            data.append({
                'open': open_price,
                'high': high,
                'low': low,
                'close': close,
                'volume': volume
            })

        # æ·»åŠ ä¸€äº›ç•°å¸¸æ•¸æ“šä¾†æ¸¬è©¦æ¸…æ´—å™¨
        anomaly_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)

        for idx in anomaly_indices:
            if idx < len(data):
                anomaly_type = np.random.choice(['zero_price', 'extreme_price', 'ohlc_violation', 'zero_volume'])

                if anomaly_type == 'zero_price':
                    # åƒ¹æ ¼ç‚º0çš„ç•°å¸¸
                    data[idx]['close'] = 0
                elif anomaly_type == 'extreme_price':
                    # æ¥µç«¯åƒ¹æ ¼ç•°å¸¸
                    data[idx]['close'] *= np.random.choice([100, 0.01])
                elif anomaly_type == 'ohlc_violation':
                    # OHLCé—œä¿‚éŒ¯èª¤
                    data[idx]['high'] = data[idx]['low'] * 0.5  # high < low
                elif anomaly_type == 'zero_volume':
                    # æˆäº¤é‡ç‚º0
                    data[idx]['volume'] = 0

        # å‰µå»º DataFrame
        dates = pd.date_range('2022-01-01', periods=n_samples, freq='15min')
        df = pd.DataFrame(data, index=dates)

        # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
        missing_mask = np.random.random(len(df)) < 0.005  # 0.5%ç¼ºå¤±
        df.loc[missing_mask, 'volume'] = np.nan

        return df

    def check_price_anomalies(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æª¢æŸ¥å’Œè™•ç†åƒ¹æ ¼ç•°å¸¸"""
        try:
            cleaned_data = data.copy()

            # 1. ç§»é™¤åƒ¹æ ¼ç‚º0æˆ–è² æ•¸çš„è¨˜éŒ„
            price_cols = ['open', 'high', 'low', 'close']
            for col in price_cols:
                if col in cleaned_data.columns:
                    # æ¨™è¨˜ç•°å¸¸å€¼
                    zero_mask = (cleaned_data[col] <= 0)
                    if zero_mask.sum() > 0:
                        self.logger.warning(f"ç™¼ç¾ {zero_mask.sum()} å€‹ {col} åƒ¹æ ¼ <= 0 çš„ç•°å¸¸å€¼")

                        # è™•ç†æ–¹å¼æ ¹æ“šåƒæ•¸æ±ºå®š
                        if params.get('zero_price_action', 'drop') == 'drop':
                            cleaned_data = cleaned_data[~zero_mask]
                        elif params.get('zero_price_action', 'drop') == 'forward_fill':
                            cleaned_data.loc[zero_mask, col] = np.nan
                            cleaned_data[col] = cleaned_data[col].ffill()

            # 2. ç§»é™¤æ¥µç«¯åƒ¹æ ¼ç•°å¸¸ï¼ˆåŸºæ–¼ç§»å‹•ä¸­ä½æ•¸ï¼‰
            price_change_threshold = params.get('extreme_price_threshold', 10.0)

            for col in price_cols:
                if col in cleaned_data.columns and len(cleaned_data) > 10:
                    # è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–ç‡
                    price_change = cleaned_data[col].pct_change().abs()

                    # åŸºæ–¼æ»¾å‹•ä¸­ä½æ•¸çš„ç•°å¸¸æª¢æ¸¬
                    rolling_median = price_change.rolling(window=20, min_periods=5).median()
                    extreme_mask = price_change > (rolling_median * price_change_threshold)

                    if extreme_mask.sum() > 0:
                        self.logger.warning(f"ç™¼ç¾ {extreme_mask.sum()} å€‹ {col} æ¥µç«¯åƒ¹æ ¼è®ŠåŒ–")

                        if params.get('extreme_price_action', 'cap') == 'cap':
                            # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
                            max_change = rolling_median * price_change_threshold
                            prev_prices = cleaned_data[col].shift(1)

                            # å‘ä¸Šèª¿æ•´
                            up_mask = extreme_mask & (cleaned_data[col] > prev_prices)
                            cleaned_data.loc[up_mask, col] = prev_prices[up_mask] * (1 + max_change[up_mask])

                            # å‘ä¸‹èª¿æ•´
                            down_mask = extreme_mask & (cleaned_data[col] < prev_prices)
                            cleaned_data.loc[down_mask, col] = prev_prices[down_mask] * (1 - max_change[down_mask])

            return cleaned_data

        except Exception as e:
            self.logger.error(f"åƒ¹æ ¼ç•°å¸¸æª¢æŸ¥å¤±æ•—: {e}")
            return data

    def check_ohlc_logic(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æª¢æŸ¥å’Œä¿®å¾©OHLCé‚è¼¯é—œä¿‚"""
        try:
            cleaned_data = data.copy()

            # OHLCé‚è¼¯æª¢æŸ¥: high >= max(open, close), low <= min(open, close)
            if all(col in cleaned_data.columns for col in ['open', 'high', 'low', 'close']):

                # æª¢æŸ¥ high æ˜¯å¦çœŸçš„æ˜¯æœ€é«˜åƒ¹
                max_oc = np.maximum(cleaned_data['open'], cleaned_data['close'])
                high_violation = cleaned_data['high'] < max_oc

                # æª¢æŸ¥ low æ˜¯å¦çœŸçš„æ˜¯æœ€ä½åƒ¹
                min_oc = np.minimum(cleaned_data['open'], cleaned_data['close'])
                low_violation = cleaned_data['low'] > min_oc

                violations = high_violation | low_violation

                if violations.sum() > 0:
                    self.logger.warning(f"ç™¼ç¾ {violations.sum()} å€‹OHLCé‚è¼¯éŒ¯èª¤")

                    if params.get('ohlc_fix_method', 'adjust') == 'adjust':
                        # èª¿æ•´ high å’Œ low ä»¥ç¬¦åˆé‚è¼¯
                        cleaned_data.loc[high_violation, 'high'] = max_oc[high_violation]
                        cleaned_data.loc[low_violation, 'low'] = min_oc[low_violation]

                    elif params.get('ohlc_fix_method', 'adjust') == 'drop':
                        # ç§»é™¤é‚è¼¯éŒ¯èª¤çš„è¨˜éŒ„
                        cleaned_data = cleaned_data[~violations]

            return cleaned_data

        except Exception as e:
            self.logger.error(f"OHLCé‚è¼¯æª¢æŸ¥å¤±æ•—: {e}")
            return data

    def check_timestamp_continuity(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æª¢æŸ¥æ™‚é–“æˆ³é€£çºŒæ€§"""
        try:
            if not isinstance(data.index, pd.DatetimeIndex):
                self.logger.warning("ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œè·³éæ™‚é–“é€£çºŒæ€§æª¢æŸ¥")
                return data

            # è¨ˆç®—æ™‚é–“é–“éš”
            time_diffs = data.index.to_series().diff()
            expected_freq = pd.Timedelta(self.timeframe)

            # æ‰¾å‡ºæ™‚é–“é–“éš”ç•°å¸¸çš„ä½ç½®
            gap_threshold = expected_freq * params.get('timestamp_gap_multiplier', 5.0)
            large_gaps = time_diffs > gap_threshold

            if large_gaps.sum() > 0:
                self.logger.warning(f"ç™¼ç¾ {large_gaps.sum()} å€‹æ™‚é–“æˆ³é–“éš”ç•°å¸¸")

            return data

        except Exception as e:
            self.logger.error(f"æ™‚é–“æˆ³é€£çºŒæ€§æª¢æŸ¥å¤±æ•—: {e}")
            return data

    def check_volume_anomalies(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æª¢æŸ¥æˆäº¤é‡ç•°å¸¸"""
        try:
            cleaned_data = data.copy()

            if 'volume' not in cleaned_data.columns:
                return cleaned_data

            # 1. è™•ç†æˆäº¤é‡ç‚º0æˆ–è² æ•¸
            zero_volume_mask = cleaned_data['volume'] <= 0
            if zero_volume_mask.sum() > 0:
                self.logger.warning(f"ç™¼ç¾ {zero_volume_mask.sum()} å€‹æˆäº¤é‡ <= 0 çš„è¨˜éŒ„")

                if params.get('zero_volume_action', 'fill_median') == 'fill_median':
                    # ç”¨ä¸­ä½æ•¸å¡«å……
                    median_volume = cleaned_data['volume'][cleaned_data['volume'] > 0].median()
                    cleaned_data.loc[zero_volume_mask, 'volume'] = median_volume
                elif params.get('zero_volume_action', 'fill_median') == 'drop':
                    cleaned_data = cleaned_data[~zero_volume_mask]

            # 2. æª¢æŸ¥æ¥µç«¯æˆäº¤é‡
            if len(cleaned_data) > 0:
                volume_median = cleaned_data['volume'].median()
                volume_std = cleaned_data['volume'].std()

                # åŸºæ–¼ä¸­ä½æ•¸å’Œæ¨™æº–å·®çš„ç•°å¸¸æª¢æ¸¬
                extreme_threshold = params.get('volume_extreme_multiplier', 5.0)
                extreme_volume_mask = cleaned_data['volume'] > (volume_median + extreme_threshold * volume_std)

                if extreme_volume_mask.sum() > 0:
                    self.logger.warning(f"ç™¼ç¾ {extreme_volume_mask.sum()} å€‹æ¥µç«¯æˆäº¤é‡")

                    if params.get('extreme_volume_action', 'cap') == 'cap':
                        # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
                        max_volume = volume_median + extreme_threshold * volume_std
                        cleaned_data.loc[extreme_volume_mask, 'volume'] = max_volume

            return cleaned_data

        except Exception as e:
            self.logger.error(f"æˆäº¤é‡ç•°å¸¸æª¢æŸ¥å¤±æ•—: {e}")
            return data

    def enhanced_price_cleaning(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """å¼·åŒ–ç‰ˆåƒ¹æ ¼æ¸…æ´—ï¼šWinsorize + è·³è®Šæª¢æ¸¬"""
        try:
            cleaned = data.copy()
            price_cols = [col for col in ['open', 'high', 'low', 'close'] if col in cleaned.columns]
            if not price_cols:
                return cleaned

            window = max(20, int(params.get('winsorize_window', 200)))
            lower_q = float(params.get('winsorize_lower', 0.01))
            upper_q = float(params.get('winsorize_upper', 0.99))
            min_periods = max(5, window // 5)

            for col in price_cols:
                rolling_lower = cleaned[col].rolling(window, min_periods=min_periods).quantile(lower_q)
                rolling_upper = cleaned[col].rolling(window, min_periods=min_periods).quantile(upper_q)

                global_lower = cleaned[col].quantile(lower_q)
                global_upper = cleaned[col].quantile(upper_q)

                cleaned[col] = cleaned[col].clip(
                    lower=rolling_lower.fillna(global_lower),
                    upper=rolling_upper.fillna(global_upper)
                )

            if 'close' not in cleaned.columns:
                return cleaned

            returns = cleaned['close'].pct_change()
            jump_threshold = float(params.get('jump_threshold', 0.05))
            jump_mask = returns.abs() > jump_threshold

            if jump_mask.any():
                jump_indices = cleaned.index[jump_mask.fillna(False)]
                for idx in jump_indices:
                    if idx == cleaned.index[0] or idx == cleaned.index[-1]:
                        continue
                    prev_idx = cleaned.index.get_loc(idx) - 1
                    next_idx = cleaned.index.get_loc(idx) + 1
                    if prev_idx < 0 or next_idx >= len(cleaned):
                        continue
                    prev_price = cleaned.iloc[prev_idx]['close']
                    next_price = cleaned.iloc[next_idx]['close']
                    if prev_price <= 0 or next_price <= 0:
                        continue
                    cleaned.at[idx, 'close'] = np.sqrt(prev_price * next_price)

            return cleaned

        except Exception as e:
            self.logger.warning(f"å¼·åŒ–åƒ¹æ ¼æ¸…æ´—å¤±æ•—: {e}")
            return data

    def volume_void_detection(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æˆäº¤é‡ç©ºæ´æª¢æ¸¬èˆ‡å¡«è£œ"""
        try:
            if 'volume' not in data.columns or 'close' not in data.columns:
                return data

            cleaned = data.copy()
            vol_window = max(10, int(params.get('vol_window', 50)))
            void_threshold = float(params.get('void_threshold', 0.1))
            vol_multiplier = float(params.get('vol_vol_multiplier', 5.0))

            vol_ma = cleaned['volume'].rolling(vol_window, min_periods=1).mean()
            void_mask = cleaned['volume'] < (vol_ma * void_threshold)

            if void_mask.any():
                price_volatility = cleaned['close'].pct_change().abs().rolling(vol_window, min_periods=1).mean()
                estimated_volume = (vol_ma * (1 + price_volatility.fillna(0) * vol_multiplier)).fillna(vol_ma)
                cleaned.loc[void_mask, 'volume'] = estimated_volume[void_mask]

            return cleaned

        except Exception as e:
            self.logger.warning(f"æˆäº¤é‡ç©ºæ´æª¢æ¸¬å¤±æ•—: {e}")
            return data

    def evaluate_ohlcv_cleaning_effectiveness(self, original: pd.DataFrame, cleaned: pd.DataFrame) -> Dict:
        """è©•ä¼°OHLCVæ•¸æ“šæ¸…æ´—æ•ˆæœ"""
        try:
            metrics = {}

            # æ•¸æ“šä¿ç•™ç‡
            metrics['data_retention_rate'] = len(cleaned) / max(len(original), 1)

            # ç•°å¸¸å€¼ç§»é™¤æ•ˆæœ
            if all(col in original.columns for col in ['open', 'high', 'low', 'close']):
                # åƒ¹æ ¼ç•°å¸¸æª¢æŸ¥
                original_zero_prices = (original[['open', 'high', 'low', 'close']] <= 0).sum().sum()
                cleaned_zero_prices = (cleaned[['open', 'high', 'low', 'close']] <= 0).sum().sum()
                metrics['zero_prices_removed'] = original_zero_prices - cleaned_zero_prices

                # OHLCé‚è¼¯ä¸€è‡´æ€§
                if len(cleaned) > 0:
                    max_oc = np.maximum(cleaned['open'], cleaned['close'])
                    min_oc = np.minimum(cleaned['open'], cleaned['close'])

                    ohlc_consistent = ((cleaned['high'] >= max_oc) & (cleaned['low'] <= min_oc)).mean()
                    metrics['ohlc_consistency_rate'] = ohlc_consistent
                else:
                    metrics['ohlc_consistency_rate'] = 0

            # æˆäº¤é‡è³ªé‡
            if 'volume' in original.columns and 'volume' in cleaned.columns:
                original_zero_volume = (original['volume'] <= 0).sum()
                cleaned_zero_volume = (cleaned['volume'] <= 0).sum()
                metrics['zero_volume_removed'] = original_zero_volume - cleaned_zero_volume

            # æ•¸æ“šå®Œæ•´æ€§
            original_missing = original.isnull().sum().sum()
            cleaned_missing = cleaned.isnull().sum().sum()
            metrics['missing_data_handled'] = original_missing - cleaned_missing

            return metrics

        except Exception as e:
            self.logger.error(f"OHLCVæ¸…æ´—æ•ˆæœè©•ä¼°å¤±æ•—: {e}")
            return {'error': str(e)}

    def apply_missing_value_treatment(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ğŸš€ ä¿®å¾©ç‰ˆï¼šæ™‚é–“åºåˆ—å°å‘çš„ç¼ºå¤±å€¼è™•ç†"""
        try:
            method = params['impute_method']

            # çµ±è¨ˆåŸå§‹ç¼ºå¤±æ¯”ä¾‹
            total_values = data.size
            missing_before = data.isnull().sum().sum()
            missing_ratio_before = missing_before / total_values if total_values > 0 else 0

            self.logger.info(f"ğŸ“Š åŸå§‹ç¼ºå¤±å€¼: {missing_before:,}/{total_values:,} ({missing_ratio_before:.2%})")

            if method == 'drop':
                # åˆªé™¤ç¼ºå¤±å€¼ï¼ˆä¿ç•™åŸé‚è¼¯ï¼‰
                threshold = params.get('missing_threshold', 0.1)
                missing_ratio = data.isnull().sum() / len(data)
                cols_to_keep = missing_ratio[missing_ratio <= threshold].index
                cleaned_data = data[cols_to_keep].dropna()

            elif method in ['forward_fill', 'ffill']:
                # ğŸš€ ä¿®å¾©ç‰ˆï¼šæ™‚é–“åºåˆ—å„ªå…ˆå¡«è£œç­–ç•¥
                # ç¬¬ä¸€æ­¥ï¼šå‰å‘å¡«å……ï¼ˆä¿æŒæ™‚é–“åºåˆ—é€£çºŒæ€§ï¼‰
                cleaned_data = data.ffill()

                # ç¬¬äºŒæ­¥ï¼šæ»¾å‹•ä¸­ä½æ•¸å¡«è£œå‰©é¤˜ç¼ºå¤±
                window = max(params.get('impute_window', 20), 10)  # ç¢ºä¿çª—å£è¶³å¤ å¤§

                for col in cleaned_data.columns:
                    remaining_missing = cleaned_data[col].isnull()
                    if remaining_missing.any():
                        # æ»¾å‹•ä¸­ä½æ•¸å¡«è£œ
                        rolling_median = data[col].rolling(window=window, min_periods=3).median()
                        cleaned_data.loc[remaining_missing, col] = rolling_median[remaining_missing]

                        # æœ€å¾Œå‚™é¸ï¼šå…¨å±€ä¸­ä½æ•¸
                        still_missing = cleaned_data[col].isnull()
                        if still_missing.any():
                            global_median = data[col].median()
                            if not pd.isna(global_median):
                                cleaned_data.loc[still_missing, col] = global_median

                self.logger.info(f"âœ… æ™‚é–“åºåˆ—å¡«è£œ: ffill + æ»¾å‹•ä¸­ä½æ•¸ (çª—å£={window})")

            elif method == 'median':
                # ğŸš€ ä¿®å¾©ç‰ˆï¼šæ™‚é–“åºåˆ—å‹å¥½çš„ä¸­ä½æ•¸å¡«å……
                cleaned_data = data.ffill()  # å…ˆå‰å‘å¡«å……

                # å†ç”¨æ»¾å‹•ä¸­ä½æ•¸å¡«å……å‰©é¤˜
                window = params.get('impute_window', 20)
                for col in cleaned_data.columns:
                    remaining_missing = cleaned_data[col].isnull()
                    if remaining_missing.any():
                        rolling_median = data[col].rolling(window=window, min_periods=3).median()
                        cleaned_data.loc[remaining_missing, col] = rolling_median[remaining_missing]

                # æœ€çµ‚å…¨å±€ä¸­ä½æ•¸å¡«å……
                for col in cleaned_data.columns:
                    global_median = data[col].median()
                    if not pd.isna(global_median):
                        cleaned_data[col] = cleaned_data[col].fillna(global_median)

                self.logger.info(f"âœ… æ™‚é–“åºåˆ—ä¸­ä½æ•¸å¡«è£œ (æ»¾å‹•çª—å£={window})")

            elif method == 'rolling_mean':
                # ğŸš€ ä¿®å¾©ç‰ˆï¼šå…ˆffillå†æ»¾å‹•å‡å€¼
                cleaned_data = data.ffill()
                window = params.get('impute_window', 20)

                for col in cleaned_data.columns:
                    remaining_missing = cleaned_data[col].isnull()
                    if remaining_missing.any():
                        rolling_mean = data[col].rolling(window=window, min_periods=3).mean()
                        cleaned_data.loc[remaining_missing, col] = rolling_mean[remaining_missing]

                self.logger.info(f"âœ… ffill + æ»¾å‹•å‡å€¼å¡«è£œ (çª—å£={window})")

            elif method == 'knn':
                # KNNå¡«å……ï¼ˆä¿ç•™ï¼Œä½†æé†’æ™‚é–“åºåˆ—é¢¨éšªï¼‰
                n_neighbors = params.get('knn_neighbors', 5)
                imputer = KNNImputer(n_neighbors=n_neighbors)
                filled_values = imputer.fit_transform(data)
                cleaned_data = pd.DataFrame(filled_values, columns=data.columns, index=data.index)
                self.logger.warning("âš ï¸ KNNå¡«è£œå¯èƒ½ç ´å£æ™‚é–“åºåˆ—ç‰¹æ€§")

            else:
                # ğŸš€ é»˜èªï¼šæ™‚é–“åºåˆ—æœ€å„ªç­–ç•¥
                cleaned_data = data.ffill()  # å‰å‘å¡«å……
                remaining_missing = cleaned_data.isnull().sum().sum()
                if remaining_missing > 0:
                    # æ»¾å‹•ä¸­ä½æ•¸å¡«å……å‰©é¤˜
                    for col in cleaned_data.columns:
                        rolling_median = data[col].rolling(window=20, min_periods=3).median()
                        cleaned_data[col] = cleaned_data[col].fillna(rolling_median)

                self.logger.info("âœ… é»˜èªæ™‚é–“åºåˆ—å¡«è£œ: ffill + æ»¾å‹•ä¸­ä½æ•¸")

            # çµ±è¨ˆå¡«è£œæ•ˆæœ
            missing_after = cleaned_data.isnull().sum().sum()
            missing_ratio_after = missing_after / cleaned_data.size if cleaned_data.size > 0 else 0
            fill_success_rate = (missing_before - missing_after) / missing_before if missing_before > 0 else 1.0

            self.logger.info(f"ğŸ“ˆ å¡«è£œæ•ˆæœ: {missing_after:,}å‰©é¤˜ ({missing_ratio_after:.2%}), æˆåŠŸç‡={fill_success_rate:.1%}")

            # æœ€çµ‚ä¿éšœï¼šç¢ºä¿ç„¡ç¼ºå¤±å€¼ï¼ˆé¿å…å°åƒ¹æ ¼æ¬„ä½ç”¨0å¡«è£œï¼‰
            if missing_after > 0:
                price_cols = [c for c in ['open', 'high', 'low', 'close'] if c in cleaned_data.columns]
                non_price_cols = [c for c in cleaned_data.columns if c not in price_cols]

                # åƒ¹æ ¼æ¬„ä½ï¼šå†æ¬¡å‰å‘å¡«è£œï¼›å¦‚ä»æœ‰ç¼ºå¤±ï¼Œç”¨é¦–å€‹æœ‰æ•ˆå€¼å¡«è£œé–‹é ­ç¼ºå¤±
                if price_cols:
                    cleaned_data[price_cols] = cleaned_data[price_cols].ffill()
                    for col in price_cols:
                        if cleaned_data[col].isnull().any():
                            series = cleaned_data[col]
                            first_valid = series.dropna().iloc[0] if series.dropna().size > 0 else None
                            if first_valid is not None:
                                cleaned_data[col] = series.fillna(first_valid)
                            else:
                                # ç„¡æœ‰æ•ˆå€¼æ™‚ï¼Œä¿ç•™ç‚ºNaNä¸¦è¨˜éŒ„
                                self.logger.warning(f"âš ï¸ ç„¡å¯ç”¨åƒ¹æ ¼å¡«è£œå€¼: {col} ä»æœ‰ç¼ºå¤±")

                # éåƒ¹æ ¼æ¬„ä½ï¼šå…è¨±0å¡«è£œä½œç‚ºæœ€å¾Œå…œåº•
                if non_price_cols:
                    residual_missing = cleaned_data[non_price_cols].isnull().sum().sum()
                    if residual_missing > 0:
                        cleaned_data[non_price_cols] = cleaned_data[non_price_cols].fillna(0)
                        self.logger.warning(f"âš ï¸ å°éåƒ¹æ ¼æ¬„ä½ç”¨0å¡«è£œäº†{int(residual_missing)}å€‹é ‘å›ºç¼ºå¤±å€¼")

            return cleaned_data

        except Exception as e:
            self.logger.error(f"ç¼ºå¤±å€¼è™•ç†å¤±æ•—: {e}")
            # å®‰å…¨å‚™é¸æ–¹æ¡ˆï¼šåƒ¹æ ¼æ¬„ä½åƒ…ffillï¼Œéåƒ¹æ ¼æ¬„ä½å…è¨±0å…œåº•
            fallback = data.ffill()
            price_cols = [c for c in ['open', 'high', 'low', 'close'] if c in fallback.columns]
            non_price_cols = [c for c in fallback.columns if c not in price_cols]
            if price_cols:
                for col in price_cols:
                    if fallback[col].isnull().any():
                        series = fallback[col]
                        first_valid = series.dropna().iloc[0] if series.dropna().size > 0 else None
                        if first_valid is not None:
                            fallback[col] = series.fillna(first_valid)
            if non_price_cols:
                fallback[non_price_cols] = fallback[non_price_cols].fillna(0)
            return fallback

    def apply_outlier_treatment(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æ‡‰ç”¨ç•°å¸¸å€¼è™•ç†"""
        try:
            method = params['outlier_method']

            if method == 'iqr':
                # ğŸš€ ä¿®å¾©ç‰ˆï¼šæ»¾å‹•çª—å£IQRæ–¹æ³•ï¼ˆé¿å…æ¥µç«¯æ®µè½è™•ç†ä¸ç•¶ï¼‰
                multiplier = params.get('iqr_multiplier', 1.5)
                window = params.get('rolling_outlier_window', 1000)  # æ»¾å‹•çª—å£å¤§å°

                cleaned_data = data.copy()

                for col in data.columns:
                    # åŸºæ–¼æ»¾å‹•çª—å£è¨ˆç®—å‹•æ…‹å››åˆ†ä½æ•¸
                    rolling_q1 = data[col].rolling(window=window, min_periods=50).quantile(0.25)
                    rolling_q3 = data[col].rolling(window=window, min_periods=50).quantile(0.75)
                    rolling_iqr = rolling_q3 - rolling_q1

                    # å‹•æ…‹é‚Šç•Œ
                    lower_bound = rolling_q1 - multiplier * rolling_iqr
                    upper_bound = rolling_q3 + multiplier * rolling_iqr

                    # æ¨™è¨˜ä¸¦ä¿®å¾©ç•°å¸¸å€¼
                    outlier_mask = (data[col] < lower_bound) | (data[col] > upper_bound)
                    outlier_count = outlier_mask.sum()

                    if outlier_count > 0:
                        # è£å‰ªåˆ°å‹•æ…‹é‚Šç•Œ
                        cleaned_data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)
                        self.logger.info(f"  {col}: ä¿®å¾©{outlier_count}å€‹æ»¾å‹•IQRç•°å¸¸å€¼")

                self.logger.info(f"âœ… æ»¾å‹•çª—å£IQRç•°å¸¸å€¼æª¢æ¸¬å®Œæˆ (çª—å£={window})")

            elif method == 'zscore':
                # ğŸš€ ä¿®å¾©ç‰ˆï¼šæ»¾å‹•çª—å£Z-scoreæ–¹æ³•ï¼ˆé¿å…æ¥µç«¯æ®µè½è™•ç†ä¸ç•¶ï¼‰
                threshold = params.get('zscore_threshold', 3.0)
                window = params.get('rolling_outlier_window', 1000)  # æ»¾å‹•çª—å£å¤§å°

                cleaned_data = data.copy()

                for col in data.columns:
                    # åŸºæ–¼æ»¾å‹•çª—å£è¨ˆç®—å‹•æ…‹çµ±è¨ˆ
                    rolling_median = data[col].rolling(window=window, min_periods=50).median()
                    rolling_std = data[col].rolling(window=window, min_periods=50).std()

                    # è¨ˆç®—æ»¾å‹•Z-score
                    z_scores = np.abs((data[col] - rolling_median) / (rolling_std + 1e-8))

                    # æ¨™è¨˜ç•°å¸¸å€¼
                    outlier_mask = z_scores > threshold
                    outlier_count = outlier_mask.sum()

                    if outlier_count > 0:
                        # ç”¨æ»¾å‹•ä¸­ä½æ•¸æ›¿æ›ç•°å¸¸å€¼
                        cleaned_data.loc[outlier_mask, col] = rolling_median[outlier_mask]
                        self.logger.info(f"  {col}: ä¿®å¾©{outlier_count}å€‹æ»¾å‹•Z-scoreç•°å¸¸å€¼")

                self.logger.info(f"âœ… æ»¾å‹•çª—å£ç•°å¸¸å€¼æª¢æ¸¬å®Œæˆ (çª—å£={window})")

            elif method == 'percentile':
                # ç™¾åˆ†ä½æ•¸è£å‰ª
                lower_pct = params.get('lower_percentile', 5)
                upper_pct = params.get('upper_percentile', 95)

                lower_bound = data.quantile(lower_pct / 100)
                upper_bound = data.quantile(upper_pct / 100)

                cleaned_data = data.clip(lower=lower_bound, upper=upper_bound, axis=1)

            elif method == 'none':
                # ä¸è™•ç†ç•°å¸¸å€¼
                cleaned_data = data

            else:
                # é»˜èªIQRæ–¹æ³•
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                cleaned_data = data.clip(lower=lower_bound, upper=upper_bound, axis=1)

            return cleaned_data

        except Exception as e:
            self.logger.error(f"ç•°å¸¸å€¼è™•ç†å¤±æ•—: {e}")
            return data

    def apply_smoothing(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """æ‡‰ç”¨æ•¸æ“šå¹³æ»‘"""
        try:
            method = params.get('smooth_method', 'none')
            smooth_min = max(1, int(self.scaled_config.get('cleaning_smooth_window_min', 5)))

            if method == 'rolling_mean':
                smooth_window = params.get('smooth_window', smooth_min)
                smoothed_data = data.rolling(window=smooth_window, min_periods=1).mean()

            elif method == 'ewm':
                alpha = params.get('exp_alpha', 0.3)
                smoothed_data = data.ewm(alpha=alpha).mean()

            elif method == 'savgol':
                # ç°¡åŒ–çš„Savitzky-Golayæ¿¾æ³¢ï¼ˆä½¿ç”¨æ»¾å‹•å¤šé …å¼è¿‘ä¼¼ï¼‰
                smooth_window = params.get('smooth_window', smooth_min)
                smoothed_data = data.rolling(window=smooth_window, min_periods=1).apply(
                    lambda x: np.polyval(np.polyfit(range(len(x)), x, 2), len(x)//2), raw=True
                )

            elif method == 'none':
                smoothed_data = data

            else:
                # é»˜èªä¸å¹³æ»‘
                smoothed_data = data

            return smoothed_data

        except Exception as e:
            self.logger.error(f"æ•¸æ“šå¹³æ»‘å¤±æ•—: {e}")
            return data

    def evaluate_cleaning_quality(self, original: pd.DataFrame, cleaned: pd.DataFrame) -> Dict:
        """è©•ä¼°æ•¸æ“šæ¸…æ´—è³ªé‡"""
        try:
            # æ•¸æ“šå®Œæ•´æ€§
            original_missing = original.isnull().sum().sum()
            cleaned_missing = cleaned.isnull().sum().sum()
            completeness_score = 1 - (cleaned_missing / max(original.size, 1))

            # æ•¸æ“šä¿çœŸåº¦ï¼ˆé€šéé‡æ§‹èª¤å·®è¡¡é‡ï¼‰
            common_cols = original.columns.intersection(cleaned.columns)
            if len(common_cols) > 0:
                original_subset = original[common_cols].fillna(0)
                cleaned_subset = cleaned[common_cols].fillna(0)

                # è¨ˆç®—æ¨™æº–åŒ–å¾Œçš„MSE
                scaler = StandardScaler()
                original_scaled = scaler.fit_transform(original_subset)
                cleaned_scaled = scaler.transform(cleaned_subset)

                mse = mean_squared_error(original_scaled, cleaned_scaled)
                fidelity_score = np.exp(-mse)  # è½‰æ›ç‚º0-1åˆ†æ•¸
            else:
                fidelity_score = 0

            # æ•¸æ“šç©©å®šæ€§ï¼ˆæ–¹å·®è®ŠåŒ–ï¼‰
            original_var = original.var().mean()
            cleaned_var = cleaned.var().mean()

            if original_var > 0:
                variance_ratio = cleaned_var / original_var
                stability_score = 1 - abs(1 - variance_ratio)  # æ¥è¿‘1çš„æ¯”ä¾‹å¾—é«˜åˆ†
            else:
                stability_score = 0

            return {
                'completeness_score': completeness_score,
                'fidelity_score': fidelity_score,
                'stability_score': stability_score,
                'data_reduction': (original.size - cleaned.size) / original.size,
                'missing_reduction': (original_missing - cleaned_missing) / max(original_missing, 1)
            }

        except Exception as e:
            self.logger.error(f"æ¸…æ´—è³ªé‡è©•ä¼°å¤±æ•—: {e}")
            return {
                'completeness_score': 0,
                'fidelity_score': 0,
                'stability_score': 0,
                'data_reduction': 0,
                'missing_reduction': 0
            }

    def objective(self, trial: optuna.Trial) -> float:
        """ä¿®å¤ç‰ˆï¼šæ”¯æŒä¸åç»­å±‚è”åŠ¨çš„æ¸…æ´—å‚æ•°ä¼˜åŒ–"""
        
        # ğŸš€ æ–°å¢ï¼šè¯»å–Layer1å‚æ•°å®ç°å¹³æ»‘çª—å£è”åŠ¨
        layer1_lookback = 500  # é»˜è®¤å€¼
        try:
            label_config = self.config_path / "label_params.json"
            if label_config.exists():
                with open(label_config, 'r', encoding='utf-8') as f:
                    layer1_result = json.load(f)
                    layer1_lookback = layer1_result.get('best_params', {}).get('lookback_window', 500)
                    self.logger.info(f"ğŸ”— Layer0è¯»å–Layer1 lookback_window: {layer1_lookback}")
        except Exception as e:
            self.logger.warning(f"æ— æ³•è¯»å–Layer1å‚æ•°: {e}")

        window_min = self.scaled_config.get('cleaning_impute_window', 10)
        smooth_min = self.scaled_config.get('cleaning_smooth_window_min', 5)
        smooth_max = self.scaled_config.get('cleaning_smooth_window_max', 20)
        params = {
            # ç¼ºå¤±å€¼è™•ç†
            'impute_method': trial.suggest_categorical('impute_method',
                                                     ['forward_fill', 'mean', 'median', 'knn', 'rolling_mean']),
            'impute_window': trial.suggest_int('impute_window', max(3, window_min // 2), max(30, window_min * 2)),
            'missing_threshold': trial.suggest_float('missing_threshold', 0.05, 0.3),
            'knn_neighbors': trial.suggest_int('knn_neighbors', 3, 10),

            # âœ… å„ªåŒ–ï¼šåƒ¹æ ¼æ¸…æ´—ï¼ˆé‡å°åŠ å¯†è²¨å¹£å¤§å¹…æ³¢å‹•ï¼‰
            'winsorize_window': trial.suggest_int('winsorize_window', 80, 300),
            'winsorize_lower': trial.suggest_float('winsorize_lower', 0.001, 0.02),
            'winsorize_upper': trial.suggest_float('winsorize_upper', 0.98, 0.999),
            'jump_threshold': trial.suggest_float('jump_threshold', 0.05, 0.12),  # æ”¾å¯¬è·³èºé–¾å€¼

            # âœ… å„ªåŒ–ï¼šæ»¾å‹•çª—å£ç•°å¸¸å€¼è™•ç†ï¼ˆé‡å°åŠ å¯†è²¨å¹£é«˜æ³¢å‹•ç‰¹æ€§ï¼‰
            'outlier_method': trial.suggest_categorical('outlier_method',
                                                      ['iqr', 'zscore', 'percentile', 'none']),
            'iqr_multiplier': trial.suggest_float('iqr_multiplier', 2.5, 4.0),  # æ›´ä¿å®ˆï¼Œé¿å…éåº¦æ¸…æ´—
            'zscore_threshold': trial.suggest_float('zscore_threshold', 2.5, 5.0),  # æ”¾å¯¬é–¾å€¼
            
            # ğŸ”§ ä¿®å¤2ï¼šåŸºäºæ•°æ®ä¿ç•™ç‡åŠ¨æ€è°ƒæ•´ç¼ºå¤±å€¼é˜ˆå€¼
            'rolling_outlier_window': trial.suggest_int('rolling_outlier_window', 
                                                       max(100, layer1_lookback // 5),  # è”åŠ¨
                                                       max(1000, layer1_lookback // 2)), # è”åŠ¨
            'lower_percentile': trial.suggest_float('lower_percentile', 1, 10),
            'upper_percentile': trial.suggest_float('upper_percentile', 90, 99),

            # æˆäº¤é‡ç©ºæ´æª¢æ¸¬
            'vol_window': trial.suggest_int('vol_window', 30, 120),
            'void_threshold': trial.suggest_float('void_threshold', 0.05, 0.3),
            'vol_vol_multiplier': trial.suggest_float('vol_vol_multiplier', 2.0, 8.0),

            # ğŸ”§ ä¿®å¤1ï¼šå¹³æ»‘çª—å£ä¸Layer1 lookback_windowè”åŠ¨
            'smooth_method': trial.suggest_categorical('smooth_method', ['rolling_mean', 'savgol', 'exp']),
            'smooth_window': trial.suggest_int('smooth_window', 
                                             max(5, smooth_min),
                                             max(20, smooth_max)),
            'exp_alpha': trial.suggest_float('exp_alpha', 0.01, 0.2),  # é™åˆ¶Î±ç¯„åœé˜²æ­¢éåº¦å¹³æ»‘

            # è³ªé‡æ¬Šé‡
            'completeness_weight': trial.suggest_float('completeness_weight', 0.3, 0.7),
            'fidelity_weight': trial.suggest_float('fidelity_weight', 0.2, 0.6),
            'stability_weight': trial.suggest_float('stability_weight', 0.1, 0.4)
        }

        try:
            # åŠ è¼‰åŸå§‹OHLCVæ•¸æ“š
            original_data = self.load_raw_ohlcv_data()

            if len(original_data) == 0:
                return -999.0

            # åŸ·è¡ŒOHLCVå°ˆç”¨æ¸…æ´—æ­¥é©Ÿ
            step1_data = self.check_price_anomalies(original_data, params)
            step2_data = self.check_ohlc_logic(step1_data, params)
            step3_data = self.check_timestamp_continuity(step2_data, params)
            step4_data = self.enhanced_price_cleaning(step3_data, params)
            step5_data = self.check_volume_anomalies(step4_data, params)
            cleaned_data = self.volume_void_detection(step5_data, params)

            if len(cleaned_data) == 0:
                return -999.0
            
            # ğŸ”§ æ–°å¢ï¼šåŠ¨æ€è°ƒæ•´ç¼ºå¤±å€¼é˜ˆå€¼
            data_retention_rate = len(cleaned_data) / len(original_data) if len(original_data) > 0 else 1.0
            
            # å¦‚æœæ•°æ®ä¸¢å¤±è¿‡å¤šï¼ŒåŠ¨æ€é™ä½ç¼ºå¤±å€¼é˜ˆå€¼
            if data_retention_rate < 0.8:
                adjusted_threshold = params.get('missing_threshold', 0.1) * 0.8
                self.logger.info(f"ğŸ”§ æ•°æ®ä¿ç•™ç‡ä½({data_retention_rate:.1%})ï¼Œé™ä½ç¼ºå¤±å€¼é˜ˆå€¼è‡³{adjusted_threshold:.3f}")
                params['missing_threshold'] = adjusted_threshold
                # é‡æ–°æ¸…æ´—ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…å¤„ç†ç¼ºå¤±å€¼ï¼‰
                if 'missing_threshold' in params:
                    missing_ratio = cleaned_data.isnull().sum().sum() / cleaned_data.size if cleaned_data.size > 0 else 0
                    if missing_ratio > adjusted_threshold:
                        cleaned_data = cleaned_data.dropna()
                    data_retention_rate = len(cleaned_data) / len(original_data)
            
            # å°†æ¸…æ´—ç»Ÿè®¡ä¿¡æ¯å†™å…¥trial.user_attrsä¾›åç»­å±‚ä½¿ç”¨
            trial.set_user_attr("data_retention_rate", data_retention_rate)
            trial.set_user_attr("smooth_window_used", params['smooth_window'])
            trial.set_user_attr("outlier_window_used", params['rolling_outlier_window'])

            # è©•ä¼°OHLCVæ¸…æ´—æ•ˆæœ
            effectiveness = self.evaluate_ohlcv_cleaning_effectiveness(original_data, cleaned_data)

            # è¨ˆç®—ç¶œåˆå¾—åˆ†
            retention_score = effectiveness.get('data_retention_rate', 0)
            consistency_score = effectiveness.get('ohlc_consistency_rate', 0)

            # è³ªé‡å¾—åˆ†ï¼šåŸºæ–¼ç•°å¸¸å€¼ç§»é™¤æ•ˆæœ
            quality_metrics = [
                effectiveness.get('zero_prices_removed', 0),
                effectiveness.get('zero_volume_removed', 0),
                effectiveness.get('missing_data_handled', 0)
            ]
            quality_score = min(1.0, sum(quality_metrics) / max(len(original_data) * 0.1, 1))

            # åŠ æ¬Šç¶œåˆå¾—åˆ†
            final_score = (retention_score * params['completeness_weight'] +
                          consistency_score * params['fidelity_weight'] +
                          quality_score * params['stability_weight'])

            # âœ… ä¿®å¾©ï¼šè¨ˆç®—æ¸…æ´—å¾Œçµ±è¨ˆä¿¡æ¯ï¼ˆåƒ…ä½¿ç”¨æ­·å²æ•¸æ“šï¼Œç„¡æ³„æ¼ï¼‰
            try:
                # è¨ˆç®—æ¸…æ´—å¾Œçš„"ç•¶å‰"æ”¶ç›Šç‡çµ±è¨ˆï¼ˆéæœªä¾†æ”¶ç›Šç‡ï¼‰
                if 'close' in cleaned_data.columns and len(cleaned_data) > 100:
                    # âœ… ä½¿ç”¨æ­·å²æ”¶ç›Šç‡ï¼ˆéæœªä¾†æ”¶ç›Šç‡ï¼‰
                    current_returns = cleaned_data['close'].pct_change()
                    current_returns_clean = current_returns.dropna()
                    
                    if len(current_returns_clean) > 100:
                        # æ¸…æ´—å¾Œçš„æ­·å²æ³¢å‹•ç‡çµ±è¨ˆ
                        cleaned_volatility = float(current_returns_clean.std())
                        cleaned_mean_return = float(current_returns_clean.mean())
                        cleaned_q90 = float(current_returns_clean.quantile(0.9))
                        cleaned_q10 = float(current_returns_clean.quantile(0.1))
                        
                        # ä¿å­˜åˆ°trial.user_attrsï¼ˆç„¡æ³„æ¼ï¼‰
                        trial.set_user_attr("cleaned_volatility", cleaned_volatility)
                        trial.set_user_attr("cleaned_mean_return", cleaned_mean_return)
                        trial.set_user_attr("cleaned_q90", cleaned_q90)
                        trial.set_user_attr("cleaned_q10", cleaned_q10)
                        trial.set_user_attr("cleaned_data_length", len(cleaned_data))
                        
                        self.logger.info(f"ğŸ“Š æ¸…æ´—çµ±è¨ˆï¼ˆæ­·å²æ³¢å‹•ç‡ï¼‰: æ³¢å‹•ç‡={cleaned_volatility:.4f}, "
                                       f"å‡å€¼={cleaned_mean_return:.6f}, "
                                       f"90%åˆ†ä½={cleaned_q90:.4f}, 10%åˆ†ä½={cleaned_q10:.4f}")
            except Exception as e:
                self.logger.warning(f"æ¸…æ´—çµ±è¨ˆè¨ˆç®—å¤±æ•—: {e}")

            return final_score

        except Exception as e:
            self.logger.error(f"æ•¸æ“šæ¸…æ´—å„ªåŒ–éç¨‹å‡ºéŒ¯: {e}")
            return -999.0

    def optimize(self, n_trials: int = 25) -> Dict:
        """åŸ·è¡ŒLayer0åŠ å¯†è²¨å¹£æ•¸æ“šæ¸…æ´—åƒæ•¸å„ªåŒ–"""
        self.logger.info("ğŸš€ é–‹å§‹Layer0æ•¸æ“šæ¸…æ´—åƒæ•¸å„ªåŒ–...")

        # å‰µå»ºç ”ç©¶
        storage_url = None
        try:
            storage_url = self.scaled_config.get('optuna_storage')
        except Exception:
            storage_url = None
        study = optuna.create_study(
            direction='maximize',
            study_name='layer0_crypto_data_cleaning',
            storage=storage_url,
            load_if_exists=bool(storage_url)
        )

        # åŸ·è¡Œå„ªåŒ–
        study.optimize(self.objective, n_trials=n_trials)

        # ç²å–æœ€å„ªåƒæ•¸
        best_params = study.best_params
        best_score = study.best_value

        self.logger.info(f"âœ… Layer0æ•¸æ“šæ¸…æ´—å„ªåŒ–å®Œæˆ! æœ€ä½³å¾—åˆ†: {best_score:.4f}")
        self.logger.info(f"æœ€å„ªåƒæ•¸: {best_params}")

        # ä½¿ç”¨æœ€å„ªåƒæ•¸é‡æ–°è©•ä¼°
        try:
            original_data = self.load_raw_ohlcv_data()
            final_cleaned_data = self.apply_cleaning(original_data, **best_params)

            final_effectiveness = self.evaluate_ohlcv_cleaning_effectiveness(original_data, final_cleaned_data)

            detailed_results = {
                'cleaning_effectiveness': final_effectiveness,
                'original_data_shape': original_data.shape,
                'cleaned_data_shape': final_cleaned_data.shape,
                'data_retention_rate': len(final_cleaned_data) / len(original_data)
            }
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•ç²å–è©³ç´°çµæœ: {e}")
            detailed_results = {}
            final_effectiveness = {}
            final_cleaned_data = None

        cleaned_file_path = None
        if final_cleaned_data is not None:
            try:
                # 1) ä¿æŒèˆŠä½ç½®ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                target_path = self.config_path / f"cleaned_ohlcv_{self.timeframe}.parquet"
                cleaned_file_path, _ = write_dataframe(final_cleaned_data, target_path)
                self.logger.info(f"âœ… Layer0æ¸…æ´—æ•¸æ“šå·²ä¿å­˜: {cleaned_file_path}")

                # 2) æ–°ä½ç½®ï¼ˆprocessedï¼‰
                # è‹¥å­˜åœ¨æœ€æ–°ç‰ˆæœ¬è™Ÿï¼Œå°‡æ¸…æ´—æ•¸æ“šä¹Ÿå¯«å…¥ç‰ˆæœ¬å­ç›®éŒ„
                results_root = Path(__file__).resolve().parent.parent / "results"
                latest_file = results_root / "latest.txt"
                version_sub = None
                if latest_file.exists():
                    try:
                        version_sub = latest_file.read_text(encoding="utf-8").strip()
                    except Exception:
                        version_sub = None
                processed_dir = self.data_path / "processed" / "cleaned" / f"{self.symbol}_{self.timeframe}"
                if version_sub:
                    processed_dir = processed_dir / version_sub
                processed_dir.mkdir(parents=True, exist_ok=True)
                processed_target = processed_dir / "cleaned_ohlcv.parquet"
                processed_file, _ = write_dataframe(final_cleaned_data, processed_target)
                self.logger.info(f"âœ… Layer0æ¸…æ´—æ•¸æ“šå·²åŒæ­¥è‡³: {processed_file}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¸…æ´—æ•¸æ“šä¿å­˜å¤±æ•—: {e}")
                cleaned_file_path = None

        # ä¿å­˜çµæœ (ä¿®å¾©JSONåºåˆ—åŒ–å•é¡Œ)
        def convert_numpy_types(obj):
            """è½‰æ›numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹"""
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, tuple):
                return tuple(convert_numpy_types(item) for item in obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj

        result = {
            'layer': 0,
            'description': 'Layer0: åŠ å¯†è²¨å¹£æ•¸æ“šæ¸…æ´—åƒæ•¸å„ªåŒ–',
            'best_params': convert_numpy_types(best_params),
            'best_score': convert_numpy_types(best_score),
            'n_trials': convert_numpy_types(n_trials),
            'detailed_results': convert_numpy_types(detailed_results),
            'metrics': convert_numpy_types(final_effectiveness),
            'cleaned_file': str(cleaned_file_path) if cleaned_file_path else None,
            'optimization_history': [
                {'trial': i, 'score': convert_numpy_types(trial.value)}
                for i, trial in enumerate(study.trials)
                if trial.value is not None
            ]
        }

        # ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆä¿æŒçµ±ä¸€å‘½åï¼‰
        output_file = self.config_path / "cleaning_params.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        self.logger.info(f"âœ… Layer0çµæœå·²ä¿å­˜è‡³: {output_file}")

        return result


def main():
    """ä¸»å‡½æ•¸"""
    optimizer = DataCleaningOptimizer(
        data_path='../data',
        config_path='../configs',
        symbol='BTCUSDT',
        timeframe='15m'
    )
    result = optimizer.optimize(n_trials=25)
    print(f"Layer0æ•¸æ“šæ¸…æ´—å„ªåŒ–å®Œæˆ: {result['best_score']:.4f}")


if __name__ == "__main__":
    main()