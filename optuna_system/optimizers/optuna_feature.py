# -*- coding: utf-8 -*-
"""
ç‰¹å¾µå·¥ç¨‹åƒæ•¸å„ªåŒ–å™¨ (ç¬¬2å±¤) - åˆ†é¡ç‰ˆæœ¬
çµ±ä¸€ä½¿ç”¨åˆ†é¡æ–¹æ³•ï¼šç‰¹å¾µé¸æ“‡ã€æ¨¡å‹è¨“ç·´ã€æ€§èƒ½è©•ä¼°
"""
import json
import logging
import os
import time
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from collections import Counter
from functools import lru_cache

import numpy as np
import optuna
from optuna.trial import TrialState
import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif, mutual_info_classif
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, balanced_accuracy_score, confusion_matrix
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures

from config.timeframe_scaler import TimeFrameScaler
from optuna_system.utils.io_utils import write_dataframe, read_dataframe

warnings.filterwarnings('ignore')


# Version sentinel for verification
PATCH_ID = "feat-obj-selected_features-fix"


class FeatureOptimizer:
    """ç‰¹å¾µå·¥ç¨‹åƒæ•¸å„ªåŒ–å™¨ - ç¬¬2å±¤å„ªåŒ–"""

    def __init__(self, data_path: str, config_path: str = "configs/",
                 symbol: str = "BTCUSDT", timeframe: str = "15m",
                 scaled_config: Dict = None):
        self.data_path = Path(data_path)
        self.config_path = Path(config_path)
        self.config_path.mkdir(exist_ok=True)
        self.symbol = symbol
        self.timeframe = timeframe
        # ä½¿ç”¨é›†ä¸­æ—¥èªŒ (ç”±ä¸Šå±¤/å…¥å£åˆå§‹åŒ–)ï¼Œé¿å…é‡è¤‡ basicConfig
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"FeatureOptimizer PATCH_ID={PATCH_ID}")

        # å¤šç›®æ¨™å„ªåŒ–è¨­å®šï¼ˆå¯é€éç’°å¢ƒæˆ– scaled_config æ§åˆ¶ï¼‰
        self.multi_objective_mode = bool(os.getenv('L2_OBJECTIVE_MODE', '').lower() in ('multi', 'multiobjective', 'pareto') or
                                         (self.scaled_config.get('l2_objective_mode', '').lower() in ('multi', 'multiobjective', 'pareto')))
        self.logger.info(f"ğŸ”§ Layer2 å¤šç›®æ¨™æ¨¡å¼: {self.multi_objective_mode}")

        def _env_float(name: str, default: float) -> float:
            val = os.getenv(name)
            if val is None:
                try:
                    return float(self.scaled_config.get(name.lower(), default))
                except Exception:
                    return float(default)
            try:
                return float(val)
            except Exception:
                return float(default)

        def _env_int(name: str, default: int) -> int:
            val = os.getenv(name)
            if val is None:
                try:
                    return int(self.scaled_config.get(name.lower(), default))
                except Exception:
                    return int(default)
            try:
                return int(val)
            except Exception:
                return int(default)

        # KPI ç´„æŸé–€æª»ï¼ˆå¯ä¾æ™‚æ¡†å·®ç•°èª¿æ•´ï¼‰
        self.kpi_constraints = {
            'min_sharpe': _env_float('L2_MIN_SHARPE', 1.0 if self.timeframe.lower() in ('15m', '15') else 1.2),
            'min_sortino': _env_float('L2_MIN_SORTINO', 1.2 if self.timeframe.lower() in ('1h', '4h') else 1.0),
            'min_calmar': _env_float('L2_MIN_CALMAR', 0.75 if self.timeframe.lower() in ('15m', '15') else 1.2),
            'min_win_rate': _env_float('L2_MIN_WINRATE', 0.6),
            'min_profit_factor': _env_float('L2_MIN_PROFIT_FACTOR', 1.3 if self.timeframe.lower() in ('15m', '15') else 1.5),
            'max_drawdown': _env_float('L2_MAX_DRAWDOWN', 0.2 if self.timeframe.lower() in ('15m', '15') else 0.15),
            'min_total_return': _env_float('L2_MIN_TOTAL_RETURN', 0.02),
            'min_annual_return': _env_float('L2_MIN_ANNUAL_RETURN', 0.15)
        }
        self.logger.info(f"ğŸ”’ Layer2 KPI Constraints: {self.kpi_constraints}")

        # å¤šç›®æ¨™åŠ æ¬Šï¼ˆå–®ç›®æ¨™æ¨¡å¼ä½¿ç”¨ï¼‰
        self.obj_weights = {
            'f1_macro': _env_float('L2_WEIGHT_F1_MACRO', 0.2),
            'f1_weighted': _env_float('L2_WEIGHT_F1_WEIGHTED', 0.2),
            'sharpe': _env_float('L2_WEIGHT_SHARPE', 0.2),
            'sortino': _env_float('L2_WEIGHT_SORTINO', 0.1),
            'calmar': _env_float('L2_WEIGHT_CALMAR', 0.1),
            'profit_factor': _env_float('L2_WEIGHT_PROFIT_FACTOR', 0.1),
            'win_rate': _env_float('L2_WEIGHT_WIN_RATE', 0.05),
            'total_return': _env_float('L2_WEIGHT_TOTAL_RETURN', 0.025),
            'annual_return': _env_float('L2_WEIGHT_ANNUAL_RETURN', 0.025)
        }
        weight_sum = sum(self.obj_weights.values())
        if weight_sum <= 0:
            # fallback weights
            self.obj_weights = {
                'f1_macro': 0.25,
                'f1_weighted': 0.25,
                'sharpe': 0.2,
                'sortino': 0.1,
                'calmar': 0.05,
                'profit_factor': 0.05,
                'win_rate': 0.05,
                'total_return': 0.03,
                'annual_return': 0.02
            }
        else:
            for k in self.obj_weights:
                self.obj_weights[k] = float(self.obj_weights[k]) / weight_sum
        self.logger.info(f"âš–ï¸ Layer2 ç›®æ¨™æ¬Šé‡: {self.obj_weights}")

        self.multiobjective_metrics = [
            'f1_macro',
            'sharpe',
            'sortino',
            'calmar',
            'profit_factor',
            'win_rate',
            'annual_return'
        ]


        self.scaled_config = scaled_config or {}
        self.scaler = TimeFrameScaler(self.logger)
        self.results_path = Path(self.config_path) / ".." / "results" / f"{self.symbol}_{self.timeframe}"
        self.results_path = self.results_path.resolve()
        self.results_path.mkdir(parents=True, exist_ok=True)

        # ç‰¹å¾µé–‹é—œèˆ‡ç­–ç•¥é…ç½®
        self.flags = self._validate_flags(self._load_feature_flags())
        self.phase_config: Dict[str, Any] = {}
        self.selection_params: Dict[str, Any] = {}

        # ğŸš€ ä¿®å¾©ç‰ˆï¼šå®Œæ•´æ•¸æ“šé åŠ è¼‰èˆ‡ç¼“å­˜ï¼ˆæ”¯æ´ lazy/full æ¨¡å¼ï¼‰
        self.logger.info("ğŸš€ é åŠ è¼‰èˆ‡ç¼“å­˜OHLCVã€ç‰¹å¾µã€åƒ¹æ ¼åºåˆ—...")
        try:
            self.ohlcv_data, self.features = self._load_and_prepare_features()

            # ğŸš€ é è¨ˆç®—åƒ¹æ ¼åºåˆ—ï¼ˆæ¨™ç±¤ç”Ÿæˆç”¨ï¼‰
            if 'close' in self.ohlcv_data.columns:
                self.close_prices = self.ohlcv_data['close'].copy()
            else:
                raise ValueError("OHLCVæ•¸æ“šä¸­ç¼ºå°‘closeåˆ—")

            # ğŸš€ é è¨ˆç®—æœ‰æ•ˆç´¢å¼•ç¯„åœï¼ˆé¿å…æ¯æ¬¡é‡æ–°è¨ˆç®—ï¼‰
            self.valid_data_range = (self.features.index.min(), self.features.index.max())

            self.logger.info(f"âœ… å®Œæ•´æ•¸æ“šé åŠ è¼‰: OHLCV={self.ohlcv_data.shape}, ç‰¹å¾µ={self.features.shape}")
            self.logger.info(f"âœ… åƒ¹æ ¼åºåˆ—ç·©å­˜: {len(self.close_prices)}å€‹åƒ¹æ ¼é»")
            self.logger.info(f"âœ… æœ‰æ•ˆç¯„åœ: {self.valid_data_range}")

            # ğŸš€ ä¸€æ¬¡æ€§æ§‹å»ºå…¨é‡ç‰¹å¾µçŸ©é™£ï¼ˆå¯é¸ï¼‰
            preload_mode = str(self.flags.get('preload_mode', 'full')).lower()
            if preload_mode == 'full':
                self.X_full = self._build_full_feature_matrix()
                self.logger.info(f"âœ… å…¨é‡ç‰¹å¾µçŸ©é™£æº–å‚™å®Œæˆ: {self.X_full.shape}")
            else:
                self.X_full = None
                self.logger.info("ğŸ•’ Lazy æ¨¡å¼å•Ÿç”¨ï¼šå»¶å¾Œæ§‹å»º X_full ä»¥é™ä½åˆå§‹åŒ–è¨˜æ†¶é«”å³°å€¼")

            # åˆå§‹åŒ–åˆ†éšæ®µç‰¹å¾µé…ç½®
            self._configure_phase_settings()

        except Exception as e:
            self.logger.error(f"âŒ æ•¸æ“šé åŠ è¼‰å¤±æ•—: {e}")
            raise ValueError(f"åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•é åŠ è¼‰æ•¸æ“š: {e}")  # Fail-Fast

    def _estimate_mem_mb(self, df: pd.DataFrame) -> float:
        try:
            return float(df.memory_usage(deep=True).sum()) / (1024 * 1024)
        except Exception:
            return 0.0

    def _enforce_memory_limits(self, X: pd.DataFrame) -> pd.DataFrame:
        """åœ¨é è¼‰æ¨¡å¼ä¸‹å°è¶…å¤§çŸ©é™£åšæ—©æœŸåˆ—éæ¿¾ï¼Œé™ä½è¨˜æ†¶é«”å ç”¨ã€‚"""
        try:
            max_cols = int(self.flags.get('max_full_features', 1200))
            max_mem_mb = float(self.flags.get('max_full_mem_mb', 1500.0))
        except Exception:
            max_cols, max_mem_mb = 1200, 1500.0

        mem_mb = self._estimate_mem_mb(X)
        if X.shape[1] <= max_cols and mem_mb <= max_mem_mb:
            self.logger.info(f"ğŸ’¾ X_full ä¼°ç®—è¨˜æ†¶é«” {mem_mb:.1f}MBï¼Œåˆ—æ•¸ {X.shape[1]}ï¼Œç„¡éœ€è£å‰ª")
            return X

        self.logger.warning(
            f"âš ï¸ X_full é”ä¸Šé™ï¼ˆcols={X.shape[1]}, memâ‰ˆ{mem_mb:.1f}MBï¼‰ï¼ŒåŸ·è¡Œæ—©æœŸåˆ—éæ¿¾"
        )

        # 1) å…ˆåšç¾æœ‰çš„è³ªé‡éæ¿¾
        X = self._filter_low_quality_features(X)
        mem_mb = self._estimate_mem_mb(X)

        # 2) è‹¥ä»è¶…é™ï¼Œä¿ç•™é«˜è®Šç•°åº¦åˆ—ï¼ˆå¿«é€Ÿè¿‘ä¼¼é‡è¦åº¦ï¼‰
        if X.shape[1] > max_cols or mem_mb > max_mem_mb:
            try:
                std = X.astype('float32').std().fillna(0.0)
                keep_k = min(max_cols, max(100, int(max_cols * 0.9)))
                top_cols = std.sort_values(ascending=False).head(keep_k).index
                X = X.loc[:, top_cols]
                self.logger.info(f"ğŸ”§ ä¾è®Šç•°åº¦ä¿ç•™å‰ {len(top_cols)} åˆ—ï¼Œnew_cols={X.shape[1]}")
            except Exception as exc:
                self.logger.warning(f"è®Šç•°åº¦è£å‰ªå¤±æ•—ï¼Œè·³é: {exc}")

        mem_mb2 = self._estimate_mem_mb(X)
        self.logger.info(f"ğŸ’¾ X_full æœ€çµ‚ä¼°ç®—è¨˜æ†¶é«” {mem_mb2:.1f}MBï¼Œåˆ—æ•¸ {X.shape[1]}")
        return X

    def _ensure_X_full(self) -> None:
        """Lazy æ¨¡å¼ä¸‹åœ¨éœ€è¦æ™‚æ§‹å»º X_fullï¼Œä¸¦åšè¨˜æ†¶é«”ä¿è­·ã€‚"""
        if isinstance(getattr(self, 'X_full', None), pd.DataFrame) and not self.X_full.empty:
            return
        X = self._build_full_feature_matrix()
        self.X_full = self._enforce_memory_limits(X)

    def _load_feature_flags(self) -> Dict:
        cfg_path = self.config_path / "feature_flags.json"
        try:
            if cfg_path.exists():
                with open(cfg_path, 'r', encoding='utf-8') as f:
                    return json.load(f) or {}
        except Exception as e:
            self.logger.warning(f"âš ï¸ feature_flags è®€å–å¤±æ•—: {e}")
        return {}

    def _validate_flags(self, flags: Dict) -> Dict:
        defaults = {
            "enable_td": True,
            "enable_wyckoff": True,
            "enable_micro": True,
            "enable_derivatives": False,
            "latency_lag": 0,
            "cv_gap": 12,
            "cv_purge": 12,
            "cv_splits": 5,
            "cv_strategy": "embargo",
            "tech": {},
            "enable_dynamic_label_balance": False,
            "enable_trade_freq_penalty": False,
            "enable_threshold_search": True,
            "threshold_tau_min": 0.30,
            "threshold_tau_max": 0.65,
            "threshold_tau_step": 0.05,
            "target_corr_keep_ratio": 0.6,
            "target_corr_min_score": 0.0,
            "layer1_range_mode": "narrow",
            "stability_selection_threshold": 0.6
        }
        if not isinstance(flags, dict):
            self.logger.warning("âš ï¸ feature_flags æ ¼å¼éæ³•ï¼Œå›é€€é è¨­")
            return defaults

        merged = defaults.copy()
        merged.update(flags)

        try:
            merged["enable_td"] = bool(merged.get("enable_td", True))
            merged["enable_wyckoff"] = bool(merged.get("enable_wyckoff", True))
            merged["enable_micro"] = bool(merged.get("enable_micro", True))
            merged["enable_derivatives"] = bool(merged.get("enable_derivatives", False))
            merged["enable_dynamic_label_balance"] = bool(merged.get("enable_dynamic_label_balance", False))
            merged["layer1_range_mode"] = str(merged.get("layer1_range_mode", "narrow")).lower()
            if merged["layer1_range_mode"] not in ("narrow", "wide"):
                self.logger.warning("âš ï¸ layer1_range_mode éæ³•ï¼Œå›é€€ narrow")
                merged["layer1_range_mode"] = "narrow"

            # Allow scaled_config to override
            if isinstance(self.scaled_config, dict):
                try:
                    if 'cv_gap' in self.scaled_config:
                        merged["cv_gap"] = int(self.scaled_config['cv_gap'])
                    if 'latency_lag' in self.scaled_config:
                        merged["latency_lag"] = int(self.scaled_config['latency_lag'])
                except Exception:
                    pass
            merged["latency_lag"] = max(0, int(merged.get("latency_lag", 0)))
            merged["cv_gap"] = max(0, int(merged.get("cv_gap", 12)))
            merged["cv_purge"] = max(0, int(merged.get("cv_purge", merged["cv_gap"])))
            merged["cv_splits"] = max(2, int(merged.get("cv_splits", 5)))

            if merged.get("cv_strategy") not in ("embargo", "purged"):
                self.logger.warning("âš ï¸ cv_strategy éæ³•ï¼Œå›é€€ embargo")
                merged["cv_strategy"] = "embargo"

            merged["enable_trade_freq_penalty"] = bool(merged.get("enable_trade_freq_penalty", False))
            merged["enable_threshold_search"] = bool(merged.get("enable_threshold_search", True))

            try:
                merged["threshold_tau_min"] = float(merged.get("threshold_tau_min", 0.30))
                merged["threshold_tau_max"] = float(merged.get("threshold_tau_max", 0.65))
                merged["threshold_tau_step"] = float(merged.get("threshold_tau_step", 0.05))
                if merged["threshold_tau_min"] >= merged["threshold_tau_max"]:
                    merged["threshold_tau_min"], merged["threshold_tau_max"] = 0.30, 0.65
                if merged["threshold_tau_step"] <= 0:
                    merged["threshold_tau_step"] = 0.05
            except Exception:
                merged["threshold_tau_min"], merged["threshold_tau_max"], merged["threshold_tau_step"] = 0.30, 0.65, 0.05

            try:
                merged["target_corr_keep_ratio"] = float(merged.get("target_corr_keep_ratio", 0.6))
            except Exception:
                merged["target_corr_keep_ratio"] = 0.6
            try:
                merged["target_corr_min_score"] = float(merged.get("target_corr_min_score", 0.0))
            except Exception:
                merged["target_corr_min_score"] = 0.0

            if not isinstance(merged.get("tech"), dict):
                self.logger.warning("âš ï¸ tech é…ç½®éæ³•ï¼Œç•¥é")
                merged["tech"] = {}

        except Exception as e:
            self.logger.warning(f"âš ï¸ feature_flags æ ¡é©—å¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­")
            return defaults

        return merged

    # -----------------------------
    # Phase 3: Label è³ªé‡æª¢æŸ¥/è‡ªå‹•é‡è¨“ç·´
    # -----------------------------
    def _periods_per_year(self, timeframe: Optional[str] = None) -> float:
        tf = (timeframe or self.timeframe or '').lower()
        mapping = {
            '1m': 252 * 24 * 60,
            '3m': 252 * 24 * 20,
            '5m': 252 * 24 * 12,
            '15m': 252 * 24 * 4,
            '30m': 252 * 24 * 2,
            '45m': 252 * 24 * 4 / 3,
            '1h': 252 * 24,
            '2h': 252 * 12,
            '4h': 252 * 6,
            '6h': 252 * 4,
            '8h': 252 * 3,
            '12h': 252 * 2,
            '1d': 252,
            '1w': 52,
            '1mo': 12
        }
        return float(mapping.get(tf, 252.0))

    def _compute_objective_metrics(self, trial: optuna.Trial, X: pd.DataFrame, y: pd.Series, lag: int) -> Dict[str, float]:
        if X.empty or y.empty:
            return {}

        classifier = self.create_enhanced_classifier(trial, X.shape[1])
        try:
            sample_weight = self._compute_sample_weights(y)
            classifier.fit(X.values, y.values, sample_weight=sample_weight)
        except TypeError:
            classifier.fit(X.values, y.values)
        except Exception as exc:
            self.logger.warning(f"âš ï¸ å…¨é‡è¨“ç·´å¤±æ•—ï¼Œæ”¹ç‚ºç„¡æ¬Šé‡: {exc}")
            classifier.fit(X.values, y.values)

        metrics: Dict[str, float] = {}

        try:
            preds = classifier.predict(X.values)
        except Exception as exc:
            self.logger.warning(f"âš ï¸ å…¨é‡é æ¸¬å¤±æ•—: {exc}")
            preds = np.full_like(y.values, fill_value=int(np.median(y.values)))

        try:
            proba = classifier.predict_proba(X.values)
        except Exception:
            proba = None

        idx = X.index
        price_series = self.close_prices.reindex(idx).ffill()
        returns = price_series.pct_change().shift(-1)
        returns = returns.reindex(idx).fillna(0.0)

        positions = pd.Series(preds - 1, index=idx)
        strategy_returns = (positions * returns).astype(float)
        strategy_returns = strategy_returns.replace([np.inf, -np.inf], 0.0).fillna(0.0)

        periods_per_year = self._periods_per_year()
        mean_ret = float(strategy_returns.mean()) if len(strategy_returns) > 0 else 0.0
        std_ret = float(strategy_returns.std(ddof=0)) if len(strategy_returns) > 0 else 0.0

        sharpe = 0.0
        if std_ret > 1e-9:
            sharpe = mean_ret / std_ret * np.sqrt(periods_per_year)

        downside = strategy_returns[strategy_returns < 0]
        downside_std = float(np.sqrt((downside ** 2).mean())) if len(downside) > 0 else 0.0
        sortino = 0.0
        if downside_std > 1e-9:
            sortino = mean_ret * np.sqrt(periods_per_year) / downside_std

        equity_curve = (strategy_returns + 1.0).cumprod()
        if len(equity_curve) == 0:
            max_dd = 0.0
            total_return = 0.0
        else:
            running_max = equity_curve.cummax()
            drawdowns = 1.0 - equity_curve / (running_max + 1e-9)
            max_dd = float(drawdowns.max()) if len(drawdowns) > 0 else 0.0
            total_return = float(equity_curve.iloc[-1] - 1.0)

        annual_return = 0.0
        if len(equity_curve) > 1:
            total_periods = len(strategy_returns)
            if total_periods > 0:
                total_return = float(equity_curve.iloc[-1] - 1.0)
                annual_return = (1.0 + total_return) ** (periods_per_year / total_periods) - 1.0

        calmar = 0.0
        if max_dd > 1e-9:
            calmar = annual_return / max_dd

        positive = strategy_returns[strategy_returns > 0]
        negative = strategy_returns[strategy_returns < 0]
        gross_profit = float(positive.sum())
        gross_loss = float(-negative.sum())
        profit_factor = float(gross_profit / gross_loss) if gross_loss > 1e-9 else float('inf')

        win_rate = float((strategy_returns > 0).mean()) if len(strategy_returns) > 0 else 0.0

        metrics['f1_macro'] = float(f1_score(y, preds, average='macro', zero_division=0))
        metrics['f1_weighted'] = float(f1_score(y, preds, average='weighted', zero_division=0))
        metrics['precision_macro'] = float(precision_score(y, preds, average='macro', zero_division=0))
        metrics['recall_macro'] = float(recall_score(y, preds, average='macro', zero_division=0))
        metrics['balanced_accuracy'] = float(balanced_accuracy_score(y, preds))

        if proba is not None:
            try:
                metrics['auc_macro'] = float(roc_auc_score(y, proba, multi_class='ovr', average='macro'))
            except Exception:
                metrics['auc_macro'] = 0.5
        else:
            metrics['auc_macro'] = 0.5

        metrics['sharpe'] = float(sharpe)
        metrics['sortino'] = float(sortino)
        metrics['calmar'] = float(calmar)
        metrics['profit_factor'] = float(min(max(profit_factor, 0.0), 10.0)) if np.isfinite(profit_factor) else 10.0
        metrics['win_rate'] = float(win_rate)
        metrics['total_return'] = float(total_return)
        metrics['annual_return'] = float(annual_return)
        metrics['max_drawdown'] = float(max_dd)
        metrics['max_drawdown_pct'] = float(max_dd)
        metrics['mean_return'] = float(mean_ret)
        metrics['std_return'] = float(std_ret)
        metrics['strategy_trades'] = float((positions.diff().abs() > 0).sum())
        metrics['avg_position'] = float(positions.abs().mean())

        metrics['lag'] = float(lag)
        metrics['n_samples'] = float(len(X))

        return metrics

    def _check_kpi_constraints(self, metrics: Dict[str, float]) -> Tuple[bool, str]:
        if not metrics:
            return False, 'metrics_unavailable'

        rules = (
            ('sharpe', self.kpi_constraints.get('min_sharpe', 0.0), 'min'),
            ('sortino', self.kpi_constraints.get('min_sortino', 0.0), 'min'),
            ('calmar', self.kpi_constraints.get('min_calmar', 0.0), 'min'),
            ('win_rate', self.kpi_constraints.get('min_win_rate', 0.0), 'min'),
            ('profit_factor', self.kpi_constraints.get('min_profit_factor', 0.0), 'min'),
            ('total_return', self.kpi_constraints.get('min_total_return', 0.0), 'min'),
            ('annual_return', self.kpi_constraints.get('min_annual_return', 0.0), 'min'),
            ('max_drawdown', self.kpi_constraints.get('max_drawdown', 1.0), 'max')
        )

        for key, threshold, mode in rules:
            if key not in metrics:
                return False, f'missing_{key}'
            value = float(metrics[key])
            if not np.isfinite(value):
                return False, f'invalid_{key}'
            if mode == 'min' and value < threshold:
                return False, f'{key}={value:.4f} < {threshold:.4f}'
            if mode == 'max' and value > threshold:
                return False, f'{key}={value:.4f} > {threshold:.4f}'

        return True, 'ok'

    def _weighted_objective_score(self, metrics: Dict[str, float]) -> float:
        score = 0.0
        for name, weight in self.obj_weights.items():
            if weight == 0:
                continue
            value = metrics.get(name)
            if value is None or not np.isfinite(value):
                continue
            capped = value
            if name in ('profit_factor', 'calmar', 'sharpe', 'sortino'):
                capped = max(min(value, 5.0), -1.0)
            if name in ('total_return', 'annual_return'):
                capped = max(min(value, 2.0), -1.0)
            score += float(weight) * float(capped)
        return float(score)

    def _current_best_value(self, study: optuna.study.Study) -> float:
        try:
            if not study.trials:
                return 0.0
            if self.multi_objective_mode:
                best_trials = getattr(study, 'best_trials', [])
                if not best_trials:
                    return 0.0
                # å–ç¬¬ä¸€å€‹ Pareto trial çš„ user_attrsï¼Œå¦‚æœæ²’æœ‰å‰‡å›é€€å…¶ values
                trial = best_trials[0]
                metrics = trial.user_attrs.get('objective_metrics')
                if metrics:
                    return self._weighted_objective_score(metrics)
                try:
                    values = trial.values or []
                    weighted = 0.0
                    total_w = 0.0
                    for name, weight in self.obj_weights.items():
                        if name not in self.multiobjective_metrics:
                            continue
                        idx = self.multiobjective_metrics.index(name)
                        if idx < len(values) and np.isfinite(values[idx]):
                            weighted += weight * float(values[idx])
                            total_w += weight
                    if total_w > 0:
                        return weighted / total_w
                except Exception:
                    pass
                return 0.0
            return float(study.best_value)
        except Exception:
            return 0.0

    def _labels_dir(self) -> Path:
        return self.data_path / 'processed' / 'labels' / f"{self.symbol}_{self.timeframe}"

    def load_latest_labels(self) -> Optional[pd.DataFrame]:
        try:
            labels_dir = self._labels_dir()
            candidates: List[Path] = []
            if labels_dir.exists():
                # éè¿´æœå°‹ç‰ˆæœ¬å­ç›®éŒ„
                candidates.extend(sorted(labels_dir.rglob('labels_*.parquet'), key=lambda p: p.stat().st_mtime, reverse=True))
                candidates.extend(sorted(labels_dir.rglob('labels_*.pkl'), key=lambda p: p.stat().st_mtime, reverse=True))
            if candidates:
                return read_dataframe(candidates[0])
            # fallback: config json materialized path
            tf_json = self.config_path / f"label_params_{self.timeframe}.json"
            if tf_json.exists():
                with open(tf_json, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                mp = data.get('materialized_path') or (data.get('metadata') or {}).get('file_path')
                if mp and Path(mp).exists():
                    return read_dataframe(Path(mp))
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•è¼‰å…¥æœ€æ–°æ¨™ç±¤: {e}")
        return None

    def analyze_label_quality(self) -> Dict[str, Any]:
        """å›å‚³åˆ†æçµæœ dictï¼Œä¸¦é™„å¸¶ pass å­—æ®µæŒ‡ç¤ºæ˜¯å¦é€šéé–¾å€¼ã€‚"""
        df = self.load_latest_labels()
        if df is None or 'label' not in df.columns:
            return {'pass': False, 'reason': 'no_labels'}
        s = df['label'].astype(int)
        dist = s.value_counts(normalize=True).to_dict()
        changes = int((s.diff() != 0).sum())
        stability = 1 - changes / max(len(s), 1)
        ok = (min(dist.get(0, 0), dist.get(2, 0)) >= 0.15) and stability >= 0.55
        return {'pass': ok, 'distribution': dist, 'stability': float(stability)}

    def auto_retrain_labels_if_needed(self) -> Optional[Dict[str, Any]]:
        try:
            analysis = self.analyze_label_quality()
            if analysis.get('pass', True):
                return None
            self.logger.info("è§¸ç™¼Layer1é‡æ–°å„ªåŒ–ï¼ˆè‡ªå‹•ï¼‰...")
            from optuna_system.optimizers.optuna_label import LabelOptimizer  # lazy import
            opt = LabelOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.config_path),
                symbol=self.symbol,
                timeframe=self.timeframe,
                scaled_config=self.scaled_config,
            )
            # é©åº¦æé«˜ trials
            result = opt.optimize(n_trials=200)
            self.logger.info(f"Layer1 é‡æ–°å„ªåŒ–å®Œæˆ: F1={result.get('best_score', 0):.4f}")
            return result
        except Exception as e:
            self.logger.warning(f"è‡ªå‹•é‡è¨“ç·´å¤±æ•—: {e}")
            return None

    def _apply_latency(self, obj: pd.DataFrame, lag: int = 1) -> pd.DataFrame:
        if lag <= 0 or obj is None:
            return obj
        return obj.ffill().shift(lag).fillna(0)

    def _allowed_slow_timeframes(self) -> List[str]:
        """Return the immediate slower timeframe(s) allowed for current dominant timeframe."""
        tf = str(self.timeframe).lower()
        mapping = {
            '15m': ['1h'],
            '30m': ['1h'],
            '1h': ['4h'],
            '2h': ['4h'],
            '4h': ['1d', '1D'],
            '1d': ['1w', '1W'],
            '1D': ['1w', '1W']
        }
        return mapping.get(tf, [])

    def _safe_merge(self, base: pd.DataFrame, extra: Optional[pd.DataFrame], prefix: str = "") -> pd.DataFrame:
        if extra is None or extra.empty:
            return base
        # åš´æ ¼å°é½Šï¼šå…ˆå°‡è¼”åŠ©ç‰¹å¾µ resample è‡³åŸºæº–ç´¢å¼•é »ç‡ï¼Œå†å°é½Šåˆ°åŸºæº–ç´¢å¼•
        extra_prefixed = extra.add_prefix(prefix)
        try:
            extra_resampled = self._resample_like(base.index, extra_prefixed)
            extra_aligned = extra_resampled.reindex(base.index).ffill()
        except Exception:
            # å¦‚æœè½‰é »/å°é½Šç•°å¸¸ï¼Œé€€åŒ–ç‚ºç°¡å–®å°é½Šä»¥ç¢ºä¿ä¸ä¸­æ–·
            extra_aligned = extra_prefixed.reindex(base.index).ffill()
        merged = pd.concat([base, extra_aligned], axis=1)
        # Remove backward fill to avoid any backward-looking leakage
        return merged.replace([np.inf, -np.inf], np.nan).ffill().fillna(0)

    def _infer_rule_from_index(self, idx: pd.DatetimeIndex) -> str:
        """å¾ DatetimeIndex æ¨æ–·é »ç‡è¦å‰‡ï¼ˆå¦‚ '15T', '1H', '4H', '1D'ï¼‰ã€‚"""
        if not isinstance(idx, pd.DatetimeIndex) or len(idx) < 3:
            return '15T'
        try:
            freq = pd.infer_freq(idx)
            if isinstance(freq, str) and len(freq) > 0:
                return freq
        except Exception:
            pass
        # å›é€€ï¼šä½¿ç”¨ä¸­ä½æ•¸é–“éš”ä¼°ç®—
        deltas = pd.Series(idx[1:]).reset_index(drop=True) - pd.Series(idx[:-1]).reset_index(drop=True)
        median_delta = pd.to_timedelta(np.median(deltas.values))
        seconds = max(1, int(median_delta.total_seconds()))
        # å¸¸ç”¨æ˜ å°„
        if seconds % (24*3600) == 0:
            days = seconds // (24*3600)
            return f"{days}D" if days > 1 else '1D'
        if seconds % 3600 == 0:
            hours = seconds // 3600
            return f"{hours}H"
        if seconds % 60 == 0:
            minutes = seconds // 60
            return f"{minutes}T"
        return f"{seconds}S"

    def _resample_like(self, base_index: pd.DatetimeIndex, extra: pd.DataFrame) -> pd.DataFrame:
        """å°‡ extra è½‰é »åˆ°èˆ‡ base_index ç›¸åŒçš„é »ç‡ï¼Œç²—â†’ç´°ç”¨ ffillï¼Œç´°â†’ç²—ç”¨ mean èšåˆã€‚"""
        if extra is None or extra.empty:
            return extra
        try:
            target_rule = self._infer_rule_from_index(base_index)
            # LRU å¿«å–éµï¼ˆç´¢å¼•æŒ‡ç´‹ + æ¬„ä½æŒ‡ç´‹ + ç›®æ¨™è¦å‰‡ï¼‰
            key = (str(extra.index[0]) if len(extra.index) else 'NA', str(extra.index[-1]) if len(extra.index) else 'NA', extra.shape[0], extra.shape[1], hash(tuple(extra.columns)), target_rule)
            if not hasattr(self, '_rs_cache'):
                self._rs_cache = {}
            cached = self._rs_cache.get(key)
            if cached is not None:
                return cached
            try:
                extra_rs = extra.resample(target_rule).mean()
            except Exception:
                extra_rs = extra
            self._rs_cache[key] = extra_rs
            return extra_rs
        except Exception:
            return extra

    def create_enhanced_classifier(self, trial: optuna.Trial, n_features: int):
        """æ ¹æ“štrialé…ç½®é¸æ“‡ä¸¦æ§‹å»ºå¢å¼·åˆ†é¡å™¨."""
        model_type = trial.suggest_categorical(
            'model_type',
            ['random_forest', 'extra_trees', 'gradient_boosting', 'lightgbm', 'xgboost']
        )

        if model_type == 'random_forest':
            return RandomForestClassifier(
                n_estimators=trial.suggest_int('rf_n_estimators', 200, 400),
                max_depth=trial.suggest_int('rf_max_depth', 12, 24),
                min_samples_split=trial.suggest_int('rf_min_samples_split', 2, 8),
                min_samples_leaf=trial.suggest_int('rf_min_samples_leaf', 1, 6),
                max_features=trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', 0.8]),
                bootstrap=True,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced_subsample'
            )

        if model_type == 'extra_trees':
            return ExtraTreesClassifier(
                n_estimators=trial.suggest_int('et_n_estimators', 200, 500),
                max_depth=trial.suggest_int('et_max_depth', 10, 18),
                min_samples_split=trial.suggest_int('et_min_samples_split', 5, 15),
                min_samples_leaf=trial.suggest_int('et_min_samples_leaf', 8, 20),
                max_features=trial.suggest_float('et_max_features', 0.3, 0.7),
                bootstrap=False,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'
            )

        if model_type == 'lightgbm':
            try:
                from lightgbm import LGBMClassifier  # type: ignore
            except ModuleNotFoundError:
                self.logger.warning("âš ï¸ æœªå®‰è£ lightgbmï¼Œæ”¹ç”¨ GradientBoostingClassifier")
            else:
                # æ›´å¯¬é¬†ä¸”å¯å‹•æ…‹æ”¾é¬†çš„æœå°‹ç©ºé–“ï¼Œç·©è§£ã€Œno positive gainã€
                relax = bool(self.flags.get('lgb_relax_search', False))
                depth_low, depth_high = (4, 8) if not relax else (4, 10)
                lgb_max_depth = trial.suggest_int('lgb_max_depth', depth_low, depth_high)
                # éµå®ˆ num_leaves <= 2^max_depthï¼Œä¸¦ç¢ºä¿ä¸‹é™ä¸æœƒå¤§æ–¼ä¸Šé™
                lgb_max_leaves = min(2 ** lgb_max_depth, 256 if relax else 128)
                lgb_low_leaves = max(4 if relax else 8, min(31, lgb_max_leaves // 2))
                lgb_num_leaves = trial.suggest_int('lgb_leaves', lgb_low_leaves, lgb_max_leaves)

                return LGBMClassifier(
                    objective='multiclass',
                    num_class=3,
                    n_estimators=trial.suggest_int('lgb_n_estimators', 400, 1200 if relax else 900),
                    learning_rate=trial.suggest_float('lgb_learning_rate', 0.02 if relax else 0.03, 0.15 if relax else 0.12),
                    max_depth=lgb_max_depth,
                    num_leaves=lgb_num_leaves,
                    subsample=trial.suggest_float('lgb_subsample', 0.6 if relax else 0.7, 1.0),
                    subsample_freq=trial.suggest_int('lgb_bagging_freq', 1, 9 if relax else 7),
                    colsample_bytree=trial.suggest_float('lgb_colsample', 0.5 if relax else 0.6, 0.95 if relax else 0.9),
                    max_bin=trial.suggest_int('lgb_max_bin', 127, 511 if relax else 255),
                    min_split_gain=trial.suggest_float('lgb_min_split_gain', 0.0, 0.05 if relax else 0.1),
                    min_child_samples=trial.suggest_int('lgb_min_child_samples', 2 if relax else 5, 60 if relax else 50),
                    min_child_weight=trial.suggest_float('lgb_min_child_weight', 1e-4 if relax else 1e-3, 5.0),
                    reg_alpha=trial.suggest_float('lgb_reg_alpha', 0.0, 1.5 if relax else 2.0),
                    reg_lambda=trial.suggest_float('lgb_reg_lambda', 0.0, 3.0 if relax else 4.0),
                    class_weight='balanced',
                    random_state=42,
                    n_jobs=-1,
                    verbosity=-1
                )

        if model_type == 'xgboost':
            try:
                from xgboost import XGBClassifier  # type: ignore
            except ModuleNotFoundError:
                self.logger.warning("âš ï¸ æœªå®‰è£ xgboostï¼Œæ”¹ç”¨ GradientBoostingClassifier")
            else:
                return XGBClassifier(
                    n_estimators=trial.suggest_int('xgb_n_estimators', 300, 800),
                    learning_rate=trial.suggest_float('xgb_learning_rate', 0.01, 0.07),
                    max_depth=trial.suggest_int('xgb_max_depth', 3, 8),
                    subsample=trial.suggest_float('xgb_subsample', 0.7, 1.0),
                    colsample_bytree=trial.suggest_float('xgb_colsample', 0.6, 0.9),
                    min_child_weight=trial.suggest_float('xgb_min_child_weight', 1.0, 5.0),
                    reg_lambda=trial.suggest_float('xgb_reg_lambda', 0.5, 2.0),
                    reg_alpha=trial.suggest_float('xgb_reg_alpha', 0.0, 1.0),
                    objective='multi:softprob',
                    num_class=3,
                    random_state=42,
                    n_jobs=-1
                )

        return GradientBoostingClassifier(
            n_estimators=trial.suggest_int('gb_n_estimators', 150, 300),
            max_depth=trial.suggest_int('gb_max_depth', 4, 10),
            learning_rate=trial.suggest_float('gb_learning_rate', 0.05, 0.2),
            subsample=trial.suggest_float('gb_subsample', 0.7, 1.0),
            max_features=trial.suggest_categorical('gb_max_features', ['sqrt', 'log2']),
            random_state=42
        )

    def remove_correlated_features_smart(self, X: pd.DataFrame, threshold: float) -> List[str]:
        """åŸºæ–¼é–¾å€¼ç§»é™¤é«˜åº¦ç›¸é—œç‰¹å¾µï¼Œä¿ç•™ä¿¡æ¯é‡æ›´é«˜è€…."""
        if X.empty:
            return []

        corr_matrix = X.corr().abs()
        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = set()

        for col in upper_tri.columns:
            high_corr = upper_tri.index[upper_tri[col] > threshold].tolist()
            if not high_corr:
                continue

            candidates = high_corr + [col]
            variances = {feat: X[feat].var() for feat in candidates if feat not in to_drop}
            if not variances:
                continue
            keep = max(variances, key=variances.get)
            for feat in candidates:
                if feat != keep:
                    to_drop.add(feat)

        return [col for col in X.columns if col not in to_drop]

    def _resolve_feature_alias(self, name: str, available) -> Optional[str]:
        """å°‡èˆŠå/åˆ¥åæ˜ å°„ç‚ºç•¶å‰å¯ç”¨æ¬„ä½åï¼Œé¿å… 'not in index'ã€‚
        è¦å‰‡ï¼š
        - ç²¾ç¢ºåŒ¹é…å„ªå…ˆ
        - å»é™¤å‰ç¶´('1h_','4h_','1d','1w_')èˆ‡ 'tech_' æ¯”å°åŸºåº•å
        - è‹¥è«‹æ±‚ç„¡ 'tech_' è€Œå¯ç”¨ä¸­æœ‰ 'tech_' ç‰ˆæœ¬ï¼Œå„ªå…ˆå–ä¹‹
        - å° gated/å¸¶å¾Œç¶´çš„åç¨±ï¼Œä»¥åŸºåº•å endswith/ç›¸ç­‰åšå¼±åŒ¹é…
        """
        try:
            avail = list(available)
            if name in avail:
                return name
            prefixes = ('1h_', '4h_', '1d_', '1w_', '15m_', '15min_')
            def split_prefix(n: str) -> Tuple[str, str]:
                for p in prefixes:
                    if n.startswith(p):
                        return p, n[len(p):]
                return '', n
            def base(n: str) -> str:
                _, tail = split_prefix(n)
                return tail[5:] if tail.startswith('tech_') else tail
            req_prefix, req_tail = split_prefix(name)
            req_base = base(name)
            # å€™é¸ï¼šåŸºåº•åä¸€è‡´
            candidates = []
            for col in avail:
                col_prefix, _ = split_prefix(col)
                col_base = base(col)
                if col_base == req_base or col.endswith(req_base) or col_base.endswith(req_base):
                    candidates.append((col, col_prefix))
            if not candidates:
                return None
            # å„ªå…ˆç›¸åŒæ™‚é–“å‰ç¶´ã€å¸¶ tech_ã€åç¨±è¼ƒé•·è€…
            def score(c: Tuple[str, str]) -> Tuple[int, int, int]:
                col, col_prefix = c
                same_prefix = 1 if col_prefix == req_prefix else 0
                has_tech = 1 if ('tech_' in col) else 0
                return (same_prefix, has_tech, len(col))
            best = sorted(candidates, key=score, reverse=True)[0][0]
            if best not in avail:
                return None
            if best != name:
                self.logger.info(f"ğŸ” åˆ¥åæ˜ å°„: '{name}' -> '{best}'")
            return best
        except Exception:
            return None

    def _normalize_column_subset(self, cols: List[str], df_train: pd.DataFrame, df_test: pd.DataFrame) -> List[str]:
        """å°å­é›†æ¬„ä½åšåˆ¥åè§£æèˆ‡äº¤é›†ä¿è­·ï¼Œç¢ºä¿ train/test å‡å­˜åœ¨ã€‚"""
        resolved: List[str] = []
        avail_train = set(df_train.columns)
        avail_test = set(df_test.columns)
        for c in cols:
            rc = c if (c in avail_train and c in avail_test) else self._resolve_feature_alias(c, avail_train.intersection(avail_test))
            if rc and (rc in avail_train) and (rc in avail_test) and rc not in resolved:
                resolved.append(rc)
        return resolved

    def _filter_by_target_correlation(self, X: pd.DataFrame, y: pd.Series, keep_ratio: float = 0.8, min_score: float = 0.0) -> List[str]:
        """ä»¥ç›®æ¨™é—œè¯åº¦ï¼ˆmutual informationï¼‰åšåˆæ­¥éæ¿¾ï¼Œä¿ç•™å‰è¿°æ¯”ä¾‹çš„é«˜é—œè¯ç‰¹å¾µ."""
        if X.empty:
            return []
        try:
            # MI åˆ†æ•¸å¿«å–ï¼šéµ=æ¬„ä½æŒ‡ç´‹ + y æŒ‡ç´‹
            cols_fp = hash(tuple(X.columns))
            y_fp = int(hash(tuple(pd.Series(y).fillna(-1).astype(int).values)))
            if not hasattr(self, '_mi_cache'):
                self._mi_cache = {}
            cache_key = (cols_fp, y_fp)
            if cache_key in self._mi_cache:
                score_series = self._mi_cache[cache_key]
            else:
                X_sanitized = X.replace([np.inf, -np.inf], np.nan).fillna(0)
                scores = mutual_info_classif(X_sanitized, y, discrete_features='auto', random_state=42)
                score_series = pd.Series(scores, index=X.columns).fillna(0.0)
                self._mi_cache[cache_key] = score_series
            kept = score_series[score_series >= float(min_score)]
            if kept.empty:
                kept = score_series
            top_k = max(3, int(len(kept) * float(keep_ratio)))
            return kept.sort_values(ascending=False).head(top_k).index.tolist()
        except Exception as e:
            self.logger.warning(f"âš ï¸ target-corr éæ¿¾å¤±æ•—: {e}")
            return list(X.columns)

    def _remove_correlated_features_smart(self, X: pd.DataFrame, threshold: float = 0.95) -> List[str]:
        """æ ¹æ“šç‰¹å¾µå…©å…©çµ•å°ç›¸é—œä¿‚æ•¸å»å†—é¤˜ï¼š
        - å…ˆä»¥è®Šç•°åº¦ç”±é«˜åˆ°ä½æ’åº
        - ä¾åºé¸å…¥ç‰¹å¾µï¼Œå°‡èˆ‡å·²é¸ç‰¹å¾µçµ•å°ç›¸é—œ > threshold çš„å…¶é¤˜ç‰¹å¾µæ¨™è¨˜ç‚ºå†—é¤˜
        - å›å‚³ä¿ç•™çš„ç‰¹å¾µåç¨±æ¸…å–®
        """
        try:
            if X is None or X.empty:
                return []
            X_num = X.replace([np.inf, -np.inf], np.nan).fillna(0).astype('float32')
            # è®Šç•°åº¦æ’åºï¼ˆé«˜è®Šç•°åº¦å„ªå…ˆä¿ç•™ï¼‰
            std_series = X_num.std().fillna(0.0)
            ordered_cols = std_series.sort_values(ascending=False).index.tolist()

            # çµ•å°ç›¸é—œçŸ©é™£
            corr = X_num.corr().abs()
            keep: List[str] = []
            removed: set = set()

            for col in ordered_cols:
                if col in removed:
                    continue
                keep.append(col)
                # å°‡èˆ‡ç•¶å‰ä¿ç•™ç‰¹å¾µé«˜åº¦ç›¸é—œè€…æ¨™è¨˜ç‚ºç§»é™¤
                try:
                    high_corr_cols = corr.index[(corr[col] > float(threshold))].tolist()
                except Exception:
                    high_corr_cols = []
                for hc in high_corr_cols:
                    if hc != col:
                        removed.add(hc)

            # æœ€å°‘ä¿ç•™3å€‹é¿å…å¾ŒçºŒå´©æ½°
            if len(keep) < 3:
                keep = ordered_cols[:max(3, len(ordered_cols))]
            return keep
        except Exception as e:
            self.logger.warning(f"âš ï¸ å»å†—é¤˜å¤±æ•—ï¼Œå›é€€å…¨éƒ¨ç‰¹å¾µ: {e}")
            return list(X.columns)

    def optimize_feature_selection_params(self, trial: optuna.Trial, n_features: int, layer1_params: Optional[Dict]) -> Dict[str, float]:
        """å‹•æ…‹ç‰¹å¾µé¸æ“‡åƒæ•¸é…ç½®ï¼Œè€ƒæ…®Layer1è¡¨ç¾."""
        coarse_ratio_low, coarse_ratio_high = (0.5, 0.7)  # å¾(0.3, 0.6)å„ªåŒ–
        stability_threshold_cap = 0.65

        if layer1_params:
            if layer1_params.get('signal_quality', 0.6) > 0.65:
                coarse_ratio_low, coarse_ratio_high = (0.55, 0.75)  # å¾(0.35, 0.68)å„ªåŒ–
                stability_threshold_cap = 0.75
            elif layer1_params.get('signal_quality', 0.6) < 0.45:
                coarse_ratio_low, coarse_ratio_high = (0.45, 0.65)  # å¾(0.25, 0.5)å„ªåŒ–
                stability_threshold_cap = 0.6

        coarse_k_min = max(40, int(n_features * coarse_ratio_low))
        coarse_k_max = min(max(coarse_k_min + 5, int(n_features * coarse_ratio_high)), n_features - 1)

        coarse_k = trial.suggest_int('coarse_k', coarse_k_min, max(coarse_k_min + 5, coarse_k_max))

        fine_ratio = trial.suggest_float('fine_ratio', 0.40, 0.70)
        fine_k = max(20, min(int(coarse_k * fine_ratio), coarse_k - 1))

        stability_threshold = trial.suggest_float('stability_threshold', 0.35, stability_threshold_cap)
        correlation_threshold = trial.suggest_float('correlation_threshold', 0.93, 0.97)

        return {
            'coarse_k': coarse_k,
            'fine_k': fine_k,
            'fine_ratio': fine_ratio,
            'stability_threshold': stability_threshold,
            'correlation_threshold': correlation_threshold
        }

    def _generate_td_features(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        if ohlcv.empty:
            return pd.DataFrame(index=ohlcv.index)

        close = ohlcv['close']
        td_up = []
        td_down = []
        up = down = 0
        for i in range(len(close)):
            if i < 4:
                up = down = 0
            else:
                if close.iloc[i] > close.iloc[i-4]:
                    up += 1
                    down = 0
                elif close.iloc[i] < close.iloc[i-4]:
                    down += 1
                    up = 0
                else:
                    up = down = 0
            td_up.append(up)
            td_down.append(down)

        df = pd.DataFrame(index=ohlcv.index)
        df['td_count_up'] = pd.Series(td_up, index=ohlcv.index)
        df['td_count_down'] = pd.Series(td_down, index=ohlcv.index)
        df['td_signal_buy'] = (df['td_count_down'] >= 9).astype(int).shift(1).fillna(0)
        df['td_signal_sell'] = (df['td_count_up'] >= 9).astype(int).shift(1).fillna(0)
        return df

    def _generate_wyckoff_features(self, ohlcv: pd.DataFrame, window: int = 20) -> pd.DataFrame:
        if ohlcv.empty:
            return pd.DataFrame(index=ohlcv.index)

        high, low, close, volume = ohlcv['high'], ohlcv['low'], ohlcv['close'], ohlcv['volume']
        local_low = low.rolling(window).min()
        local_high = high.rolling(window).max()
        vol_ma10 = volume.rolling(10).mean()

        spring = ((low < local_low * 0.995) & (close > local_low) & (volume <= vol_ma10)).astype(int).shift(1).fillna(0)
        upthrust = ((high > local_high * 1.005) & (close < local_high) & (volume <= vol_ma10)).astype(int).shift(1).fillna(0)

        price_trend = close.diff(window)
        vol_trend = volume.rolling(window).mean().diff(window)
        vol_div = (((price_trend > 0) & (vol_trend < 0)) | ((price_trend < 0) & (vol_trend > 0))).astype(int).fillna(0)

        vol_ratio = volume / volume.rolling(20).mean()
        price_vol = close.pct_change().rolling(20).std()
        phase = pd.Series(1, index=ohlcv.index)
        phase[(vol_ratio > 1.5) & (price_vol > price_vol.rolling(20).mean())] = 2
        phase[vol_ratio > 2.0] = 3
        phase[price_vol > price_vol.quantile(0.8)] = 4
        phase[vol_ratio < 0.7] = 5

        df = pd.DataFrame(index=ohlcv.index)
        df['wyk_spring'] = spring
        df['wyk_upthrust'] = upthrust
        df['wyk_volume_divergence'] = vol_div
        df['wyk_phase'] = phase.fillna(1).astype(int)
        return df

    def _generate_regime_features(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        """ç°¡ç‰ˆ regime ç‰¹å¾µï¼šåŸºæ–¼å¯¦ç¾æ³¢å‹•èˆ‡ç§»å‹•è¶¨å‹¢åŠƒåˆ†é«˜/ä½æ³¢ã€è¶¨å‹¢/ç›¤æ•´ã€‚"""
        if ohlcv is None or ohlcv.empty:
            return pd.DataFrame(index=getattr(ohlcv, 'index', None))
        try:
            close = ohlcv['close'].astype(float)
            ret = close.pct_change().fillna(0.0)
            vol30 = ret.rolling(30).std().fillna(0.0)
            vol90 = ret.rolling(90).std().fillna(0.0)
            vol_z = (vol30 - vol30.rolling(90).mean()) / (vol30.rolling(90).std() + 1e-8)
            # è¶¨å‹¢æŒ‡æ¨™ï¼šé•·çŸ­å‡ç·šå·®
            ma_fast = close.rolling(20).mean()
            ma_slow = close.rolling(60).mean()
            trend = (ma_fast - ma_slow) / (ma_slow + 1e-8)
            regime_high_vol = (vol30 > vol90).astype(int)
            regime_trend = (trend.abs() > trend.abs().rolling(60).median().fillna(0)).astype(int)
            df = pd.DataFrame({
                'regime_high_vol': regime_high_vol,
                'regime_trend': regime_trend,
                'vol_z': vol_z.fillna(0.0)
            }, index=ohlcv.index)
            return df.replace([np.inf, -np.inf], np.nan).ffill().fillna(0)
        except Exception as e:
            self.logger.warning(f"regime ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")
            return pd.DataFrame(index=ohlcv.index)

    def _generate_micro_features_from_ohlcv(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        if ohlcv.empty:
            return pd.DataFrame(index=ohlcv.index)

        high, low, close, volume = ohlcv['high'], ohlcv['low'], ohlcv['close'], ohlcv['volume']
        rng = (high - low).clip(lower=1e-8)
        mid = (high + low) / 2

        df = pd.DataFrame(index=ohlcv.index)
        df['micro_spread_est'] = (high - low) * 0.3
        df['micro_spread_ratio_est'] = (high - low) / close
        df['micro_mid_est'] = mid
        df['micro_depth_imb_5'] = (close - mid) / mid
        df['micro_depth_imb_10'] = df['micro_depth_imb_5'] * 0.8
        df['micro_depth_imb_20'] = df['micro_depth_imb_5'] * 0.6
        df['micro_imp_buy_0p5'] = (high - close) / close
        df['micro_imp_sell_0p5'] = (close - low) / close
        df['micro_ob_slope_bid'] = volume / (rng * 1000)
        df['micro_ob_slope_ask'] = volume / (rng * 1000)
        df['micro_price_eff'] = ((high - low) / close).clip(upper=0.8)
        return df.replace([np.inf, -np.inf], np.nan)

    def _load_derivatives_features(self, index: pd.DatetimeIndex) -> pd.DataFrame:
        if not self.flags.get('enable_derivatives', False):
            return pd.DataFrame(index=index)

        base_path = Path('data') / 'derived' / self.symbol
        if not base_path.exists():
            self.logger.warning(f"âš ï¸ è¡ç”Ÿå“è³‡æ–™ä¸å­˜åœ¨: {base_path}")
            return pd.DataFrame(index=index)

        datasets = ['funding', 'open_interest', 'long_short', 'liquidations', 'basis']
        frames = []
        for name in datasets:
            df = None
            for ext in ['parquet', 'csv']:
                file_path = base_path / f"{name}_{self.timeframe}.{ext}"
                if file_path.exists():
                    try:
                        if ext == 'parquet':
                            df = read_dataframe(file_path)
                        else:
                            df = pd.read_csv(file_path, parse_dates=True, index_col=0)
                        break
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è®€å–è¡ç”Ÿå“æ•¸æ“šå¤±æ•— {file_path}: {e}")
            if df is None or df.empty:
                continue
            df = df.reindex(index)
            missing_ratio = 1.0 - df.notna().mean().mean()
            if missing_ratio > 0.2:
                self.logger.warning(f"âš ï¸ è¡ç”Ÿå“ {name} ç¼ºå¤±æ¯”ä¾‹è¼ƒé«˜: {missing_ratio:.1%}")
            df = self._apply_latency(df, self.flags.get('latency_lag', 1))
            frames.append(df.add_prefix(f"deriv_{name}_"))

        if not frames:
            return pd.DataFrame(index=index)
        return pd.concat(frames, axis=1).fillna(0)

    def _make_cv_splits(self, X: pd.DataFrame, n_splits: int = 5) -> List[Tuple[np.ndarray, np.ndarray]]:
        tscv = TimeSeriesSplit(n_splits=n_splits)
        gap = int(self.flags.get('cv_gap', 12))
        purge = int(self.flags.get('cv_purge', gap))
        strategy = self.flags.get('cv_strategy', 'embargo')
        splits = []
        for train_idx, test_idx in tscv.split(X):
            train_idx = list(train_idx)
            test_idx = list(test_idx)
            if strategy == 'embargo':
                if len(train_idx) > purge:
                    train_idx = train_idx[:-purge]
                if len(test_idx) > gap:
                    test_idx = test_idx[gap:]
            elif strategy == 'purged':
                purge_left = purge_right = purge
                if test_idx:
                    start, end = test_idx[0], test_idx[-1]
                    train_idx = [i for i in train_idx if (i < start - purge_left or i > end + purge_right)]
            splits.append((np.array(train_idx), np.array(test_idx)))
        return splits


    def _load_and_prepare_features(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ğŸš€ é åŠ è¼‰OHLCVæ•¸æ“šå’Œç‰¹å¾µï¼ˆå„ªå…ˆä½¿ç”¨ç‰©åŒ–Layer0ï¼Œä¸¦å…·å‚™I/Oå®¹éŒ¯ï¼‰"""
        try:
            # 1) å„ªå…ˆè®€å–ç‰©åŒ–æ¸…æ´—æ•¸æ“š data/processed/cleaned/{symbol}_{timeframe}/cleaned_ohlcv*
            processed_dir = self.data_path / "processed" / "cleaned" / f"{self.symbol}_{self.timeframe}"
            ohlcv_data: Optional[pd.DataFrame] = None
            if processed_dir.exists():
                candidates = list(processed_dir.glob("cleaned_ohlcv*.parquet")) + \
                             list(processed_dir.glob("cleaned_ohlcv*.pkl")) + \
                             list(processed_dir.glob("cleaned_ohlcv*.pickle"))
                if candidates:
                    # é¸æ“‡æœ€æ–°ä¿®æ”¹æ™‚é–“çš„æª”æ¡ˆ
                    latest = max(candidates, key=lambda p: p.stat().st_mtime)
                    self.logger.info(f"ğŸ” å˜—è©¦è¼‰å…¥ç‰©åŒ–æ¸…æ´—æ•¸æ“š: {latest}")
                    try:
                        ohlcv_data = read_dataframe(latest)
                        self.logger.info(f"âœ… ä½¿ç”¨ç‰©åŒ–Layer0æ¸…æ´—æ•¸æ“š: {latest} -> {ohlcv_data.shape}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ç‰©åŒ–æ¸…æ´—æ•¸æ“šè®€å–å¤±æ•—ï¼Œå›é€€: {e}")

            # 2) å›é€€åˆ°èˆŠä½å€ optuna_system/configs/cleaned_ohlcv_{timeframe}.parquet
            if ohlcv_data is None:
                cleaned_candidate = self.config_path / f"cleaned_ohlcv_{self.timeframe}.parquet"
                if cleaned_candidate.exists():
                    self.logger.info(f"ğŸ” å˜—è©¦è¼‰å…¥èˆŠä½å€æ¸…æ´—æ•¸æ“š: {cleaned_candidate}")
                    try:
                        ohlcv_data = read_dataframe(cleaned_candidate)
                        self.logger.info(f"âœ… ä½¿ç”¨èˆŠä½å€æ¸…æ´—æ•¸æ“š: {ohlcv_data.shape}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ èˆŠä½å€æ¸…æ´—æ•¸æ“šè®€å–å¤±æ•—ï¼Œå›é€€: {e}")

            # 3) æœ€å¾Œå›é€€åˆ° raw OHLCV
            if ohlcv_data is None:
                ohlcv_file = self.data_path / "raw" / self.symbol / f"{self.symbol}_{self.timeframe}_ohlcv.parquet"
                self.logger.info(f"ğŸ” æŸ¥æ‰¾OHLCVæºæ–‡ä»¶: {ohlcv_file.absolute()}")
                if ohlcv_file.exists():
                    try:
                        ohlcv_data = read_dataframe(ohlcv_file)
                        self.logger.info(f"âœ… åŠ è¼‰åŸå§‹OHLCVæ•¸æ“š: {ohlcv_data.shape}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ åŸå§‹OHLCVè®€å–å¤±æ•—: {e}")

            # 4) å¦‚æœä»ç„¡æ³•ç²å–ï¼Œç”Ÿæˆæ¨¡æ“¬æ•¸æ“šï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
            if ohlcv_data is None:
                self.logger.warning("âŒ æœªæ‰¾åˆ°å¯ç”¨OHLCVæ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ“¬æ•¸æ“šç”¨æ–¼æ¸¬è©¦")
                ohlcv_data = self._generate_mock_data()

            # ğŸš€ çµ±ä¸€ä½¿ç”¨å…§å»ºç‰¹å¾µç”Ÿæˆï¼Œèˆ‡srcå®Œå…¨è§£è€¦
            features_df = self._generate_features(ohlcv_data)
            self.logger.info(f"âœ… ä½¿ç”¨å…§å»ºç‰¹å¾µç”Ÿæˆ: {features_df.shape}")

            # æ¸…ç†ç‰¹å¾µæ•¸æ“šï¼ˆç§»é™¤bfillä»¥å…å¼•å…¥å‘å¾Œå¡«è£œï¼‰
            features_df = features_df.ffill()
            features_df = features_df.replace([np.inf, -np.inf], np.nan).fillna(0)

            return ohlcv_data, features_df

        except Exception as e:
            self.logger.error(f"æ•¸æ“šåŠ è¼‰å¤±æ•—: {e}")
            raise  # ğŸš€ Fail-Fast: é—œéµéŒ¯èª¤ç›´æ¥æ‹‹å‡º

    def _generate_mock_data(self) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ“¬OHLCVæ•¸æ“š"""
        np.random.seed(42)
        n_samples = 2000

        # ç”ŸæˆåŸºç¤åƒ¹æ ¼åºåˆ—
        base_price = 50000
        price_changes = np.random.normal(0, 0.02, n_samples)
        prices = [base_price]

        for change in price_changes:
            new_price = prices[-1] * (1 + change)
            prices.append(max(new_price, 100))

        prices = np.array(prices[1:])

        # ç”ŸæˆOHLCVæ•¸æ“š
        data = []
        for i, close in enumerate(prices):
            volatility = np.random.uniform(0.005, 0.03)
            high = close * (1 + np.random.uniform(0, volatility))
            low = close * (1 - np.random.uniform(0, volatility))
            open_price = low + (high - low) * np.random.uniform(0.2, 0.8)

            high = max(high, open_price, close)
            low = min(low, open_price, close)
            volume = np.random.uniform(100, 10000)

            data.append({
                'open': open_price,
                'high': high,
                'low': low,
                'close': close,
                'volume': volume
            })

        dates = pd.date_range('2022-01-01', periods=n_samples, freq='15min')
        return pd.DataFrame(data, index=dates)

    def _generate_labels(self, price_series: pd.Series, lag: int = 12,
                        profit_quantile: float = 0.85, loss_quantile: float = 0.15,
                        lookback_window: int = 500) -> pd.Series:
        """å‘é‡åŒ–æ¨™ç±¤ç”Ÿæˆï¼šrolling quantile + shift(1) é¿å…æ´©éœ²ä¸¦åŠ é€Ÿã€‚"""
        future_prices = price_series.shift(-lag)
        returns = (future_prices - price_series) / price_series
 
        ret_past = returns.shift(1)
        min_periods = min(lookback_window, 100)
        upper = ret_past.rolling(window=lookback_window, min_periods=min_periods).quantile(profit_quantile)
        lower = ret_past.rolling(window=lookback_window, min_periods=min_periods).quantile(loss_quantile)
 
        upper = upper.clip(lower=1e-4)
        lower = lower.clip(upper=-1e-4)
 
        labels = pd.Series(1, index=price_series.index, dtype=int)
        labels = labels.mask(returns > upper, 2).mask(returns < lower, 0)
 
        if lag > 0:
            labels = labels.iloc[:-lag]
 
        # å¯é¸ï¼šæ³¢å‹•ç¸®æ”¾ï¼ˆé«˜æ³¢æ”¾å¯¬ã€ä½æ³¢æ”¶æ–‚ï¼‰ï¼Œæå‡å¯åˆ†æ€§ï¼ˆæ——æ¨™æ§åˆ¶ï¼‰
        try:
            if bool(self.flags.get('enable_volatility_scaled_labels', False)):
                ret = price_series.pct_change().fillna(0.0)
                vol = ret.rolling(60).std().fillna(0.0)
                vol_q = vol.rolling(500).quantile(0.5).fillna(vol.median())
                high_vol = vol > vol_q
                # åœ¨é«˜æ³¢æ®µæ”¾å¯¬è²·è³£é–€æª»ï¼Œä½¿è²·/è³£æ¨£æœ¬æ›´æ˜“é”æˆï¼›ä½æ³¢å‰‡æ›´ä¿å®ˆ
                adj = np.where(high_vol.reindex(labels.index, method='ffill').fillna(False), 0.02, -0.02)
                labels = labels.copy()
                # åƒ…åœ¨ä¸­æ€§å€é™„è¿‘åšç´°å¾®èª¿æ•´ï¼ˆé¿å…å¤§å¹…æ”¹å‹•ï¼‰
                labels[(returns.iloc[:len(labels)] > upper.iloc[:len(labels)] - adj) & (labels == 1)] = 2
                labels[(returns.iloc[:len(labels)] < lower.iloc[:len(labels)] + adj) & (labels == 1)] = 0
        except Exception as e:
            self.logger.warning(f"æ³¢å‹•ç¸®æ”¾æ¨™ç±¤å¤±æ•—: {e}")

        label_counts = labels.value_counts().sort_index()
        self.logger.info(f"ğŸ·ï¸ å‘é‡åŒ–æ¨™ç±¤: lag={lag}, profit_q={profit_quantile:.3f}, loss_q={loss_quantile:.3f}")
        self.logger.info(f"ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {dict(label_counts)} (ç¸½è¨ˆ{int(labels.notna().sum())})")
        return labels.dropna().astype(int)

    def generate_labels(self, price_data: pd.Series, params: Dict[str, Any]) -> pd.Series:
        """å°é½Š Layer2 èª¿ç”¨ä»‹é¢ï¼šå¾ params ä¸­è®€å–é‡åŒ–åˆ†ä½æˆ–å›é€€è‡³ Layer1 å‘½åï¼Œèª¿ç”¨å…§éƒ¨å‘é‡åŒ–ç”Ÿæˆã€‚

        æ”¯æ´éµï¼š
        - lag
        - profit_quantile / buy_quantile
        - loss_quantile / sell_quantile
        - lookback_window
        """
        try:
            lag = int(params.get('lag', max(1, int(self.scaled_config.get('label_lag_min', 3)))))
        except Exception:
            lag = 3
        try:
            profit_q = float(params.get('profit_quantile', params.get('buy_quantile', 0.75)))
        except Exception:
            profit_q = 0.75
        try:
            loss_q = float(params.get('loss_quantile', params.get('sell_quantile', 0.25)))
        except Exception:
            loss_q = 0.25
        try:
            lookback_window = int(params.get('lookback_window', int(self.scaled_config.get('lookback_window_min', 300))))
        except Exception:
            lookback_window = 300
        return self._generate_labels(price_data, lag, profit_q, loss_q, lookback_window)

    def apply_labels(self, data: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """å°‡ä»¥å‘é‡åŒ–æ–¹æ³•ç”Ÿæˆçš„æ¨™ç±¤é™„åŠ è‡³ OHLCV è³‡æ–™ï¼Œä¸¦å°é½Šç´¢å¼•ã€‚"""
        if 'close' not in data.columns:
            raise ValueError("è³‡æ–™å¿…é ˆåŒ…å«closeæ¬„ä½")
        labels = self.generate_labels(data['close'], params)
        aligned = data.loc[labels.index].copy()
        aligned['label'] = labels.astype(int)
        return aligned

    def build_features_for_materialization(self, ohlcv_data: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """é‡å»ºèˆ‡å„ªåŒ–æµç¨‹ä¸€è‡´çš„ç‰¹å¾µé›†ï¼Œç”¨æ–¼ç‰©åŒ–ï¼š
        1) ç›¡é‡ä½¿ç”¨ src.features.FeatureEngineering ç”Ÿæˆå®Œæ•´åŸºç¤ç‰¹å¾µ
        2) ç–ŠåŠ æœ¬æ¨¡çµ„çš„æŠ€è¡“æŒ‡æ¨™èˆ‡å¤šæ™‚æ¡†é–€æ§ç‰¹å¾µ
        3) é€²è¡Œç‰¹å¾µè³ªé‡éæ¿¾ï¼Œç¢ºä¿ä¸€è‡´æ€§
        """
        if ohlcv_data is None or ohlcv_data.empty:
            return pd.DataFrame()
        base_features = pd.DataFrame(index=ohlcv_data.index)
        # çµ±ä¸€ä½¿ç”¨å…§å»ºæŠ€è¡“/å¤šæ™‚æ¡†ç‰¹å¾µ
        try:
            tech_features = self.generate_technical_features(ohlcv_data, params)
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•ç”ŸæˆæŠ€è¡“ç‰¹å¾µ: {e}")
            tech_features = pd.DataFrame(index=ohlcv_data.index)
        X = self._safe_merge(base_features, tech_features)
        try:
            X = self._filter_low_quality_features(X)
        except Exception:
            pass
        return X

    def generate_technical_features(self, ohlcv_data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """ç”ŸæˆæŠ€è¡“æŒ‡æ¨™ç‰¹å¾µï¼ˆæ”¯æ´å¤šæ™‚æ¡†èˆ‡é…ç½®åŒ–åƒæ•¸ï¼‰"""
        flags_tech = self.flags.get('tech', {})

        base_features = self._calc_base_indicators(ohlcv_data, self.timeframe, flags_tech)
        features = base_features.copy()

        # Phase 1.2: å¤šå°ºåº¦å¢å¼·ç‰¹å¾µ
        try:
            enhanced = self._generate_enhanced_features(ohlcv_data)
            if isinstance(enhanced, pd.DataFrame) and not enhanced.empty:
                features = self._safe_merge(features, enhanced, prefix='')
        except Exception as e:
            self.logger.warning(f"å¢å¼·ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")

        # Phase 1.3: é«˜ç´šæŠ€è¡“æŒ‡æ¨™
        try:
            adv = self._generate_advanced_indicators(ohlcv_data)
            if isinstance(adv, pd.DataFrame) and not adv.empty:
                features = self._safe_merge(features, adv, prefix='')
        except Exception as e:
            self.logger.warning(f"é«˜ç´šæŒ‡æ¨™ç”Ÿæˆå¤±æ•—: {e}")

        # æ–°å¢ï¼šé‡åƒ¹/å‹•é‡/è¶¨å‹¢å¼·åº¦ï¼ˆå‰è¦–å®‰å…¨ï¼‰
        try:
            cmf = self._calc_cmf(ohlcv_data['high'], ohlcv_data['low'], ohlcv_data['close'], ohlcv_data['volume'])
            adl = self._calc_adl(ohlcv_data['high'], ohlcv_data['low'], ohlcv_data['close'], ohlcv_data['volume'])
            stoch_rsi = self._calc_stoch_rsi(ohlcv_data['close'])
            adx = self._calc_adx(ohlcv_data['high'], ohlcv_data['low'], ohlcv_data['close'])

            add_df = pd.DataFrame({
                'tech_cmf': cmf,
                'tech_adl': adl,
                'tech_stoch_rsi': stoch_rsi,
                'tech_adx': adx,
            }, index=ohlcv_data.index)

            features = self._safe_merge(features, add_df, prefix='')
        except Exception as e:
            self.logger.warning(f"å¢è£œé‡åƒ¹/å‹•é‡/è¶¨å‹¢å¼·åº¦æŒ‡æ¨™å¤±æ•—: {e}")

        # Phase 1.4: Regime-aware ç‰¹å¾µ
        try:
            if bool(self.flags.get('enable_regime_features', True)):
                regime_df = self._generate_regime_features(ohlcv_data)
                if isinstance(regime_df, pd.DataFrame) and not regime_df.empty:
                    features = self._safe_merge(features, regime_df, prefix='')
        except Exception as e:
            self.logger.warning(f"regime ç‰¹å¾µåˆä½µå¤±æ•—: {e}")

        mtf_cfg = flags_tech.get('multi_timeframes', {})
        if isinstance(mtf_cfg, dict) and mtf_cfg.get('enabled'):
            rules = mtf_cfg.get('rules', {})
            tf_list = [tf for tf in mtf_cfg.get('timeframes', []) if tf in self._allowed_slow_timeframes()]
            for tf_key in tf_list:
                rule = rules.get(tf_key)
                if not rule:
                    continue
                freq = rule.get('rule') if isinstance(rule, dict) else rule
                freq_map = {
                    '15m': '15T', '15min': '15T', '15': '15T',
                    '1h': '1H', '1hour': '1H',
                    '4h': '4H', '4hour': '4H',
                    '1d': '1D', '1day': '1D',
                    '1w': '1W', '1week': '1W', '1W': '1W'
                }
                freq_str = str(freq).lower()
                freq = freq_map.get(freq_str, freq)
                resampled = self._resample_ohlcv(ohlcv_data, freq)
                if resampled.empty:
                    continue
                tf_override = rule if isinstance(rule, dict) else {}
                tf_features = self._calc_base_indicators(resampled, tf_key, flags_tech, base_index=None, tf_overrides_override=tf_override)
                # align slow features strictly forward-only
                tf_features = tf_features.shift(1).ffill()
                tf_features = tf_features.reindex(ohlcv_data.index).ffill().fillna(0)
                base_cols = list(tf_features.columns)
                gating_signal = None
                if base_cols:
                    rsi_cols = [col for col in base_cols if 'tech_rsi_' in col]
                    if rsi_cols:
                        gate_threshold = float(tf_override.get('gate_threshold', 0.55))
                        gating_signal = (tf_features[rsi_cols].mean(axis=1) > gate_threshold * 100).astype(float)
                if gating_signal is not None:
                    gated = tf_features.multiply(gating_signal, axis=0).add_suffix('_gated')
                    # ç¢ºä¿ gated æ¬„ä½ç©©å®šå­˜åœ¨ï¼šæœªè§¸ç™¼è™•ä»¥ 0 ä¿ç•™æ¬„ä½ï¼Œé¿å…ä¸åŒæŠ˜ç”Ÿæˆä¸ä¸€è‡´
                    gated = gated.reindex(tf_features.index).fillna(0)
                    tf_features = pd.concat([tf_features, gated], axis=1)
                features = self._safe_merge(features, tf_features, prefix=f"{tf_key}_")

        return features

    def _generate_enhanced_features(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        """Phase 1.2: å¤šå°ºåº¦ç‰¹å¾µï¼ˆå‹•é‡/æˆäº¤é‡åŠ é€Ÿåº¦ç­‰ï¼‰ã€‚"""
        if ohlcv is None or ohlcv.empty:
            return pd.DataFrame(index=ohlcv.index if isinstance(ohlcv, pd.DataFrame) else None)
        windows = [200, 400, 800, 1200]
        out = {}
        close = ohlcv['close']
        volume = ohlcv['volume']
        for w in windows:
            try:
                mom = close.pct_change(w)
                out[f'momentum_acc_{w}'] = mom.diff(5)
            except Exception:
                continue
            try:
                vol_chg = volume.pct_change(w)
                out[f'volume_acc_{w}'] = vol_chg.diff(5)
            except Exception:
                continue
        df = pd.DataFrame(out, index=ohlcv.index)
        return df.replace([np.inf, -np.inf], np.nan)

    def _generate_advanced_indicators(self, ohlcv: pd.DataFrame) -> pd.DataFrame:
        """Phase 1.3: é«˜ç´šæŒ‡æ¨™ï¼ˆWyckoffç´¯ç©/TDåºåˆ—ç°¡åŒ–ç‰ˆï¼‰ã€‚"""
        if ohlcv is None or ohlcv.empty:
            return pd.DataFrame(index=ohlcv.index if isinstance(ohlcv, pd.DataFrame) else None)
        high, low, close, volume = ohlcv['high'], ohlcv['low'], ohlcv['close'], ohlcv['volume']
        rng = (high - low).replace(0, np.nan)
        # Wyckoffç´¯ç©/æ´¾ç™¼ç·šï¼ˆç°¡åŒ–ï¼‰
        money_flow = ((close - low) - (high - close)) / rng * volume
        wyckoff_ad = money_flow.fillna(0).cumsum()
        # TDåºåˆ—ï¼ˆç°¡åŒ–setupè¨ˆæ•¸ï¼‰
        td_setup = pd.Series(0, index=close.index)
        for i in range(4, len(close)):
            try:
                if close.iloc[i] > close.iloc[i-4]:
                    td_setup.iloc[i] = td_setup.iloc[i-1] + 1 if td_setup.iloc[i-1] > 0 else 1
                elif close.iloc[i] < close.iloc[i-4]:
                    td_setup.iloc[i] = td_setup.iloc[i-1] - 1 if td_setup.iloc[i-1] < 0 else -1
                else:
                    td_setup.iloc[i] = 0
            except Exception:
                td_setup.iloc[i] = 0
        return pd.DataFrame({
            'wyk_ad_line': wyckoff_ad,
            'td_setup': td_setup
        }, index=close.index)

    # ---- æ–°å¢ï¼šé‡åƒ¹/å‹•é‡/è¶¨å‹¢å¼·åº¦æŒ‡æ¨™ï¼ˆå‰è¦–å®‰å…¨ï¼‰ ----
    def _calc_cmf(self, high: pd.Series, low: pd.Series, close: pd.Series,
                  volume: pd.Series, window: int = 20) -> pd.Series:
        rng = (high - low).replace(0, np.nan)
        mf_multiplier = ((close - low) - (high - close)) / (rng + 1e-9)
        mf_volume = mf_multiplier * volume
        cmf = (mf_volume.rolling(window, min_periods=max(3, window // 3)).sum() /
               (volume.rolling(window, min_periods=max(3, window // 3)).sum() + 1e-9))
        return cmf.fillna(0)

    def _calc_adl(self, high: pd.Series, low: pd.Series, close: pd.Series,
                  volume: pd.Series) -> pd.Series:
        rng = (high - low).replace(0, np.nan)
        mf_multiplier = ((close - low) - (high - close)) / (rng + 1e-9)
        mf_volume = mf_multiplier * volume
        adl = mf_volume.cumsum()
        return adl.fillna(method='ffill').fillna(0)

    def _calc_rsi(self, close: pd.Series, window: int = 14) -> pd.Series:
        delta = close.diff()
        gain = delta.where(delta > 0, 0).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / (loss.replace(0, np.nan))
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def _calc_stoch_rsi(self, close: pd.Series, window: int = 14) -> pd.Series:
        rsi = self._calc_rsi(close, window)
        min_rsi = rsi.rolling(window).min()
        max_rsi = rsi.rolling(window).max()
        stoch_rsi = (rsi - min_rsi) / ((max_rsi - min_rsi) + 1e-9)
        return stoch_rsi.clip(0, 1).fillna(0)

    def _calc_adx(self, high: pd.Series, low: pd.Series, close: pd.Series,
                  period: int = 14) -> pd.Series:
        # True Range components
        tr1 = (high - low).abs()
        tr2 = (high - close.shift(1)).abs()
        tr3 = (low - close.shift(1)).abs()
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

        # Directional Movement
        plus_dm = (high.diff()).clip(lower=0)
        minus_dm = (-low.diff()).clip(lower=0)
        plus_dm[plus_dm < minus_dm] = 0
        minus_dm[minus_dm <= plus_dm] = 0

        # Wilder smoothing
        atr = tr.ewm(alpha=1/period, adjust=False).mean()
        plus_di = 100 * (plus_dm.ewm(alpha=1/period, adjust=False).mean() / (atr + 1e-9))
        minus_di = 100 * (minus_dm.ewm(alpha=1/period, adjust=False).mean() / (atr + 1e-9))

        dx = (100 * (plus_di - minus_di).abs() / ((plus_di + minus_di) + 1e-9)).fillna(0)
        adx = dx.ewm(alpha=1/period, adjust=False).mean()
        return adx.fillna(0)

    def _calc_base_indicators(self, ohlcv: pd.DataFrame, tf_key: str, flags_tech: Dict,
                               base_index: Optional[pd.DatetimeIndex] = None,
                               tf_overrides_override: Optional[Dict] = None) -> pd.DataFrame:
        if ohlcv.empty:
            return pd.DataFrame(index=base_index if base_index is not None else ohlcv.index)

        tf_overrides_base = (flags_tech.get('tf_overrides') or {}).get(tf_key, {})
        tf_overrides = {}
        if isinstance(tf_overrides_override, dict):
            tf_overrides.update(tf_overrides_override)
        tf_overrides.update(tf_overrides_base)

        def _as_window_list(value, default):
            if isinstance(value, (list, tuple)):
                return [int(v) for v in value if int(v) > 0]
            if value is None:
                return list(default)
            return [int(value)]

        fibo_base = flags_tech.get('fibo_ma', [3, 5, 8, 13, 21, 34, 55, 89, 144, 233])
        fibo_ma = [_w for _w in fibo_base]
        fibo_ma = [self.scaler.scale_window(w, self.timeframe, cap=400) for w in fibo_ma]
        fibo_ma = _as_window_list(tf_overrides.get('fibo_ma', fibo_ma), fibo_ma)
        fibo_reference_base = flags_tech.get('fibo_reference_windows', [55, 89, 144])
        fibo_reference_scaled = [self.scaler.scale_window(w, self.timeframe, cap=400) for w in fibo_reference_base]
        fibo_reference_windows = _as_window_list(tf_overrides.get('fibo_reference_windows', fibo_reference_scaled), fibo_reference_scaled)

        retracement_levels = flags_tech.get('fibo_retracement_levels', [0.236, 0.382, 0.5, 0.618, 0.786])
        extension_levels = flags_tech.get('fibo_extension_levels', [1.272, 1.414, 1.618, 2.0])
        try:
            retracement_levels = [float(level) for level in retracement_levels if 0 < float(level) < 1]
        except Exception:
            retracement_levels = [0.382, 0.5, 0.618]
        try:
            extension_levels = [float(level) for level in extension_levels if float(level) > 0]
        except Exception:
            extension_levels = [1.272, 1.618]
        rsi_windows = _as_window_list(tf_overrides.get('rsi_windows', flags_tech.get('rsi_windows',
                                        [5, 7, 9, 14, 21, 28, 42])), [5, 7, 9, 14, 21, 28, 42])
        bb_base = flags_tech.get('bb_windows', [10, 20, 34, 50])
        bb_base_scaled = [self.scaler.scale_window(w, self.timeframe, cap=400) for w in bb_base]
        bb_windows = _as_window_list(tf_overrides.get('bb_windows', bb_base_scaled), bb_base_scaled)
        bb_std_list = np.clip(flags_tech.get('bb_std_list', [1.5, 2.0, 2.5]), 1.0, 3.0)
        mfi_base = flags_tech.get('mfi_windows', [14])
        mfi_scaled = [self.scaler.scale_window(w, self.timeframe, cap=400) for w in mfi_base]
        mfi_windows = _as_window_list(tf_overrides.get('mfi_windows', mfi_scaled), mfi_scaled)
        cci_base = flags_tech.get('cci_windows', [20])
        cci_scaled = [self.scaler.scale_window(w, self.timeframe, cap=400) for w in cci_base]
        cci_windows = _as_window_list(tf_overrides.get('cci_windows', cci_scaled), cci_scaled)

        kdj_cfg = tf_overrides.get('kdj', flags_tech.get('kdj', {
            'rsv_window': 9,
            'k': 3,
            'd': 3
        }))
        if not isinstance(kdj_cfg, dict):
            kdj_cfg = {'rsv_window': 9, 'k': 3, 'd': 3}

        rsv_window = max(1, self.scaler.scale_window(int(kdj_cfg.get('rsv_window', kdj_cfg.get('rsv', 9))), self.timeframe, cap=400))
        k_smooth = max(1, int(kdj_cfg.get('k', 3)))
        d_smooth = max(1, int(kdj_cfg.get('d', 3)))

        def _format_level(level: float) -> str:
            level_str = f"{level:.3f}".rstrip('0').rstrip('.')
            return level_str.replace('-', 'm').replace('.', '_')

        close = ohlcv['close']
        high = ohlcv['high']
        low = ohlcv['low']
        volume = ohlcv['volume']

        features = pd.DataFrame(index=ohlcv.index)

        features['tech_returns'] = close.pct_change()
        features['tech_high_low_ratio'] = high / low
        features['tech_open_close_ratio'] = ohlcv['open'] / close

        range_epsilon = 1e-9
        for ref_win in fibo_reference_windows:
            ref_win = max(1, int(ref_win))
            if ref_win <= 1:
                continue

            high_roll = high.rolling(ref_win).max()
            low_roll = low.rolling(ref_win).min()
            price_range = high_roll - low_roll

            features[f'tech_fibo_range_ratio_{ref_win}'] = (close - low_roll) / (price_range + range_epsilon)

            for level in retracement_levels:
                level_value = high_roll - level * price_range
                level_label = _format_level(level)
                features[f'tech_fibo_retrace_price_{ref_win}_{level_label}'] = level_value
                features[f'tech_fibo_retrace_gap_{ref_win}_{level_label}'] = (close - level_value) / (price_range + range_epsilon)

            for level in extension_levels:
                level_value = high_roll + level * price_range
                level_label = _format_level(level)
                features[f'tech_fibo_extension_price_{ref_win}_{level_label}'] = level_value
                features[f'tech_fibo_extension_gap_{ref_win}_{level_label}'] = (close - level_value) / (price_range + range_epsilon)

        for win in fibo_ma:
            sma = close.rolling(win).mean()
            ema = close.ewm(span=win).mean()
            features[f'tech_sma_{win}'] = sma
            features[f'tech_ema_{win}'] = ema
            features[f'tech_price_sma_ratio_{win}'] = close / sma
            features[f'tech_price_ema_ratio_{win}'] = close / ema

        delta = close.diff()
        for win in rsi_windows:
            gain = delta.where(delta > 0, 0).rolling(win).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(win).mean()
            rs = gain / (loss.replace(0, np.nan))
            features[f'tech_rsi_{win}'] = 100 - (100 / (1 + rs))

        for win in bb_windows:
            sma = close.rolling(win).mean()
            std = close.rolling(win).std()
            for std_k in bb_std_list:
                upper = sma + std_k * std
                lower = sma - std_k * std
                width = (upper - lower) / sma
                pos = (close - lower) / (upper - lower)
                features[f'tech_bb_upper_{win}_{std_k}'] = upper
                features[f'tech_bb_lower_{win}_{std_k}'] = lower
                features[f'tech_bb_width_{win}_{std_k}'] = width
                features[f'tech_bb_pos_{win}_{std_k}'] = pos

        volume_window = max(1, self.scaler.scale_window(int(self.flags.get('volume_sma_window', 20)), self.timeframe, cap=400))
        vol_ma = volume.rolling(volume_window).mean()
        features['tech_volume_sma_ratio'] = volume / vol_ma
        features['tech_price_volume'] = close * volume

        for win in [10, 20]:
            features[f'tech_volatility_{win}'] = close.pct_change().rolling(win).std()

        ema_12 = close.ewm(span=self.scaler.scale_window(12, self.timeframe, cap=400)).mean()
        ema_26 = close.ewm(span=self.scaler.scale_window(26, self.timeframe, cap=400)).mean()
        macd = ema_12 - ema_26
        features['tech_macd'] = macd
        features['tech_macd_signal'] = macd.ewm(span=9).mean()
        features['tech_macd_histogram'] = features['tech_macd'] - features['tech_macd_signal']

        for base_win in [14, 21]:
            win = self.scaler.scale_window(base_win, self.timeframe, cap=400)
            high_max = high.rolling(win).max()
            low_min = low.rolling(win).min()
            features[f'tech_williams_r_{win}'] = -100 * (high_max - close) / (high_max - low_min)

        for base_win in [10, 20]:
            win = self.scaler.scale_window(base_win, self.timeframe, cap=400)
            features[f'tech_momentum_{win}'] = close / close.shift(win) - 1

        typical_price = (high + low + close) / 3
        vol_sum = volume.rolling(20).sum()
        vwap_20 = (typical_price * volume).rolling(20).sum() / vol_sum
        features['tech_vwap_20'] = vwap_20
        features['tech_price_vwap_ratio'] = close / vwap_20

        for win in [20, 50]:
            high_c = high.rolling(win).max()
            low_c = low.rolling(win).min()
            features[f'tech_price_channel_high_{win}'] = high_c
            features[f'tech_price_channel_low_{win}'] = low_c
            features[f'tech_price_channel_pos_{win}'] = (close - low_c) / (high_c - low_c)

        obv_delta = np.where(close > close.shift(1), volume,
                             np.where(close < close.shift(1), -volume, 0))
        features['tech_obv'] = pd.Series(obv_delta, index=close.index).cumsum()

        for win in mfi_windows:
            win = max(1, int(win))
            positive_flow = (typical_price.where(typical_price > typical_price.shift(1), 0) * volume).rolling(win).sum()
            negative_flow = (typical_price.where(typical_price < typical_price.shift(1), 0).abs() * volume).rolling(win).sum()
            mfr = positive_flow / (negative_flow + 1e-9)
            features[f'tech_mfi_{win}'] = 100 - (100 / (1 + mfr))

        for win in cci_windows:
            win = max(1, int(win))
            tp_ma = typical_price.rolling(win).mean()
            tp_md = (typical_price - tp_ma).abs().rolling(win).mean()
            features[f'tech_cci_{win}'] = (typical_price - tp_ma) / (0.015 * tp_md.replace(0, np.nan))

        lowest_low = low.rolling(rsv_window).min()
        highest_high = high.rolling(rsv_window).max()
        rsv = ((close - lowest_low) / (highest_high - lowest_low + 1e-9)) * 100
        k = rsv.ewm(alpha=1 / k_smooth, adjust=False).mean()
        d = k.ewm(alpha=1 / d_smooth, adjust=False).mean()
        j = 3 * k - 2 * d
        features['tech_kdj_k'] = k
        features['tech_kdj_d'] = d
        features['tech_kdj_j'] = j

        features = features.replace([np.inf, -np.inf], np.nan).ffill()
        if base_index is not None:
            features = features.reindex(base_index, method='ffill')
        features = features.fillna(0)
        return features

    def _resample_ohlcv(self, ohlcv: pd.DataFrame, rule: str) -> pd.DataFrame:
        agg = {
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }
        try:
            resampled = ohlcv.resample(rule).agg(agg)
            return resampled.dropna()
        except Exception as e:
            self.logger.warning(f"âš ï¸ é‡æ¡æ¨£å¤±æ•— rule={rule}: {e}")
            return pd.DataFrame(columns=ohlcv.columns)

    def calculate_comprehensive_metrics(self, y_true, y_pred, y_pred_proba=None, returns=None):
        metrics = {}
        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)

        if y_pred_proba is not None:
            metrics['auc_macro'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovo', average='macro')
        else:
            metrics['auc_macro'] = 0.5

        # Class distribution
        unique, counts = np.unique(y_true, return_counts=True)
        total = len(y_true)
        metrics['class_distribution'] = {f'class_{int(i)}': count/total for i, count in zip(unique, counts)}

        signal_strength = np.abs(y_pred - 1)
        metrics['avg_signal_strength'] = np.mean(signal_strength)
        metrics['strong_signal_ratio'] = np.mean(signal_strength > 0.5)

        if returns is not None:
            positions = y_pred - 1  # Simplified
            strategy_returns = positions * returns.shift(-1)
            metrics['strategy_return'] = np.sum(strategy_returns.dropna())
            std = np.std(strategy_returns.dropna())
            metrics['sharpe'] = np.mean(strategy_returns.dropna()) / std * np.sqrt(252) if std > 0 else 0
            metrics['win_rate'] = np.mean(strategy_returns.dropna() > 0)
            metrics['max_drawdown'] = np.min(strategy_returns.cumsum().dropna() - strategy_returns.cumsum().dropna().expanding().max())
            trades = np.abs(np.diff(positions))
            metrics['trade_frequency'] = np.sum(trades) / len(trades) if len(trades) > 0 else 0

        self.logger.info(f"ğŸ“Š Layer2æ€§èƒ½æŒ‡æ¨™ï¼šF1={metrics['f1_weighted']:.4f}, æº–ç¢ºç‡={metrics['accuracy']:.4f}, AUC={metrics['auc_macro']:.4f}")
        return metrics

    def _evaluate_trading_performance(self, model, X_final, y, prices) -> float:
        """ğŸš€ 123.mdå»ºè­°ï¼šç°¡åŒ–ç‰ˆäº¤æ˜“æ€§èƒ½è©•ä¼°ï¼ˆå‹ç‡ã€å¤æ™®ç‡ç´„æŸï¼‰"""
        try:
            # è¨“ç·´æ¨¡å‹ä¸¦é æ¸¬
            model.fit(X_final, y)
            predictions = model.predict(X_final)

            # è¨ˆç®—äº¤æ˜“æ”¶ç›Š - ä¿®å¾©numpyæ•¸çµ„ç´¢å¼•å•é¡Œ
            positions = pd.Series((predictions - 1), index=y.index)  # è½‰æ›ç‚º {-1, 0, 1} Series
            returns = prices.pct_change().shift(-1)  # ä¸‹ä¸€æœŸæ”¶ç›Š

            # å°é½Šæ•¸æ“š
            common_idx = positions.index.intersection(returns.index)
            positions_aligned = positions.reindex(common_idx)
            returns_aligned = returns.reindex(common_idx)

            # è¨ˆç®—ç­–ç•¥æ”¶ç›Š
            strategy_returns = positions_aligned * returns_aligned
            strategy_returns = strategy_returns.dropna()

            if len(strategy_returns) == 0:
                return -0.5  # æ‡²ç½°

            # è¨ˆç®—é—œéµæŒ‡æ¨™
            win_rate = np.mean(strategy_returns > 0)
            total_return = strategy_returns.sum()

            # è¨ˆç®—å¤æ™®ç‡
            if strategy_returns.std() > 0:
                sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252 * 24 * 4)  # 15åˆ†é˜æ•¸æ“š
            else:
                sharpe = 0

            # æœˆå›å ±ä¼°ç®—ï¼ˆå‡è¨­252å€‹äº¤æ˜“æ—¥ï¼‰
            monthly_return = total_return * 21 / len(strategy_returns) if len(strategy_returns) > 0 else 0

            # ğŸš€ 123.mdç›®æ¨™ç´„æŸï¼šå‹ç‡â‰¥60%ã€å¤æ™®ç‡â‰¥1.2ã€æœˆå›å ±â‰¥10%
            penalty = 0
            if win_rate < 0.60:
                penalty -= 0.2 * (0.60 - win_rate)
            if sharpe < 1.2:
                penalty -= 0.1 * (1.2 - sharpe)
            if monthly_return < 0.10:
                penalty -= 0.1 * (0.10 - monthly_return)

            # çå‹µè¶…é¡è¡¨ç¾
            bonus = 0
            if win_rate > 0.65:
                bonus += 0.1 * (win_rate - 0.65)
            if sharpe > 1.5:
                bonus += 0.1 * (sharpe - 1.5)

            performance_score = penalty + bonus

            self.logger.info(f"ğŸ“ˆ äº¤æ˜“æ€§èƒ½: å‹ç‡={win_rate:.3f}, å¤æ™®={sharpe:.3f}, æœˆå›å ±={monthly_return:.3f}, åˆ†æ•¸={performance_score:.3f}")

            return performance_score

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ€§èƒ½è©•ä¼°å¤±æ•—: {e}")
            return -0.2  # å°å¹…æ‡²ç½°

    def _calculate_trade_frequency_penalty(self, labels, lag) -> float:
        """ğŸš€ 123.mdå»ºè­°ï¼šäº¤æ˜“é »ç‡æ§åˆ¶ï¼ˆé¿å…éåº¦äº¤æ˜“ï¼‰"""
        try:
            # è¨ˆç®—ä¿¡è™Ÿè®ŠåŒ–é »ç‡
            signal_changes = np.abs(np.diff(labels))
            change_rate = np.sum(signal_changes > 0) / len(signal_changes) if len(signal_changes) > 0 else 0

            # æ ¹æ“šlagèª¿æ•´åˆç†äº¤æ˜“é »ç‡
            # lagè¶Šå°ï¼Œå…è¨±çš„äº¤æ˜“é »ç‡è¶Šé«˜
            max_reasonable_freq = 0.3 / (lag / 12.0)  # åŸºæ–¼12æœŸåŸºæº–èª¿æ•´

            # å¦‚æœäº¤æ˜“éæ–¼é »ç¹ï¼Œæ–½åŠ æ‡²ç½°
            if change_rate > max_reasonable_freq:
                penalty = 0.1 * (change_rate - max_reasonable_freq) / max_reasonable_freq
                self.logger.info(f"âš ï¸ äº¤æ˜“é »ç‡éé«˜: {change_rate:.3f} > {max_reasonable_freq:.3f}, æ‡²ç½°={penalty:.3f}")
                return min(penalty, 0.3)  # æœ€å¤§æ‡²ç½°30%

            return 0.0  # ç„¡æ‡²ç½°

        except Exception as e:
            self.logger.warning(f"âš ï¸ äº¤æ˜“é »ç‡è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def _shuffle_label_validation(self, X: pd.DataFrame, y: pd.Series, 
                                  model, current_score: float, threshold: float = 0.4) -> tuple:
        """ğŸ¯ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦ä¾èµ–çœŸå®æ ‡ç­¾æ¨¡å¼"""
        try:
            self.logger.info("ğŸ”€ æ‰§è¡Œæ‰“ä¹±æ ‡ç­¾æµ‹è¯•...")
            
            # åˆ›å»ºæ‰“ä¹±çš„æ ‡ç­¾
            y_shuffled = y.copy().values
            np.random.shuffle(y_shuffled)
            y_shuffled_series = pd.Series(y_shuffled, index=y.index)
            
            # ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å’Œæ‰“ä¹±çš„æ ‡ç­¾è¿›è¡Œäº¤å‰éªŒè¯
            shuffled_scores = cross_val_score(
                model, X, y_shuffled_series, 
                cv=TimeSeriesSplit(n_splits=3), 
                scoring='f1_weighted',
                n_jobs=-1
            )
            shuffled_score = shuffled_scores.mean()
            
            # æ£€æŸ¥æ‰“ä¹±æ ‡ç­¾åçš„æ€§èƒ½
            if shuffled_score > threshold:
                # æ‰“ä¹±æ ‡ç­¾åä»æœ‰é¢„æµ‹èƒ½åŠ›ï¼Œè¯´æ˜å­˜åœ¨æ•°æ®æ³„éœ²æˆ–è¿‡æ‹Ÿåˆ
                return False, f"âŒ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•å¤±è´¥: æ‰“ä¹±åF1={shuffled_score:.4f} > {threshold}ï¼Œç–‘ä¼¼æ•°æ®æ³„éœ²"
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™æ¯”ä¾‹
            performance_drop = (current_score - shuffled_score) / current_score if current_score > 0 else 0
            if performance_drop < 0.3:  # æ€§èƒ½ä¸‹é™ä¸è¶³30%
                return False, f"âŒ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•å¤±è´¥: æ€§èƒ½ä¸‹é™ä»…{performance_drop:.1%}ï¼Œæ¨¡å‹å¯èƒ½æœªå­¦ä¹ åˆ°æœ‰æ•ˆæ¨¡å¼"
            
            self.logger.info(f"âœ… æ‰“ä¹±æ ‡ç­¾æµ‹è¯•é€šè¿‡: åŸå§‹F1={current_score:.4f}, æ‰“ä¹±åF1={shuffled_score:.4f}, ä¸‹é™{performance_drop:.1%}")
            return True, f"âœ… æ‰“ä¹±æ ‡ç­¾æµ‹è¯•é€šè¿‡ï¼Œæ€§èƒ½åˆç†ä¸‹é™{performance_drop:.1%}"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•å¤±è´¥: {e}")
            return True, f"âš ï¸ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•å¼‚å¸¸ä½†ç»§ç»­: {e}"

    def _random_feature_validation(self, X: pd.DataFrame, y: pd.Series, 
                                   model, current_score: float, threshold: float = 0.4) -> tuple:
        """ğŸ¯ éšæœºç‰¹å¾æµ‹è¯•ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦ä»…ä¾èµ–å™ªå£°ç‰¹å¾"""
        try:
            self.logger.info("ğŸ² æ‰§è¡Œéšæœºç‰¹å¾æµ‹è¯•...")
            
            # ç”Ÿæˆä¸åŸå§‹ç‰¹å¾ç›¸åŒç»´åº¦çš„éšæœºç‰¹å¾
            X_random = pd.DataFrame(
                np.random.randn(*X.shape), 
                index=X.index, 
                columns=[f'random_feature_{i}' for i in range(X.shape[1])]
            )
            
            # ä½¿ç”¨éšæœºç‰¹å¾å’ŒçœŸå®æ ‡ç­¾è¿›è¡Œäº¤å‰éªŒè¯
            random_scores = cross_val_score(
                model, X_random, y, 
                cv=TimeSeriesSplit(n_splits=3), 
                scoring='f1_weighted',
                n_jobs=-1
            )
            random_score = random_scores.mean()
            
            # æ£€æŸ¥éšæœºç‰¹å¾çš„é¢„æµ‹èƒ½åŠ›
            if random_score > threshold:
                # éšæœºç‰¹å¾æœ‰é¢„æµ‹èƒ½åŠ›ï¼Œè¯´æ˜å¯èƒ½å­˜åœ¨é—®é¢˜
                return False, f"âŒ éšæœºç‰¹å¾æµ‹è¯•å¤±è´¥: éšæœºç‰¹å¾F1={random_score:.4f} > {threshold}ï¼Œæ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆ"
            
            # æ£€æŸ¥çœŸå®ç‰¹å¾vséšæœºç‰¹å¾çš„æ€§èƒ½å·®å¼‚
            feature_advantage = (current_score - random_score) / current_score if current_score > 0 else 0
            if feature_advantage < 0.2:  # çœŸå®ç‰¹å¾ä¼˜åŠ¿ä¸è¶³20%
                return False, f"âŒ éšæœºç‰¹å¾æµ‹è¯•å¤±è´¥: çœŸå®ç‰¹å¾ä¼˜åŠ¿ä»…{feature_advantage:.1%}ï¼Œç‰¹å¾å·¥ç¨‹æ•ˆæœæœ‰é™"
            
            self.logger.info(f"âœ… éšæœºç‰¹å¾æµ‹è¯•é€šè¿‡: çœŸå®ç‰¹å¾F1={current_score:.4f}, éšæœºç‰¹å¾F1={random_score:.4f}, ä¼˜åŠ¿{feature_advantage:.1%}")
            return True, f"âœ… éšæœºç‰¹å¾æµ‹è¯•é€šè¿‡ï¼ŒçœŸå®ç‰¹å¾æ˜¾è‘—ä¼˜äºéšæœºç‰¹å¾{feature_advantage:.1%}"
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ éšæœºç‰¹å¾æµ‹è¯•å¤±è´¥: {e}")
            return True, f"âš ï¸ éšæœºç‰¹å¾æµ‹è¯•å¼‚å¸¸ä½†ç»§ç»­: {e}"

    def _validate_score_legitimacy(self, score: float, X: pd.DataFrame, y: pd.Series, model) -> tuple:
        """ğŸš€ æ–‡æ¡£ä¿®å¤ç‰ˆï¼šå¤šé‡éªŒè¯ç¡®ä¿åˆ†æ•°çœŸå®æ€§ï¼ˆç§»é™¤0.8ç¡¬é˜ˆå€¼ï¼‰"""
        try:
            self.logger.info(f"ğŸ” å¼€å§‹å¤šé‡éªŒè¯åˆ†æ•°: {score:.4f}")

            # 1. ğŸš€ éšæœºæ•°æ®æµ‹è¯•
            X_random = pd.DataFrame(np.random.randn(*X.shape), index=X.index, columns=X.columns)
            try:
                random_scores = cross_val_score(model, X_random, y, cv=TimeSeriesSplit(n_splits=3), n_jobs=-1)
                random_score = random_scores.mean()
                if random_score > 0.4:  # éšæœºæ•°æ®ä¸åº”æœ‰é¢„æµ‹èƒ½åŠ›
                    return False, f"âŒ éšæœºæ•°æ®æµ‹è¯•å¤±è´¥: {random_score:.3f} > 0.4"
                self.logger.info(f"âœ… éšæœºæ•°æ®æµ‹è¯•é€šè¿‡: {random_score:.3f}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ éšæœºæ•°æ®æµ‹è¯•å¤±è´¥: {e}")

            # 2. ğŸš€ æ ‡ç­¾æ‰“ä¹±æµ‹è¯•
            y_shuffled = y.copy().values
            np.random.shuffle(y_shuffled)
            y_shuffled_series = pd.Series(y_shuffled, index=y.index)
            try:
                shuffled_scores = cross_val_score(model, X, y_shuffled_series, cv=TimeSeriesSplit(n_splits=3), n_jobs=-1)
                shuffled_score = shuffled_scores.mean()
                if shuffled_score > 0.4:  # æ‰“ä¹±æ ‡ç­¾ä¸åº”æœ‰é¢„æµ‹èƒ½åŠ›
                    return False, f"âŒ æ ‡ç­¾æ‰“ä¹±æµ‹è¯•å¤±è´¥: {shuffled_score:.3f} > 0.4"
                self.logger.info(f"âœ… æ ‡ç­¾æ‰“ä¹±æµ‹è¯•é€šè¿‡: {shuffled_score:.3f}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ ‡ç­¾æ‰“ä¹±æµ‹è¯•å¤±è´¥: {e}")

            # 3. ğŸš€ æ—¶é—´åºåˆ—å‰å‘éªŒè¯å·®å¼‚æ£€æŸ¥
            try:
                tscv = TimeSeriesSplit(n_splits=5)
                ts_scores = cross_val_score(model, X, y, cv=tscv, n_jobs=-1)
                ts_score = ts_scores.mean()
                score_diff = abs(score - ts_score)
                if score_diff > 0.15:  # æ—¶åºéªŒè¯ä¸å¸¸è§„éªŒè¯å·®å¼‚è¿‡å¤§
                    return False, f"âŒ æ—¶åºéªŒè¯å·®å¼‚è¿‡å¤§: {score:.3f} vs {ts_score:.3f} (diff={score_diff:.3f})"
                self.logger.info(f"âœ… æ—¶åºéªŒè¯ä¸€è‡´æ€§é€šè¿‡: å·®å¼‚={score_diff:.3f}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ—¶åºéªŒè¯æµ‹è¯•å¤±è´¥: {e}")

            # 4. ğŸš€ ç‰¹å¾é‡è¦æ€§åˆç†æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            if hasattr(model, 'feature_importances_'):
                try:
                    importances = model.feature_importances_
                    max_importance = np.max(importances)
                    if max_importance > 0.8:  # å•ä¸ªç‰¹å¾è´¡çŒ®è¿‡å¤§
                        return False, f"âŒ å•ä¸ªç‰¹å¾æƒé‡è¿‡é«˜: {max_importance:.3f} > 0.8ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ"
                    self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§å¹³è¡¡: æœ€å¤§æƒé‡={max_importance:.3f}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ç‰¹å¾é‡è¦æ€§æ£€æŸ¥å¤±è´¥: {e}")

            # 5. ğŸš€ åˆ†æ•°åˆç†æ€§è¯„ä¼°ï¼ˆæ— ç¡¬ä¸Šé™ï¼‰
            if score > 0.9:
                self.logger.warning(f"âš ï¸ åˆ†æ•°æé«˜ {score:.4f}ï¼Œå»ºè®®é¢å¤–éªŒè¯ä½†ä¸æ‹’ç»")
            elif score > 0.75:
                self.logger.info(f"ğŸ“ˆ åˆ†æ•°è¾ƒé«˜ {score:.4f}ï¼Œç¬¦åˆä¼˜ç§€ç‰¹å¾å·¥ç¨‹é¢„æœŸ")
            elif score > 0.5:
                self.logger.info(f"ğŸ“Š åˆ†æ•°æ­£å¸¸ {score:.4f}ï¼Œç¬¦åˆé¢„æœŸèŒƒå›´")
            else:
                self.logger.info(f"ğŸ“‰ åˆ†æ•°è¾ƒä½ {score:.4f}ï¼Œå¯ç»§ç»­ä¼˜åŒ–")

            return True, "âœ… å¤šé‡éªŒè¯é€šè¿‡"

        except Exception as e:
            self.logger.error(f"âŒ å¤šé‡éªŒè¯å¤±è´¥: {e}")
            return True, f"âš ï¸ éªŒè¯å¼‚å¸¸ä½†ç»§ç»­: {e}"  # éªŒè¯å¤±è´¥æ—¶ä¸é˜»æ­¢ä¼˜åŒ–

    def _get_env_float(self, name: str, default: float) -> float:
        try:
            return float(os.getenv(name, str(default)))
        except Exception:
            return default

    def _soft_penalize_kpis(self, metrics: Dict[str, float]) -> Dict[str, float]:
        penalties = dict(metrics)
        min_win = self._get_env_float("L2_MIN_WINRATE", 0.65)
        min_sharpe = self._get_env_float("L2_MIN_SHARPE", 1.2)
        max_dd = self._get_env_float("L2_MAX_DRAWDOWN", 0.25)

        if penalties.get("sharpe", 0.0) < min_sharpe:
            penalties["sharpe"] = 0.1
        if penalties.get("win_rate", 0.0) < min_win:
            penalties["win_rate"] = 0.01
        if penalties.get("profit_factor", 0.0) < 1.0:
            penalties["profit_factor"] = 0.9
        if penalties.get("total_return", 0.0) <= 0:
            penalties["total_return"] = -0.1
        if penalties.get("annual_return", 0.0) <= 0:
            penalties["annual_return"] = -0.1
        if penalties.get("max_drawdown", 1.0) > max_dd:
            penalties["max_drawdown"] = 1.0
        return penalties

    def _kpis_to_multi_values(self, metrics: Dict[str, float]) -> List[float]:
        return [
            metrics.get("sharpe", 0.0),
            metrics.get("calmar", metrics.get("sortino", 0.0)),
            metrics.get("profit_factor", 0.0),
            metrics.get("win_rate", 0.0),
            metrics.get("total_return", 0.0),
            metrics.get("annual_return", 0.0),
            -metrics.get("max_drawdown", 1.0),
        ]

    def objective(self, trial: optuna.Trial) -> float:
        """ğŸš€ 123.mdå»ºè­°ï¼šåƒæ•¸åŒ–æ¨™ç±¤ç”Ÿæˆ + æ€§èƒ½ç´„æŸçš„ç›®æ¨™å‡½æ•¸"""

        try:
            # Phase 4.1: æ“´å±•ç©ºé–“
            selection_method = trial.suggest_categorical('feature_selection_method', ['stability', 'mutual_info'])
            noise_reduction = trial.suggest_categorical('noise_reduction', [True, False])
            feature_interaction = trial.suggest_categorical('feature_interaction', [False, True])
            # ğŸš€ Fail-Fastæª¢æŸ¥é åŠ è¼‰æ•¸æ“š
            if len(self.features) == 0 or len(self.close_prices) == 0:
                raise ValueError("é åŠ è¼‰æ•¸æ“šç‚ºç©ºï¼Œåˆå§‹åŒ–å¤±æ•—")

            # ğŸš€ æ–°å¢ï¼šè®€å–Layer1æ¨™ç±¤å„ªåŒ–çµæœï¼Œå¯¦ç¾è¯å‹•å„ªåŒ–
            layer1_params = None
            try:
                tf_specific = self.config_path / f"label_params_{self.timeframe}.json"
                layer1_config_file = tf_specific if tf_specific.exists() else (self.config_path / "label_params.json")
                if layer1_config_file.exists():
                    with open(layer1_config_file, 'r', encoding='utf-8') as f:
                        layer1_result = json.load(f)
                        layer1_params = layer1_result.get('best_params', {})
                        self.logger.info(
                            f"ğŸ“– è®€å–Layer1å„ªåŒ–çµæœ: lag={layer1_params.get('lag')}, "
                            f"buy_q={layer1_params.get('buy_quantile', 0):.3f}"
                        )
            except Exception as e:
                self.logger.warning(f"ç„¡æ³•è®€å–Layer1çµæœ: {e}")

            base_lag_min, base_lag_max = self.scaler.get_base_lag_range(self.timeframe)
            lag_meta_min, lag_meta_max = self.scaler.adjust_lag_range_with_meta(
                self.timeframe,
                (base_lag_min, base_lag_max),
                self.scaled_config.get('meta_vol', 0.02)
            )

            # ğŸš€ ä¿®å¾©ç‰ˆï¼šè‡ªé©æ‡‰åƒæ•¸ç¯„åœè¨­ç½®
            total_features = len(self.features.columns)
            data_size = len(self.close_prices)
            
            # ğŸš€ Layer1è¯å‹•ï¼šåŸºæ–¼æ¨™ç±¤è³ªé‡å‹•æ…‹èª¿æ•´ç‰¹å¾µåƒæ•¸
            feature_boost = 0
            lookback_reduction = 0
            if layer1_params:
                # åˆ†æLayer1æ¨™ç±¤è³ªé‡
                buy_q = layer1_params.get('buy_quantile', 0.7)
                sell_q = layer1_params.get('sell_quantile', 0.3)
                
                # è¨ˆç®—æ¨™ç±¤ç†µä¼°è¨ˆï¼ˆåˆ†ä½æ•¸å·®è·è¶Šå°ï¼Œæ¨™ç±¤åˆ†ä½ˆè¶Šå‡å‹»ï¼‰
                quantile_gap = buy_q - sell_q
                estimated_entropy = 1.2 - (quantile_gap - 0.4) * 2  # ç¶“é©—å…¬å¼
                
                # æ¨™ç±¤ç†µä½æ–¼0.9æ™‚ï¼Œå¢åŠ ç‰¹å¾µæ•¸é‡ä»¥æå‡å€åˆ†èƒ½åŠ›
                if estimated_entropy < 0.9:
                    feature_boost = 20
                    self.logger.info(f"ğŸ”§ æ¨™ç±¤ç†µåä½({estimated_entropy:.3f})ï¼Œå¢åŠ ç‰¹å¾µæ•¸é‡+{feature_boost}")
                
                # æ¨™ç±¤æŒæœ‰ç‡åé«˜æ™‚ï¼Œç¸®å°lookbackçª—å£ä»¥å¢åŠ éˆæ•åº¦
                target_hold = layer1_params.get('target_hold_ratio', 0.5)
                if target_hold > 0.6:
                    lookback_reduction = 100
                    self.logger.info(f"ğŸ”§ ç›®æ¨™æŒæœ‰ç‡åé«˜({target_hold:.1%})ï¼Œç¸®çŸ­lookbackçª—å£-{lookback_reduction}")

            # ğŸš€ ä¿®å¤ç‰ˆï¼šåŸºäºLayer1ç»“æœåŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´
            if layer1_params:
                layer1_range_mode = self.flags.get('layer1_range_mode', 'narrow')
                l1_lag = layer1_params.get('lag', 12)
                feature_lag_min_default = lag_meta_min
                feature_lag_max_default = lag_meta_max
                min_lag = max(feature_lag_min_default, int(l1_lag) - 4)
                max_lag = min(feature_lag_max_default, int(l1_lag) + 4)
                if min_lag > max_lag:
                    max_lag = min_lag
                self.logger.info(f"ğŸ”— Layer1è¯å‹•lagé„°åŸŸ: {l1_lag} Â±4 â†’ æœç´¢ç¯„åœ[{min_lag}, {max_lag}]")
                
                l1_buy_q = layer1_params.get('buy_quantile', 0.75)
                l1_sell_q = layer1_params.get('sell_quantile', 0.25)

                if layer1_range_mode == 'wide':
                    profit_quantile_min = max(0.72, l1_buy_q - 0.04)
                    profit_quantile_max = min(0.78, l1_buy_q + 0.04)
                    loss_quantile_min = max(0.05, min(0.28, l1_sell_q - 0.04))
                    loss_quantile_max = min(0.45, max(0.06, l1_sell_q + 0.04))
                else:
                    profit_quantile_min = max(0.73, l1_buy_q - 0.02)
                    profit_quantile_max = min(0.77, l1_buy_q + 0.02)
                    loss_quantile_min = max(0.05, min(0.27, l1_sell_q - 0.02))
                    loss_quantile_max = min(0.45, max(0.06, l1_sell_q + 0.02))

                # å®‰å…¨æ ¡æ­£ï¼šé¿å…ä½æ–¼é«˜ï¼ˆOptuna è¦æ±‚ low<=highï¼‰
                if loss_quantile_min > loss_quantile_max:
                    mid = float(l1_sell_q)
                    span = 0.01
                    loss_quantile_min, loss_quantile_max = max(0.03, mid - span), min(0.49, mid + span)
                if self.timeframe == "15m":
                    loss_quantile_max = min(loss_quantile_max, 0.20)
                if profit_quantile_min > profit_quantile_max:
                    mid = float(l1_buy_q)
                    span = 0.01
                    profit_quantile_min, profit_quantile_max = max(0.51, mid - span), min(0.97, mid + span)
                
                self.logger.info(f"ğŸ”— Layer1è¯å‹•åˆ†ä½æ•¸: buy_q={l1_buy_q:.3f} â†’ profit_q[{profit_quantile_min:.3f}, {profit_quantile_max:.3f}]")
                self.logger.info(f"ğŸ”— Layer1è¯å‹•åˆ†ä½æ•¸: sell_q={l1_sell_q:.3f} â†’ loss_q[{loss_quantile_min:.3f}, {loss_quantile_max:.3f}]")
            else:
                min_lag = self.scaler.get_base_lag_range(self.timeframe)[0]
                max_lag = lag_meta_max
                profit_quantile_min, profit_quantile_max = (0.72, 0.78)
                loss_quantile_min, loss_quantile_max = (0.22, 0.28)
                self.logger.warning("âš ï¸ Layer1è¯å‹•å¤±æ•—ï¼Œä½¿ç”¨ç¸®çª„å¾Œçš„åƒæ•¸æœç´¢")
                if min_lag > max_lag:
                    max_lag = min_lag

            lag = trial.suggest_int('lag', min_lag, max_lag)
            # æœ€çµ‚é˜²å‘†ï¼ˆå†æª¢ä¸€æ¬¡ï¼‰
            if loss_quantile_min > loss_quantile_max:
                loss_quantile_min, loss_quantile_max = loss_quantile_max, loss_quantile_min
            if profit_quantile_min > profit_quantile_max:
                profit_quantile_min, profit_quantile_max = profit_quantile_max, profit_quantile_min

            profit_quantile = trial.suggest_float('profit_quantile', float(profit_quantile_min), float(profit_quantile_max))
            loss_quantile = trial.suggest_float('loss_quantile', float(loss_quantile_min), float(loss_quantile_max))

            lookback_window = trial.suggest_int('lookback_window', 450, 550)

            self.logger.info(f"ğŸ“Š è‡ªé©æ‡‰åƒæ•¸: lag={lag}, lookback={lookback_window}, features={total_features}")

            # ğŸš€ ä½¿ç”¨é ç·©å­˜çš„åƒ¹æ ¼åºåˆ—ç”Ÿæˆæ¨™ç±¤
            labels = self._generate_labels(
                self.close_prices,
                lag=lag,
                profit_quantile=profit_quantile,
                loss_quantile=loss_quantile,
                lookback_window=lookback_window
            )

            if self.flags.get('enable_dynamic_label_balance', False):
                labels = self._rebalance_labels(
                    labels,
                lag=lag,
                profit_quantile=profit_quantile,
                loss_quantile=loss_quantile,
                lookback_window=lookback_window
            )

            # ğŸ”§ æ•¸æ“šå°é½Šèˆ‡æ¸…ç†
            common_idx = self.features.index.intersection(labels.index)
            self._ensure_X_full()
            X = self.X_full.reindex(common_idx).astype('float32').fillna(0)
            y = labels.loc[common_idx].astype(int)

            # ç‰¹å¾µè³ªé‡éæ¿¾
            X = self._filter_low_quality_features(X)

            if len(X) < 1000:  # ç¢ºä¿æœ‰è¶³å¤ çš„æ•¸æ“š
                return 0.0

            n_features = len(X.columns)
            self.logger.info(f"ğŸ“Š ç¸½ç‰¹å¾µæ•¸: {n_features}")

            selection_cfg = self.optimize_feature_selection_params(trial, n_features, layer1_params)
            coarse_k = selection_cfg['coarse_k']
            fine_k = selection_cfg['fine_k']
            corr_threshold = selection_cfg['correlation_threshold']

            boost_msg = f", ç‰¹å¾µå¢å¼·+{feature_boost}" if feature_boost > 0 else ""
            lookback_msg = f", çª—å£ç¸®çŸ­-{lookback_reduction}" if lookback_reduction > 0 else ""
            self.logger.info(f"ğŸ”§ Layer1è¯å‹•ç‰¹å¾µé¸æ“‡: coarse_k={coarse_k} ({coarse_k/n_features:.1%}), fine_k={fine_k}{boost_msg}{lookback_msg}")

            # çµ±ä¸€é †åºï¼šå…ˆå»ç›¸é—œ(å°å†—é¤˜) â†’ å†æ‡‰ç”¨ç©©å®šæ€§é®ç½© â†’ å†ç²—/ç²¾é¸
            # æ³¨æ„ï¼šæ­¤è™•ä¸å†æ–¼å…¨è³‡æ–™é›†å±¤ç´šç›´æ¥è£åˆ‡ç‚º stable_colsï¼Œé¿å…"å…ˆç©©å®šå†å»ç›¸é—œ"çš„åå‘æ•ˆæœ

            # ğŸš€ æº–å‚™é¸æ“‡å™¨ç·©å­˜ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
            if not hasattr(self, '_selector_cache'):
                self._selector_cache = {
                    'variance': {},
                    'coarse': {},
                    'fine': {}
                }
            variance_cache = self._selector_cache['variance']
            coarse_cache = self._selector_cache['coarse']
            fine_cache = self._selector_cache['fine']

            current_splits = self.flags.get('cv_splits', 5)

            # ğŸš€ ä½¿ç”¨è‡ªå®šç¾© CV åˆ‡åˆ†ç­–ç•¥ï¼ˆå¤šéšæ®µå¯èª¿æ•´ n_splitsï¼‰
            outer_cv = list(self._make_cv_splits(X, n_splits=current_splits))
            cv_scores = []
            # initialize feature aggregation to avoid NameError
            phase = 'full'
            selection_counts: Counter = Counter()
            selected_union: set = set()
            best_fold_cols: List[str] = []
            best_fold_score: float = -1.0

            self.logger.info(f"ğŸ”„ é–‹å§‹åµŒå¥—äº¤å‰é©—è­‰: n_features={n_features}, {len(X)}æ¨£æœ¬")

            for fold_idx, (train_idx, test_idx) in enumerate(outer_cv):
                try:
                    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                    self.logger.info(f"  Fold {fold_idx+1}: è¨“ç·´={len(X_train)}, æ¸¬è©¦={len(X_test)}")

                    variance_key = (current_splits, fold_idx)
                    if variance_key in variance_cache:
                        cols_var = variance_cache[variance_key]
                    else:
                        variance_selector = VarianceThreshold(threshold=0.01)
                        variance_selector.fit(X_train)
                        cols_var = X_train.columns[variance_selector.get_support(indices=True)].tolist()
                        variance_cache[variance_key] = cols_var

                    if len(cols_var) == 0:
                        cv_scores.append(0.0)
                        continue

                    X_train_var = X_train[cols_var]
                    X_test_var = X_test[cols_var]

                    # å…ˆåšä¸€æ¬¡ç›®æ¨™ç›¸é—œæ€§éæ¿¾ï¼ˆmutual informationï¼‰
                    try:
                        tc_keep_ratio = float(self.flags.get('target_corr_keep_ratio', 0.6))
                        tc_min_score = float(self.flags.get('target_corr_min_score', 0.0))
                        tc_cols = self._filter_by_target_correlation(X_train_var, y_train, keep_ratio=tc_keep_ratio, min_score=tc_min_score)
                        if len(tc_cols) >= 3:
                            X_train_var = X_train_var[tc_cols]
                            X_test_var = X_test_var[tc_cols]
                    except Exception as e:
                        self.logger.warning(f"target-corr éæ¿¾è·³é: {e}")

                    # å†åšä¸€æ¬¡å»å†—é¤˜ï¼ˆpairwise correlationï¼‰
                    pre_corr_cols = self._remove_correlated_features_smart(X_train_var, corr_threshold)
                    if len(pre_corr_cols) >= 3:
                        X_train_var = X_train_var[pre_corr_cols]
                        X_test_var = X_test_var[pre_corr_cols]

                    # å†æ‡‰ç”¨ç©©å®šæ€§é®ç½©ï¼ˆè‹¥æœ‰ï¼‰ï¼Œé¿å…èˆ‡å‰ä¸€æ­¥è¡çªè€Œé€ æˆç‰¹å¾µé›†éåº¦æ”¶ç¸®
                    if hasattr(self, 'stable_cols') and self.stable_cols:
                        pre_keep = [c for c in X_train_var.columns if c in self.stable_cols]
                        if len(pre_keep) >= max(10, int(0.2 * X_train_var.shape[1])):
                            X_train_var = X_train_var[pre_keep]
                            X_test_var = X_test_var[pre_keep]

                    fold_n_features = X_train_var.shape[1]
                    fold_coarse_k = min(coarse_k, int(fold_n_features * 0.80), fold_n_features - 1)
                    if fold_coarse_k < 10:
                        # ä¿åº•ç­–ç•¥ï¼šæ¡ç”¨è®Šç•°åº¦ Top-10 é¿å…ç›´æ¥ 0 åˆ†
                        self.logger.warning(f"  Fold {fold_idx+1}: ç‰¹å¾µæ•¸ä¸è¶³ {fold_n_features}ï¼Œå•Ÿç”¨ä¿åº•ç­–ç•¥ï¼ˆTop-10 by varianceï¼‰")
                        try:
                            variances = X_train_var.var().sort_values(ascending=False)
                            top_k = min(10, max(3, X_train_var.shape[1] - 1))
                            fallback_cols = variances.head(top_k).index.tolist()
                            if len(fallback_cols) >= 3:
                                X_train_var = X_train_var[fallback_cols]
                                X_test_var = X_test_var[fallback_cols]
                                fold_n_features = X_train_var.shape[1]
                                fold_coarse_k = min(top_k, fold_n_features - 1)
                            else:
                                cv_scores.append(0.0)
                                continue
                        except Exception as e:
                            self.logger.warning(f"  ä¿åº•ç­–ç•¥å¤±æ•—: {e}")
                            cv_scores.append(0.0)
                            continue

                    cols_var_tuple = tuple(cols_var)
                    coarse_key = (current_splits, fold_idx, tuple(X_train_var.columns), fold_coarse_k)
                    if coarse_key in coarse_cache:
                        cols_coarse = coarse_cache[coarse_key]
                    else:
                        coarse_selector = SelectKBest(
                            f_classif if selection_method == 'stability' else mutual_info_classif,
                            k=fold_coarse_k
                        )
                        coarse_selector.fit(X_train_var, y_train)
                        cols_coarse = X_train_var.columns[coarse_selector.get_support(indices=True)].tolist()
                        coarse_cache[coarse_key] = cols_coarse

                    if len(cols_coarse) == 0:
                        cv_scores.append(0.0)
                        continue

                    # å°é½Š coarse å­é›†æ¬„ä½ï¼ˆå«åˆ¥åè§£æèˆ‡äº¤é›†ä¿è­·ï¼‰
                    cols_coarse = self._normalize_column_subset(cols_coarse, X_train_var, X_test_var)
                    if len(cols_coarse) < 3:
                        cv_scores.append(0.0)
                        continue
                    X_train_coarse = X_train_var[cols_coarse]
                    X_test_coarse = X_test_var[cols_coarse]

                    coarse_n_features = X_train_coarse.shape[1]
                    fold_fine_k = min(fine_k, coarse_n_features - 1)

                    fine_key = (current_splits, fold_idx, tuple(X_train_coarse.columns), fold_fine_k)
                    if fine_key in fine_cache:
                        cols_fine = fine_cache[fine_key]
                    else:
                        fine_selector = SelectKBest(mutual_info_classif, k=fold_fine_k)
                        fine_selector.fit(X_train_coarse, y_train)
                        cols_fine = X_train_coarse.columns[fine_selector.get_support(indices=True)].tolist()
                        fine_cache[fine_key] = cols_fine

                    if len(cols_fine) < 3:
                        cv_scores.append(0.0)
                        continue

                    # å°é½Š fine å­é›†æ¬„ä½ï¼ˆå«åˆ¥åè§£æèˆ‡äº¤é›†ä¿è­·ï¼‰
                    cols_fine = self._normalize_column_subset(cols_fine, X_train_coarse, X_test_coarse)
                    if len(cols_fine) < 3:
                        cv_scores.append(0.0)
                        continue
                    X_train_fine_df = X_train_coarse[cols_fine]
                    X_test_fine_df = X_test_coarse[cols_fine]

                    # æœ€çµ‚ä¸å†é‡è¤‡å»ç›¸é—œï¼Œé¿å…"é‡è¤‡ç "é€ æˆä¸ä¸€è‡´
                    selected_cols = list(X_train_fine_df.columns)
                    df_train_final = X_train_fine_df[selected_cols]
                    X_test_fine_df = X_test_fine_df[selected_cols]

                    if df_train_final.shape[1] < 3:
                        cv_scores.append(0.0)
                        continue

                    # å¯é¸äº¤äº’ç‰¹å¾µï¼ˆé™åˆ¶åœ¨è¼ƒå°ç¶­åº¦ï¼‰
                    X_train_final = df_train_final.values
                    X_test_final = X_test_fine_df.values
                    interaction_max_dim = int(self.flags.get('interaction_max_dim', 40))
                    interaction_degree = int(self.flags.get('interaction_degree', 2))
                    if feature_interaction and df_train_final.shape[1] <= interaction_max_dim:
                        try:
                            poly = PolynomialFeatures(degree=interaction_degree, include_bias=False)
                            X_train_final = poly.fit_transform(X_train_final)
                            X_test_final = poly.transform(X_test_final)
                        except Exception as e:
                            self.logger.warning(f"äº¤äº’ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")

                    # Phase 2.2: æ¯æŠ˜æ¨™æº–åŒ– + å¯é¸PCAï¼ˆä¿ç•™95%æ–¹å·®ï¼‰
                    try:
                        scaler = StandardScaler()
                        X_train_final = scaler.fit_transform(X_train_final)
                        X_test_final = scaler.transform(X_test_final)
                        # PCA æˆåˆ†å›çŒï¼šå¦‚å•Ÿç”¨å‰‡å°‡å‰Kä¸»æˆåˆ†åŠ å…¥è€Œéå®Œå…¨å–ä»£
                        pca_threshold = int(self.flags.get('pca_dim_threshold', 60))
                        pca_variance = float(self.flags.get('pca_variance', 0.95))
                        max_pcs = int(self.flags.get('pca_max_components', 30))
                        if noise_reduction and df_train_final.shape[1] > pca_threshold:
                            pca = PCA(n_components=pca_variance, random_state=42)
                            pca.fit(X_train_final)
                            X_train_pca = pca.transform(X_train_final)
                            X_test_pca = pca.transform(X_test_final)
                            if isinstance(pca.n_components_, int):
                                n_pcs = min(max_pcs, int(pca.n_components_))
                            else:
                                n_pcs = min(max_pcs, X_train_pca.shape[1])
                            X_train_final = np.hstack([X_train_final, X_train_pca[:, :n_pcs]])
                            X_test_final = np.hstack([X_test_final, X_test_pca[:, :n_pcs]])
                    except Exception as e:
                        self.logger.warning(f"æ¨™æº–åŒ–/PCAå¤±æ•—: {e}")

                    classifier = self.create_enhanced_classifier(trial, df_train_final.shape[1])

                    # æ—©æœŸæ–·è¨€ï¼šé¿å…å¸¸æ•¸æ¨™ç±¤æˆ–æ¥µä½å¤šæ¨£æ€§é€ æˆ LGBM ç„¡æ³•åˆ†è£‚
                    try:
                        if pd.Series(y_train).nunique() <= 1:
                            self.logger.warning("âš ï¸ ç•¥éæœ¬æŠ˜ï¼šy_train å–®ä¸€é¡åˆ¥ï¼Œç„¡æ³•è¨“ç·´")
                            cv_scores.append(0.0)
                            continue
                    except Exception:
                        pass

                    # é¡åˆ¥å†å¹³è¡¡ï¼šä»¥æ¨£æœ¬æ¬Šé‡è¨“ç·´
                    try:
                        sample_weight = self._compute_sample_weights(y_train)
                        classifier.fit(X_train_final, y_train, sample_weight=sample_weight)
                    except TypeError:
                        classifier.fit(X_train_final, y_train)
                    except Exception as e:
                        self.logger.warning(f"æ¨£æœ¬æ¬Šé‡è¨“ç·´å¤±æ•—ï¼Œæ”¹ç‚ºç„¡æ¬Šé‡: {e}")
                        classifier.fit(X_train_final, y_train)

                    # é–¾å€¼æœå°‹ï¼šTrade vs Hold å…©éšæ®µæ±ºç­–ä»¥æœ€å¤§åŒ– Macro F1
                    y_pred = None
                    try:
                        if hasattr(classifier, 'predict_proba'):
                            y_proba = classifier.predict_proba(X_test_final)
                            if self.flags.get('enable_threshold_search', True):
                                # è‡ªé©æ‡‰æœå°‹ï¼šå…ˆç²—æƒå†ç´°æƒ
                                best_tau, y_pred = self._search_best_trade_threshold(y_test, y_proba)
                                self.logger.info(f"  é–¾å€¼æœå°‹å®Œæˆ: best_tau={best_tau:.3f}")
                    except Exception as e:
                        self.logger.warning(f"é–¾å€¼æœå°‹å¤±æ•—: {e}")
                        y_pred = None
                    if y_pred is None:
                        y_pred = classifier.predict(X_test_final)
                    # è¤‡åˆè©•åˆ†ï¼šæé«˜ macro F1 æ¬Šé‡ä»¥é™ä½æŒæœ‰é¡ä¸»å°
                    f1_w = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                    f1_m = f1_score(y_test, y_pred, average='macro', zero_division=0)
                    fold_score = 0.5 * f1_m + 0.5 * f1_w

                    # æ“´å……åº¦é‡
                    try:
                        prec_m = precision_score(y_test, y_pred, average='macro', zero_division=0)
                        rec_m = recall_score(y_test, y_pred, average='macro', zero_division=0)
                        bal_acc = balanced_accuracy_score(y_test, y_pred)
                    except Exception:
                        prec_m, rec_m, bal_acc = 0.0, 0.0, 0.0

                    auc_macro = None
                    try:
                        if hasattr(classifier, 'predict_proba'):
                            y_proba = classifier.predict_proba(X_test_final)
                            auc_macro = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')
                    except Exception:
                        auc_macro = None

                    try:
                        cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])
                    except Exception:
                        cm = None

                    if fold_score > 0.7:  # è¾ƒé«˜åˆ†æ•°æ—¶è¿›è¡Œé¢å¤–è®°å½•
                        self.logger.info(f"ğŸ“ˆ Fold {fold_idx+1} F1åˆ†æ•¸è¼ƒé«˜: {fold_score:.4f}ï¼Œå°‡é€²è¡Œå¤šé‡é©—è­‰")

                    cv_scores.append(fold_score)
                    # ç´¯è¨ˆçµ±è¨ˆ
                    if 'metrics_buf' not in locals():
                        metrics_buf = {
                            'f1_macro': [], 'f1_weighted': [],
                            'precision_macro': [], 'recall_macro': [],
                            'balanced_accuracy': [], 'auc_macro': []
                        }
                        conf_mat_sum = np.zeros((3, 3), dtype=int)
                    metrics_buf['f1_macro'].append(float(f1_m))
                    metrics_buf['f1_weighted'].append(float(f1_w))
                    metrics_buf['precision_macro'].append(float(prec_m))
                    metrics_buf['recall_macro'].append(float(rec_m))
                    metrics_buf['balanced_accuracy'].append(float(bal_acc))
                    if auc_macro is not None:
                        metrics_buf['auc_macro'].append(float(auc_macro))
                    if cm is not None and cm.shape == (3, 3):
                        conf_mat_sum += cm
                    self.logger.info(
                        f"    Fold {fold_idx+1}: {len(cols_coarse)}ç²—é¸â†’{len(cols_fine)}ç²¾é¸â†’{df_train_final.shape[1]}æœ€çµ‚, F1={fold_score:.4f}"
                    )

                    # aggregate selected features this fold
                    selection_counts.update(selected_cols)
                    selected_union.update(selected_cols)
                    if fold_score > best_fold_score:
                        best_fold_score = fold_score
                        best_fold_cols = list(selected_cols)

                except (ValueError, ZeroDivisionError) as critical_error:
                    self.logger.error(f"âŒ Fold {fold_idx+1} é—œéµéŒ¯èª¤: {critical_error}")
                    raise critical_error

                except Exception as fold_error:
                    self.logger.warning(f"  Fold {fold_idx+1} å¤±æ•—: {fold_error}")
                    cv_scores.append(0.0)

            base_score = np.mean(cv_scores) if cv_scores else 0.0

            # åŒ¯ç¸½åº¦é‡
            metrics_summary = {}
            if 'metrics_buf' in locals():
                metrics_summary = {
                    k: float(np.mean(v)) if len(v) > 0 else None
                    for k, v in metrics_buf.items()
                }
                metrics_summary['cv_scores_mean'] = float(base_score)
                metrics_summary['cv_scores_std'] = float(np.std(cv_scores)) if cv_scores else 0.0

            if base_score > 0.7:  # å¯¹è¾ƒé«˜åˆ†æ•°è¿›è¡Œå¤šé‡éªŒè¯
                validation_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
                is_valid, validation_msg = self._validate_score_legitimacy(base_score, X, y, validation_model)

                if not is_valid:
                    self.logger.error(f"âŒ å¤šé‡éªŒè¯å¤±è´¥: {validation_msg}")
                    return max(0.0, base_score * 0.85)  # éªŒè¯å¤±è´¥çµ¦äºˆè¼•å¾®æ‡²ç½°
                else:
                    self.logger.info(f"âœ… å¤šé‡éªŒè¯é€šè¿‡: {validation_msg}")

            self.logger.info(f"ğŸ¯ åµŒå¥—CVå¹³å‡F1: {base_score:.4f} (Â±{np.std(cv_scores):.4f})")

            metrics = self._compute_objective_metrics(trial, X, y, lag)
            metrics['cv_base_score'] = float(base_score)
            trial.set_user_attr('objective_metrics', metrics)
            metrics_summary.setdefault('combined_metrics_full', metrics)

        if self.multi_objective_mode:
            penalized_metrics = self._soft_penalize_kpis(metrics)
            values = self._kpis_to_multi_values(penalized_metrics)
            trial.set_user_attr('objective_values', dict(zip(self.multiobjective_metrics, values)))
            trial.set_user_attr('objective_metrics_penalized', penalized_metrics)
            return values

        if self.multi_objective_mode:
                values: List[float] = []
                for name in self.multiobjective_metrics:
                    val = metrics.get(name)
                    if val is None or not np.isfinite(val):
                        values.append(0.0)
                    else:
                        if name in ('profit_factor', 'calmar', 'sharpe', 'sortino'):
                            val = max(min(val, 5.0), -1.0)
                        if name in ('annual_return', 'total_return'):
                            val = max(min(val, 2.0), -1.0)
                        values.append(float(val))
                trial.set_user_attr('objective_values', dict(zip(self.multiobjective_metrics, values)))
                self.logger.info(f"ğŸ¯ å¤šç›®æ¨™è©•ä¼°: {dict(zip(self.multiobjective_metrics, values))}")
                return values

            weighted_score = self._weighted_objective_score(metrics)
            final_score = base_score * 0.4 + weighted_score * 0.6
            trial.set_user_attr('objective_score', float(final_score))

            # è¨˜éŒ„ LGBM çš„ã€Œno positive gainã€è·¡è±¡ï¼Œä¾› callback åƒè€ƒ
            try:
                if isinstance(classifier, dict) and 'lightgbm' in str(type(classifier)).lower():
                    pass
            except Exception:
                pass

            if self.flags.get('enable_trade_freq_penalty', False) and len(y) > 1:
                trade_freq_penalty = self._calculate_trade_frequency_penalty(y, lag)
                final_score *= (1 - trade_freq_penalty)

            if final_score > 0.70:  # æé«˜éªŒè¯é—¨æ§›è‡³0.70
                self.logger.info(f"ğŸ“‹ é«˜åˆ†æ•° {final_score:.4f} éœ€è¦å¤šé‡æ£€éªŒï¼Œå¼€å§‹éªŒè¯...")
                
                validation_model = RandomForestClassifier(
                    n_estimators=30, max_depth=6, random_state=42, n_jobs=-1
                )
                
                validation_passed = 0
                
                shuffle_valid, shuffle_msg = self._shuffle_label_validation(X, y, validation_model, final_score)
                if shuffle_valid:
                    validation_passed += 1
                    self.logger.info(f"âœ… æ‰“ä¹±æ ‡ç­¾æµ‹è¯•é€šè¿‡: {shuffle_msg}")
                else:
                    self.logger.warning(f"âš ï¸ æ‰“ä¹±æ ‡ç­¾æµ‹è¯•æœªé€šè¿‡: {shuffle_msg}")
                
                random_valid, random_msg = self._random_feature_validation(X, y, validation_model, final_score)
                if random_valid:
                    validation_passed += 1
                    self.logger.info(f"âœ… éšæœºç‰¹å¾æµ‹è¯•é€šè¿‡: {random_msg}")
                else:
                    self.logger.warning(f"âš ï¸ éšæœºç‰¹å¾æµ‹è¯•æœªé€šè¿‡: {random_msg}")
                
                if validation_passed >= 1:
                    self.logger.info("ğŸ¯ å¤šé‡æ£€éªŒé€šè¿‡ï¼(è‡³å°‘1é¡¹éªŒè¯é€šè¿‡)")
                else:
                    self.logger.warning("âš ï¸ å¤šé‡æ£€éªŒæœªé€šè¿‡ï¼Œåº”ç”¨è½»å¾®æƒ©ç½š")
                    final_score *= 0.9  # è½»å¾®æƒ©ç½šè€Œéå½’é›¶
            else:
                self.logger.info(f"ğŸ“Š åˆ†æ•° {final_score:.4f} æ— éœ€å¤šé‡æ£€éªŒï¼ˆ< 0.70ï¼‰")

            expected_range = (0.60, 0.85)  # æé«˜æœŸæœ›èŒƒå›´è‡³0.60-0.85
            target_excellent = 0.80         # ä¼˜ç§€ç›®æ ‡ï¼š0.80+
            
            if final_score >= target_excellent:
                self.logger.info(f"ğŸ¯ ä¼˜ç§€ï¼Layer2åˆ†æ•°è¾¾åˆ°ç›®æ ‡: {final_score:.4f} >= {target_excellent}")
            elif final_score >= expected_range[1]:
                self.logger.info(f"ğŸ“ˆ å¾ˆå¥½ï¼åˆ†æ•°è¶…å‡ºé¢„æœŸ: {final_score:.4f} > {expected_range[1]}")
            elif final_score >= expected_range[0]:
                self.logger.info(f"âœ… è‰¯å¥½ï¼åˆ†æ•°åœ¨å¯æ¥å—èŒƒå›´: {final_score:.4f} âˆˆ {expected_range}")
            else:
                self.logger.info(f"ğŸ“Š éœ€ç»§ç»­ä¼˜åŒ–: {final_score:.4f} < {expected_range[0]} (ç›®æ ‡: 0.80+)")

            self.logger.info(f"âœ… åµŒå¥—CVè©•ä¼°å®Œæˆ: F1_weighted={final_score:.4f} (é€šéå¤šé‡æª¢é©—ï¼Œç„¡æ•¸æ“šæ´©éœ²)")

            # finalize selected_features to avoid NameError
            selected_features: List[str]
            stability_report: Dict[str, Any] = {}
            if selection_counts:
                max_count = selection_counts.most_common(1)[0][1]
                threshold = max(1, int(0.6 * max_count))
                selected_features = [f for f, c in selection_counts.items() if c >= threshold]
                stability_report = {
                    'counts': dict(selection_counts),
                    'max_count': int(max_count),
                    'threshold': int(threshold),
                    'selected_count': len(selected_features)
                }
            elif selected_union:
                selected_features = sorted(list(selected_union))
                stability_report = {
                    'counts': {},
                    'selected_count': len(selected_features)
                }
            elif best_fold_cols:
                selected_features = list(best_fold_cols)
                stability_report = {
                    'counts': {},
                    'selected_count': len(selected_features)
                }
            else:
                # robust fallback: top-variance features if none selected
                top_k = min(30, X.shape[1]) if X.shape[1] > 0 else 0
                selected_features = X.var().sort_values(ascending=False).head(top_k).index.tolist() if top_k > 0 else []
                stability_report = {
                    'counts': {},
                    'selected_count': len(selected_features),
                    'fallback': 'top_variance'
                }

            trial.set_user_attr('selected_features', selected_features)
            trial.set_user_attr('feature_phase', phase)
            trial.set_user_attr('stability_report', stability_report)
            trial.set_user_attr('base_score', base_score)
            trial.set_user_attr('cv_scores', cv_scores)
            trial.set_user_attr('cv_metrics', metrics_summary)
            if 'conf_mat_sum' in locals():
                trial.set_user_attr('confusion_matrix', conf_mat_sum.tolist())

            try:
                report = {
                    'trial': trial.number,
                    'score': float(final_score),
                    'base_score': float(base_score),
                    'cv_std': float(np.std(cv_scores)) if cv_scores else None,
                    'selected_features': selected_features,
                    'metrics': metrics_summary,
                    'stability': stability_report,
                }
                reports_dir = self.results_path / 'reports'
                reports_dir.mkdir(parents=True, exist_ok=True)
                report_path = reports_dir / f'l2_trial_{trial.number:04d}.json'
                with open(report_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                self.logger.info(f"ğŸ“ ä¿å­˜Layer2è©¦é©—å ±å‘Š: {report_path}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ä¿å­˜Layer2è©¦é©—å ±å‘Šå¤±æ•—: {e}")

            self.latest_selected_features = selected_features
            self.latest_feature_phase = phase

            return final_score

        except (ValueError, ZeroDivisionError, KeyError) as critical_error:
            self.logger.error(f"âŒ é—œéµéŒ¯èª¤ - Fail-Fast: {critical_error}")
            raise critical_error

        except (MemoryError, TimeoutError) as resource_error:
            self.logger.error(f"âŒ è³‡æºé™åˆ¶éŒ¯èª¤: {resource_error}")
            raise resource_error

        except Exception as e:
            self.logger.warning(f"âš ï¸ è©¦é©—å¤±æ•—ï¼Œè·³éæ­¤æ¬¡: {e}")
            return 0.0

    def _generate_features(self, ohlcv_data: pd.DataFrame) -> pd.DataFrame:
        """ä¿®å¾©ç‰ˆç‰¹å¾µç”Ÿæˆï¼ˆä¸ä¾è³´å¤–éƒ¨é¡ï¼‰"""
        features = pd.DataFrame(index=ohlcv_data.index)

        # åŸºæœ¬åƒ¹æ ¼ç‰¹å¾µ
        features['returns'] = ohlcv_data['close'].pct_change()
        features['high_low_ratio'] = ohlcv_data['high'] / ohlcv_data['low']
        features['open_close_ratio'] = ohlcv_data['open'] / ohlcv_data['close']

        # ç§»å‹•å¹³å‡ç·š
        for window in [5, 10, 20, 50]:
            features[f'sma_{window}'] = ohlcv_data['close'].rolling(window).mean()
            features[f'price_sma_{window}_ratio'] = ohlcv_data['close'] / features[f'sma_{window}']

            features[f'ema_{window}'] = ohlcv_data['close'].ewm(span=window).mean()
            features[f'price_ema_{window}_ratio'] = ohlcv_data['close'] / features[f'ema_{window}']

        # RSI
        for window in [14, 21]:
            delta = ohlcv_data['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / loss
            features[f'rsi_{window}'] = 100 - (100 / (1 + rs))

        # å¸ƒæ—å¸¶
        for window in [20]:
            sma = ohlcv_data['close'].rolling(window).mean()
            std = ohlcv_data['close'].rolling(window).std()
            features[f'bb_upper_{window}'] = sma + (2.0 * std)
            features[f'bb_lower_{window}'] = sma - (2.0 * std)
            features[f'bb_position_{window}'] = (ohlcv_data['close'] - features[f'bb_lower_{window}']) / (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}'])

        # æˆäº¤é‡ç‰¹å¾µ
        features['volume_sma_ratio'] = ohlcv_data['volume'] / ohlcv_data['volume'].rolling(20).mean()
        features['price_volume'] = ohlcv_data['close'] * ohlcv_data['volume']

        # æ³¢å‹•ç‡
        for window in [10, 20]:
            features[f'volatility_{window}'] = ohlcv_data['close'].pct_change().rolling(window).std()

        # MACDæŒ‡æ¨™
        ema_12 = ohlcv_data['close'].ewm(span=12).mean()
        ema_26 = ohlcv_data['close'].ewm(span=26).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # å¨å»‰æŒ‡æ¨™
        for window in [14, 21]:
            high_max = ohlcv_data['high'].rolling(window).max()
            low_min = ohlcv_data['low'].rolling(window).min()
            features[f'williams_r_{window}'] = -100 * (high_max - ohlcv_data['close']) / (high_max - low_min)

        # å‹•é‡æŒ‡æ¨™
        for window in [10, 20]:
            features[f'momentum_{window}'] = ohlcv_data['close'] / ohlcv_data['close'].shift(window) - 1

        # æ¸…ç†æ•¸æ“šï¼ˆç§»é™¤bfillï¼‰
        features = features.ffill()
        features = features.replace([np.inf, -np.inf], np.nan).fillna(0)

        return features

    def initialize_phased_features(self) -> None:
        """åˆå§‹åŒ–åˆ†éšæ®µç‰¹å¾µæ± """
        self.core_feature_groups = {
            'momentum': ['f_sma_5', 'f_sma_10', 'f_sma_20', 'f_ema_10', 'f_ema_20'],
            'volatility': ['f_bb_position_20', 'f_atr_14', 'f_volatility_20'],
            'volume': ['volume_sma_ratio', 'f_mfi_14', 'price_volume'],
            'price_patterns': ['returns', 'high_low_ratio', 'open_close_ratio'],
            'rsi_core': ['f_rsi_14', 'f_rsi_21']
        }

        self.strategy_feature_groups = {
            'advanced_ma': ['f_sma_50', 'f_ema_50', 'macd', 'macd_signal'],
            'wyckoff': ['wyk_spring', 'wyk_upthrust', 'wyk_bc', 'wyk_st'],
            'fibonacci': ['fibo_0.382', 'fibo_0.618', 'fibo_extension_1.618'],
            'td_sequence': ['td_count_buy', 'td_count_sell'],
            'micro_structure': ['spread_est', 'depth_imbalance']
        }

    def _configure_phase_settings(self) -> None:
        self.initialize_phased_features()
        cfg_path = self.config_path / "feature_flags.json"
        if not cfg_path.exists():
            self.phase_config = {}
            self.selection_params = {}
            return
        try:
            with open(cfg_path, 'r', encoding='utf-8') as f:
                flags = json.load(f)
            self.phase_config = flags.get("feature_phases", {})
            self.selection_params = flags.get("selection_params", {})
        except Exception as e:
            self.logger.warning(f"âš ï¸ è®€å–åˆ†éšæ®µé…ç½®å¤±æ•—: {e}")
            self.phase_config = {}
            self.selection_params = {}

    def _flatten_feature_groups(self, groups: Dict[str, List[str]]) -> List[str]:
        flattened: List[str] = []
        for feats in groups.values():
            flattened.extend(feats)
        return flattened

    def _get_phase_feature_pool(self, phase: str) -> List[str]:
        core = self._flatten_feature_groups(self.core_feature_groups)
        strategy = self._flatten_feature_groups(self.strategy_feature_groups)
        if phase == "core":
            return core
        if phase == "strategy":
            return list(dict.fromkeys(core + strategy))
        return list(self.X_full.columns)

    def _get_phase_threshold(self, phase: str) -> float:
        defaults = {'core': 0.8, 'strategy': 0.65, 'full': 0.6}
        phase_meta = self.phase_config.get(phase, {})
        return float(phase_meta.get('stability_threshold', defaults.get(phase, 0.6)))

    def _get_selection_bounds(self, phase: str, total_features: int) -> Tuple[Tuple[int, int], Tuple[int, int]]:
        phase_params = self.selection_params.get(phase, {})
        coarse_ratio = phase_params.get('coarse_ratio', [0.3, 0.6])
        fine_ratio = phase_params.get('fine_ratio', [0.4, 0.8])
        coarse_bounds = (
            max(10, int(total_features * coarse_ratio[0])),
            min(total_features - 1, int(total_features * coarse_ratio[1]))
        )
        fine_bounds = (
            max(5, int(coarse_bounds[0] * fine_ratio[0])),
            max(coarse_bounds[0] + 1, int(coarse_bounds[1] * fine_ratio[1]))
        )
        return coarse_bounds, fine_bounds

    def _phase_bonus(self, phase: str, score: float) -> float:
        bonuses = {'core': 0.0, 'strategy': 0.02, 'full': 0.05}
        threshold = 0.65
        return bonuses.get(phase, 0.0) if score > threshold else 0.0

    def _phase_stability_check(self, X: pd.DataFrame, y: pd.Series, phase: str, selected: List[str]) -> Dict[str, Any]:
        if not selected:
            return {'stable': False, 'overall_stability': 0.0}
        X_sel = X[selected]
        n_bootstrap = int(self.selection_params.get(phase, {}).get('bootstrap_runs', 5))
        stability_scores: List[float] = []
        feature_rankings: List[pd.Series] = []

        for i in range(n_bootstrap):
            sample_idx = np.random.choice(len(X_sel), size=max(50, int(len(X_sel) * 0.8)), replace=True)
            X_boot, y_boot = X_sel.iloc[sample_idx], y.iloc[sample_idx]

            rf = RandomForestClassifier(n_estimators=100, random_state=42 + i, n_jobs=-1)
            rf.fit(X_boot.fillna(0), y_boot)

            importances = pd.Series(rf.feature_importances_, index=X_boot.columns)
            feature_rankings.append(importances.rank(ascending=False))

            cv_score = cross_val_score(
                rf,
                X_boot.fillna(0),
                y_boot,
                cv=TimeSeriesSplit(n_splits=3),
                scoring='f1_weighted'
            ).mean()
            stability_scores.append(cv_score)

        score_stability = 1 - np.std(stability_scores) / (np.mean(stability_scores) + 1e-8)
        ranking_df = pd.DataFrame(feature_rankings).T
        if ranking_df.shape[1] > 1:
            corr_matrix = ranking_df.corr().values
            rank_consistency = np.mean(corr_matrix[np.triu_indices_from(corr_matrix, k=1)])
        else:
            rank_consistency = 1.0

        overall_stability = 0.6 * score_stability + 0.4 * rank_consistency
        is_stable = overall_stability >= self._get_phase_threshold(phase)

        return {
            'stable': is_stable,
            'overall_stability': overall_stability,
            'score_stability': score_stability,
            'rank_consistency': rank_consistency,
            'mean_score': float(np.mean(stability_scores)),
            'bootstrap_runs': n_bootstrap
        }

    def _phased_feature_selection(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        coarse_k: int,
        fine_k: int,
        stability_threshold: float
    ) -> List[str]:
        if X.empty:
            return []

        variance_selector = VarianceThreshold(threshold=0.01)
        try:
            variance_selector.fit(X)
            cols_var = X.columns[variance_selector.get_support(indices=True)].tolist()
        except ValueError:
            cols_var = list(X.columns)
        if len(cols_var) <= min(10, coarse_k):
            return cols_var

        coarse_selector = SelectKBest(f_classif, k=min(coarse_k, len(cols_var) - 1))
        coarse_selector.fit(X[cols_var], y)
        cols_coarse = [cols_var[i] for i in coarse_selector.get_support(indices=True)]
        if len(cols_coarse) <= min(5, fine_k):
            return cols_coarse

        fine_selector = SelectKBest(mutual_info_classif, k=min(fine_k, len(cols_coarse) - 1))
        fine_selector.fit(X[cols_coarse], y)
        cols_fine = [cols_coarse[i] for i in fine_selector.get_support(indices=True)]
        if not cols_fine:
            return cols_coarse

        final_cols = self.remove_correlated_features_smart(X[cols_fine], 0.9)
        if not final_cols:
            final_cols = cols_fine

        stability = self._phase_stability_check(X[final_cols], y, 'full', final_cols)
        if not stability.get('stable', True):
            self.logger.warning(
                "âš ï¸ åˆ†éšæ®µç©©å®šæ€§æœªé”æ¨™: overall=%.3f", stability.get('overall_stability', 0.0)
            )

        return final_cols

    def objective_phased(self, trial: optuna.Trial, X: pd.DataFrame, y: pd.Series) -> Tuple[float, Dict[str, Any]]:
        phase = trial.suggest_categorical('feature_phase', ['core', 'strategy', 'full'])
        phase_pool = [col for col in self._get_phase_feature_pool(phase) if col in X.columns]
        if not phase_pool:
            phase_pool = list(X.columns)

        stability_threshold = self._get_phase_threshold(phase)
        coarse_bounds, fine_bounds = self._get_selection_bounds(phase, len(phase_pool))

        if coarse_bounds[0] >= coarse_bounds[1]:
            coarse_bounds = (max(10, min(coarse_bounds)), max(20, min(len(phase_pool) - 1, max(coarse_bounds) + 5)))
        if fine_bounds[0] >= fine_bounds[1]:
            fine_bounds = (max(5, min(fine_bounds)), max(10, min(coarse_bounds[1] - 1, max(fine_bounds) + 3)))

        coarse_k = trial.suggest_int('coarse_k', *coarse_bounds)
        fine_k = trial.suggest_int('fine_k', *fine_bounds)

        X_phase = X[phase_pool]
        selected_features = self._phased_feature_selection(X_phase, y, coarse_k, fine_k, stability_threshold)
        if not selected_features or len(selected_features) == 0:
            self.logger.warning(f"âš ï¸ {phase} éšæ®µç„¡é¸ä¸­ç‰¹å¾µï¼Œprune æ­¤ trial")
            raise optuna.TrialPruned(f"{phase} éšæ®µç„¡æœ‰æ•ˆç‰¹å¾µå­é›†")

        cv_scores = self._cross_validate_selected_features(X_phase[selected_features], y)
        if not cv_scores or all(s == 0 for s in cv_scores):
            self.logger.warning(f"âš ï¸ {phase} éšæ®µ CV ç„¡æœ‰æ•ˆåˆ†æ•¸ï¼Œprune æ­¤ trial")
            raise optuna.TrialPruned(f"{phase} éšæ®µ CV ç„¡æœ‰æ•ˆè©•åˆ†")
        
        base_score = float(np.mean(cv_scores))
        stability_report = self._phase_stability_check(X_phase, y, phase, selected_features)
        final_score = base_score + self._phase_bonus(phase, base_score)
        
        self.logger.info(
            f"âœ… {phase} éšæ®µå®Œæˆ: {len(selected_features)} ç‰¹å¾µ, CV={base_score:.4f}, æœ€çµ‚={final_score:.4f}"
        )

        return final_score, {
            'phase': phase,
            'selected_features': selected_features,
            'base_score': base_score,
            'stability': stability_report
        }

    def _cross_validate_selected_features(self, X: pd.DataFrame, y: pd.Series) -> List[float]:
        if X.empty:
            return []
        splitter = TimeSeriesSplit(n_splits=3)
        scores = []
        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        for train_idx, test_idx in splitter.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            scores.append(f1_score(y_test, y_pred, average='weighted'))
        return scores

    def materialize_best_features(self, data: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """æ ¹æ“šæœ€ä½³åƒæ•¸ç‰©åŒ–ç‰¹å¾µè³‡æ–™é›†"""
        if 'label' not in data.columns:
            raise ValueError("ç‰©åŒ–ç‰¹å¾µæ™‚éœ€è¦åŒ…å«labelæ¬„ä½")
        selected_columns = params.get('selected_features')
        if not selected_columns:
            raise ValueError("æœ€ä½³åƒæ•¸ç¼ºå°‘selected_featuresè³‡è¨Š")
        available_cols = [col for col in selected_columns if col in data.columns]
        missing = [col for col in selected_columns if col not in data.columns]
        if missing:
            self.logger.warning("âš ï¸ ç‰©åŒ–ç‰¹å¾µç¼ºå¤±åˆ—: %s", missing)
        result = data[available_cols + ['label']].copy()
        result = result.fillna(0)
        result.attrs['feature_phase'] = params.get('feature_phase')
        return result

    def apply_transform(self, data: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
        """çµ±ä¸€ç‰©åŒ–æ¥å£ï¼Œå§”æ´¾è‡³ materialize_best_featuresã€‚"""
        return self.materialize_best_features(data, params)

    def calculate_label_quality(self, labels: pd.Series, params: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—æ¨™ç±¤è³ªé‡ï¼ˆèˆ‡ Layer1 å ±è¡¨å£å¾‘ä¸€è‡´ï¼‰ï¼š
        - balance_score: ä»¥ç›®æ¨™ 25/50/25 ç‚ºåŸºæº–è¨ˆç®— KL è½‰æ›åˆ†æ•¸
        - stability_score: 1 - è®ŠåŒ–é »ç‡
        - f1_score: ç°¡åŒ–ç‚º macroF1 ä»£ç†ï¼ˆæ­¤è™•ä¿å®ˆä»¥ 0.5 ä½œåŸºæº–ï¼Œå¯å¾ŒçºŒæ›¿æ›æ¨¡å‹å¯¦æ¸¬ï¼‰
        - distribution: [sell, hold, buy]
        - total_samples
        """
        try:
            if not isinstance(labels, pd.Series) or labels.empty:
                return {'balance_score': 0.0, 'stability_score': 0.0, 'f1_score': 0.0,
                        'distribution': [0.0, 0.0, 0.0], 'total_samples': 0}
            value_counts = labels.value_counts(normalize=True)
            target = np.array([0.25, 0.5, 0.25])
            actual = np.array([value_counts.get(0, 0.0), value_counts.get(1, 0.0), value_counts.get(2, 0.0)])
            kl = np.sum(target * np.log((target + 1e-8) / (actual + 1e-8)))
            balance_score = float(np.exp(-kl))
            changes = int((labels.diff() != 0).sum())
            stability_score = float(max(0.0, 1.0 - changes / max(len(labels), 1)))
            macro_f1_proxy = 0.5
            return {
                'balance_score': balance_score,
                'stability_score': stability_score,
                'f1_score': macro_f1_proxy,
                'distribution': actual.tolist(),
                'total_samples': int(len(labels))
            }
        except Exception as exc:
            self.logger.warning(f"ç„¡æ³•è¨ˆç®—æ¨™ç±¤è³ªé‡: {exc}")
            return {'balance_score': 0.0, 'stability_score': 0.0, 'f1_score': 0.0,
                    'distribution': [0.0, 0.0, 0.0], 'total_samples': 0}

    def optimize(self, n_trials: int = 50, timeframes: List[str] = None) -> Dict:
        """ğŸš€ 123.mdå»ºè­°ï¼šåƒæ•¸åŒ–æ¨™ç±¤ç”Ÿæˆçš„ç‰¹å¾µå·¥ç¨‹å„ªåŒ–"""
        if timeframes is None:
            timeframes = [self.timeframe]

        results = {}
        meta_vol = self.scaled_config.get('meta_vol', 0.02)

        for tf in timeframes:
            self.logger.info(f"ğŸš€ é–‹å§‹Layer2åƒæ•¸åŒ–æ¨™ç±¤+ç‰¹å¾µå·¥ç¨‹å„ªåŒ–... æ™‚æ¡†: {tf}")
            self.timeframe = tf

            if len(self.features) == 0:
                raise ValueError("ç‰¹å¾µæ•¸æ“šé åŠ è¼‰å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå„ªåŒ–")
            if len(self.close_prices) == 0:
                raise ValueError("åƒ¹æ ¼æ•¸æ“šé åŠ è¼‰å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå„ªåŒ–")
            if not hasattr(self, 'valid_data_range'):
                raise ValueError("æ•¸æ“šç¯„åœé è¨ˆç®—å¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå„ªåŒ–")

            self.logger.info(f"ğŸ“Š æ•¸æ“šæ¦‚æ³: OHLCV={self.ohlcv_data.shape}, ç‰¹å¾µ={self.features.shape}")

            # ğŸš€ åœ¨ trial ä¹‹å‰å…ˆå›ºå®šç©©å®šç‰¹å¾µé›†ï¼ˆä½¿ç”¨ Layer1 æœ€ä½³æˆ–é»˜èªæ¨™ç±¤åƒæ•¸ï¼‰
            try:
                baseline_params = None
                tf_specific = self.config_path / f"label_params_{self.timeframe}.json"
                fallback = self.config_path / "label_params.json"
                if tf_specific.exists():
                    with open(tf_specific, 'r', encoding='utf-8') as f:
                        baseline_params = json.load(f).get('best_params', {})
                elif fallback.exists():
                    with open(fallback, 'r', encoding='utf-8') as f:
                        baseline_params = json.load(f).get('best_params', {})
                # é»˜èªåŸºç·š
                lag_b = int(baseline_params.get('lag', max(1, self.scaled_config.get('label_lag_min', 3))))
                buy_q_b = float(baseline_params.get('buy_quantile', 0.75))
                sell_q_b = float(baseline_params.get('sell_quantile', 0.25))
                lookback_b = int(self.scaled_config.get('lookback_window_min', 300))
                y_base = self._generate_labels(self.close_prices, lag_b, buy_q_b, sell_q_b, lookback_b)
                self._ensure_X_full()
                common_idx_base = self.X_full.index.intersection(y_base.index)
                X_base = self.X_full.reindex(common_idx_base).astype('float32').fillna(0)
                y_base = y_base.reindex(common_idx_base).astype(int)
                # å…ˆåšä¸€æ¬¡å»å†—é¤˜ï¼ˆpairwise correlationï¼‰å†é€²è¡Œç©©å®šæ€§é¸æ“‡
                try:
                    baseline_corr_th = float(self.flags.get('baseline_corr_threshold', 0.9))
                except Exception:
                    baseline_corr_th = 0.9
                base_keep = self.remove_correlated_features_smart(X_base, baseline_corr_th)
                X_base = X_base[base_keep] if len(base_keep) > 0 else X_base
                n_base = X_base.shape[1]
                coarse_b = min(max(50, int(n_base * 0.6)), n_base - 1) if n_base > 1 else 1
                fine_b = min(60, max(15, coarse_b - 1))
                self.stable_cols = self._stability_select_features(
                    X_base, y_base,
                    coarse_k=coarse_b,
                    fine_k=fine_b,
                    threshold=float(self.flags.get('stability_selection_threshold', 0.6))
                )
                self.logger.info(f"ğŸ§® å›ºå®šç©©å®šç‰¹å¾µæ•¸: {len(self.stable_cols)} / {n_base}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç©©å®šç‰¹å¾µé è¨ˆç®—å¤±æ•—: {e}")
                self.stable_cols = []

            storage_url = None
            try:
                storage_url = self.scaled_config.get('optuna_storage')
            except Exception:
                storage_url = None
            # Callback: æ ¹æ“š trial user_attrs èª¿æ•´ LGBM æœå°‹ç©ºé–“ï¼ˆç°¡åŒ–ç‰ˆï¼šé€éæ——æ¨™å½±éŸ¿ create_enhanced_classifier çš„ç¯„åœï¼‰
            def _relax_search_space_cb(study, trial):
                try:
                    no_gain = bool(trial.user_attrs.get('lgb_no_gain', False))
                    bad_score = (trial.value is None) or (float(trial.value) < 0.35)
                    if no_gain or bad_score:
                        cnt = int(study.user_attrs.get('relax_count', 0)) + 1
                        study.set_user_attr('relax_count', cnt)
                        # å°‡æ——æ¨™è¨­ç‚º Trueï¼Œå¾ŒçºŒ objective è®€å–å¾Œæ”¾é¬†æœç´¢ç©ºé–“
                        study.set_user_attr('relax_search_space', True)
                        self.flags['lgb_relax_search'] = True
                        self.logger.info(f"ğŸ”§ è§¸ç™¼ LGBM æœå°‹ç©ºé–“æ”¾é¬†ï¼Œç¬¬{cnt}æ¬¡")
                except Exception:
                    pass

            study_kwargs = {
                'study_name': f'feature_optimization_layer2_{tf}',
                'pruner': optuna.pruners.MedianPruner(n_warmup_steps=7),
                'storage': storage_url,
                'load_if_exists': bool(storage_url)
            }
            if self.multi_objective_mode:
                study = optuna.create_study(
                    directions=['maximize'] * len(self.multiobjective_metrics),
                    **study_kwargs
                )
            else:
                study = optuna.create_study(
                    direction='maximize',
                    **study_kwargs
                )
            study.set_user_attr('meta_vol', meta_vol)

            self.logger.info(f"ğŸš€ å¼€å§‹Layer2ç‰¹å¾ä¼˜åŒ–: {n_trials} trials")
            successful_trials = 0
            failed_trials = 0
            consecutive_failures = 0
            trial_durations: List[float] = []

            self.logger.info(f"ğŸš€ Layer2 flags: {self.flags}")
            for trial_idx in range(n_trials):
                try:
                    start_trial = time.perf_counter()
                    study.optimize(self.objective, n_trials=1, timeout=450, callbacks=[_relax_search_space_cb])
                    duration = time.perf_counter() - start_trial
                    trial_durations.append(duration)
                    successful_trials += 1
                    consecutive_failures = 0

                    if trial_idx % 20 == 0 and trial_idx > 0:
                        current_best = study.best_value if study.trials else 0.0
                        self.logger.info(
                            f"ğŸ“Š Progress: {trial_idx+1}/{n_trials} trials, æˆåŠŸ: {successful_trials}, "
                            f"å¤±è´¥: {failed_trials}, å½“å‰æœ€ä½³: {current_best:.4f}"
                        )

                except Exception as e:
                    duration = time.perf_counter() - start_trial
                    trial_durations.append(duration)
                    failed_trials += 1
                    consecutive_failures += 1
                    self.logger.warning(f"âš ï¸ Trial {trial_idx} å¤±è´¥: {e}")

                    if consecutive_failures >= 10:
                        self.logger.error(f"âŒ è¿ç»­{consecutive_failures}æ¬¡å¤±è´¥ï¼Œåœæ­¢ä¼˜åŒ–")
                        break
                    if failed_trials > n_trials * 0.5:
                        self.logger.error(f"âŒ å¤±è´¥ç‡è¿‡é«˜ ({failed_trials}/{trial_idx+1})ï¼Œåœæ­¢ä¼˜åŒ–")
                        break
                    continue

            avg_duration = float(np.mean(trial_durations)) if trial_durations else 0.0
            self.logger.info(f"â±ï¸ å¹³å‡ trial è€—æ™‚: {avg_duration:.2f} ç§’")

            self.logger.info(f"âœ… Layer2ä¼˜åŒ–å®Œæˆ: {successful_trials}/{n_trials} æˆåŠŸtrialsï¼Œå¤±è´¥: {failed_trials}")

            if successful_trials == 0:
                raise ValueError("æ‰€æœ‰trialséƒ½å¤±è´¥ï¼Œä¼˜åŒ–æ— æ•ˆ")

            if successful_trials < n_trials * 0.1:
                self.logger.warning(f"âš ï¸ æˆåŠŸç‡è¿‡ä½ ({successful_trials}/{n_trials})ï¼Œç»“æœå¯èƒ½ä¸å¯é ")

            best_params = study.best_params
            best_score = study.best_value

            best_trial = study.best_trial
            selected_features = best_trial.user_attrs.get('selected_features', [])
            feature_phase = best_trial.user_attrs.get('feature_phase')
            stability_report = best_trial.user_attrs.get('stability_report', {})
            cv_metrics = best_trial.user_attrs.get('cv_metrics', {})
            cv_scores_attr = best_trial.user_attrs.get('cv_scores', [])
            confusion_matrix_attr = best_trial.user_attrs.get('confusion_matrix')

            best_params['selected_features'] = selected_features
            best_params['feature_phase'] = feature_phase
            best_params['phase_stability'] = stability_report
            best_params['selected_features_count'] = len(selected_features)
            if cv_metrics:
                best_params['cv_metrics'] = cv_metrics
            if cv_scores_attr:
                best_params['cv_scores'] = [float(x) for x in cv_scores_attr]

            self.logger.info(f"æ¨™ç±¤å„ªåŒ–å®Œæˆ! æœ€ä½³å¾—åˆ†: {best_score:.4f}")
            self.logger.info(f"ğŸ† æœ€å„ªåƒæ•¸: {best_params}")

            try:
                cleaned_file = self.config_path / f"cleaned_ohlcv_{tf}.parquet"
                if cleaned_file.exists():
                    df_cleaned = read_dataframe(cleaned_file)
                    self.logger.info(f"âœ… ä½¿ç”¨Layer0æ¸…æ´—æ•¸æ“šç”Ÿæˆæœ€çµ‚æ¨™ç±¤: {cleaned_file}")
                else:
                    ohlcv_file = self.data_path / "raw" / self.symbol / f"{self.symbol}_{tf}_ohlcv.parquet"
                    df_cleaned = read_dataframe(ohlcv_file)
                    self.logger.info(f"âœ… ä½¿ç”¨åŸå§‹OHLCVæ•¸æ“šç”Ÿæˆæœ€çµ‚æ¨™ç±¤: {ohlcv_file}")
                price_data2 = df_cleaned['close']
                final_labels = self.generate_labels(price_data2, best_params)
                final_quality = self.calculate_label_quality(final_labels, best_params)
                labeled_data = self.apply_labels(df_cleaned, best_params)
            except Exception as e:
                self.logger.warning(f"ç„¡æ³•ç”Ÿæˆæœ€çµ‚æ¨™ç±¤: {e}")
                final_quality = {}
                labeled_data = None

            result = {
                'timeframe': tf,
                'best_params': best_params,
                'best_score': best_score,
                'n_trials': n_trials,
                'final_quality': final_quality,
                'meta_vol': meta_vol,
                'labeled_data': labeled_data,
                'cv_metrics': cv_metrics,
                'cv_scores': [float(x) for x in cv_scores_attr] if cv_scores_attr else [],
                'confusion_matrix': confusion_matrix_attr,
                'optimization_history': [
                    {'trial': i, 'score': trial.value}
                    for i, trial in enumerate(study.trials)
                    if trial.value is not None
                ]
            }

            # é¿å…å°‡ DataFrame å¯«å…¥ JSONï¼ˆåƒ…ä¿å­˜å¯åºåˆ—åŒ–æ‘˜è¦ï¼‰
            json_safe = {k: v for k, v in result.items() if k != 'labeled_data'}
            output_file = self.config_path / "feature_params.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(json_safe, f, indent=2, ensure_ascii=False)

            tf_output = self.config_path / f"feature_params_{tf}.json"
            with open(tf_output, 'w', encoding='utf-8') as f:
                json.dump(json_safe, f, indent=2, ensure_ascii=False)

            self.logger.info(f"âœ… çµæœå·²ä¿å­˜è‡³: {output_file}")
            self.logger.info(f"âœ… æ™‚æ¡†å°ˆå±¬çµæœå·²ä¿å­˜è‡³: {tf_output}")

            results[tf] = result

        return results[self.timeframe] if len(timeframes) == 1 else results

    def optimize_multi_timeframes(self, timeframes: List[str], n_trials: int = 50) -> Dict:
        """ğŸš€ å¤šæ™‚æ¡†åˆ†åˆ¥å„ªåŒ–ï¼šç‚ºæ¯å€‹æ™‚æ¡†æ‰¾åˆ°æœ€ä½³ç‰¹å¾µå­é›†"""
        self.logger.info(f"ğŸš€ é–‹å§‹å¤šæ™‚æ¡†ç‰¹å¾µå„ªåŒ–: {timeframes}")

        best_configs = {}

        for timeframe in timeframes:
            self.logger.info(f"\nğŸ“ˆ é–‹å§‹å„ªåŒ–æ™‚æ¡†: {timeframe}")

            optimizer = FeatureOptimizer(
                data_path=str(self.data_path),
                config_path=str(self.config_path),
                symbol=self.symbol,
                timeframe=timeframe
            )

            try:
                result = optimizer.optimize(n_trials=n_trials)

                best_configs[timeframe] = {
                    'timeframe': timeframe,
                    'best_score': result['best_score'],
                    'best_params': result['best_params'],
                    'coarse_k': result['best_params'].get('coarse_k', 60),
                    'fine_k': result['best_params'].get('fine_k', 20),
                    'lag': result['best_params'].get('lag', 12),
                    'profit_quantile': result['best_params'].get('profit_quantile', 0.85),
                    'loss_quantile': result['best_params'].get('loss_quantile', 0.15),
                    'lookback_window': result['best_params'].get('lookback_window', 500),
                    'n_trials': n_trials,
                    'data_shape': result['data_shape'],
                    'feature_range': f"{result['best_params'].get('coarse_k', 60)}ç²—é¸â†’{result['best_params'].get('fine_k', 20)}ç²¾é¸"
                }

                self.logger.info(f"âœ… {timeframe} å„ªåŒ–å®Œæˆ: F1={result['best_score']:.4f}")
                self.logger.info(f"   æœ€ä½³ç‰¹å¾µé¸æ“‡: {best_configs[timeframe]['feature_range']}")

            except Exception as e:
                self.logger.error(f"âŒ {timeframe} å„ªåŒ–å¤±æ•—: {e}")
                best_configs[timeframe] = {
                    'timeframe': timeframe,
                    'error': str(e),
                    'best_score': 0.0
                }

        multi_tf_result = {
            'optimization_type': 'multi_timeframe_feature_selection',
            'timeframes': timeframes,
            'best_configs': best_configs,
            'summary': {
                'successful_optimizations': len([cfg for cfg in best_configs.values() if 'error' not in cfg]),
                'failed_optimizations': len([cfg for cfg in best_configs.values() if 'error' in cfg]),
                'best_performing_timeframe': max(
                    [(tf, cfg['best_score']) for tf, cfg in best_configs.items() if 'error' not in cfg],
                    key=lambda x: x[1], default=('none', 0.0)
                )[0]
            },
            'feature_selection_strategy': {
                'coarse_selection_range': '140-203å€‹ç‰¹å¾µ (70%-100%)',
                'fine_selection_range': '10-25å€‹ç‰¹å¾µ (å¾ç²—é¸çµæœä¸­ç²¾é¸)',
                'multi_timeframe_approach': 'æ¯å€‹æ™‚æ¡†ç¨ç«‹å„ªåŒ–ï¼Œä½¿ç”¨å…¨é‡203ç‰¹å¾µæ± ï¼Œç¢ºä¿æ‰€æœ‰ç‰¹å¾µéƒ½æœ‰è¢«é¸ä¸­æ©Ÿæœƒ'
            }
        }

        output_file = self.config_path / "multi_timeframe_feature_optimization.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(multi_tf_result, f, indent=2, ensure_ascii=False)

        self.logger.info(f"\nğŸ¯ å¤šæ™‚æ¡†å„ªåŒ–å®Œæˆæ‘˜è¦:")
        self.logger.info(f"   æˆåŠŸ: {multi_tf_result['summary']['successful_optimizations']}/{len(timeframes)} å€‹æ™‚æ¡†")
        self.logger.info(f"   æœ€ä½³æ™‚æ¡†: {multi_tf_result['summary']['best_performing_timeframe']}")

        for tf, cfg in best_configs.items():
            if 'error' not in cfg:
                self.logger.info(f"   {tf}: F1={cfg['best_score']:.4f}, {cfg['feature_range']}")
            else:
                self.logger.error(f"   {tf}: å¤±æ•— - {cfg['error']}")

        self.logger.info(f"ğŸ’¾ å¤šæ™‚æ¡†çµæœå·²ä¿å­˜è‡³: {output_file}")

        return multi_tf_result

    def _calc_mfi(self, typical_price: pd.Series, volume: pd.Series, window: int) -> pd.Series:
        positive_flow = (typical_price - typical_price.shift(1)).clip(lower=0) * volume
        negative_flow = (typical_price.shift(1) - typical_price).clip(lower=0) * volume
        pos_sum = positive_flow.rolling(window).sum()
        neg_sum = negative_flow.rolling(window).sum()
        money_ratio = pos_sum / (neg_sum + 1e-9)
        return 100 - (100 / (1 + money_ratio))

    def _calc_cci(self, typical_price: pd.Series, window: int) -> pd.Series:
        tp_ma = typical_price.rolling(window).mean()
        tp_md = (typical_price - tp_ma).abs().rolling(window).mean()
        return (typical_price - tp_ma) / (0.015 * tp_md.replace(0, np.nan))

    def _calc_kdj(self, high: pd.Series, low: pd.Series, close: pd.Series,
                  rsv_window: int, k_smooth: int, d_smooth: int) -> Tuple[pd.Series, pd.Series, pd.Series]:
        lowest = low.rolling(rsv_window).min()
        highest = high.rolling(rsv_window).max()
        rsv = ((close - lowest) / (highest - lowest + 1e-9)) * 100
        k = rsv.ewm(alpha=1 / k_smooth, adjust=False).mean()
        d = k.ewm(alpha=1 / d_smooth, adjust=False).mean()
        j = 3 * k - 2 * d
        return k, d, j

    def _calc_obv(self, close: pd.Series, volume: pd.Series) -> pd.Series:
        direction = np.sign(close.diff()).fillna(0)
        obv = (direction * volume).cumsum()
        return obv

    def _filter_low_quality_features(self, X: pd.DataFrame) -> pd.DataFrame:
        original_cols = X.columns
        constant_cols = original_cols[X.nunique(dropna=False) <= 1]
        if len(constant_cols) > 0:
            X = X.drop(columns=constant_cols)
        # ä½å¤šæ¨£æ€§åˆ—éæ¿¾ï¼šnunique/len < Îµï¼ˆé¿å…é•·æ®µ ffill é€ æˆçš„ä½ä¿¡æ¯åˆ—ï¼‰
        try:
            eps = float(self.flags.get('low_diversity_threshold', 0.003))
            nunique_ratio = X.nunique(dropna=False) / max(1, len(X))
            low_div_cols = nunique_ratio[nunique_ratio < eps].index
            if len(low_div_cols) > 0:
                X = X.drop(columns=low_div_cols)
        except Exception as e:
            self.logger.warning(f"ä½å¤šæ¨£æ€§éæ¿¾å¤±æ•—ï¼Œè·³é: {e}")
        missing_rate = X.isnull().mean()
        high_missing_cols = missing_rate[missing_rate > 0.5].index
        if len(high_missing_cols) > 0:
            X = X.drop(columns=high_missing_cols)
        if X.shape[1] > 1:
            corr_matrix = X.corr().abs()
            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
            high_corr_cols = [col for col in upper.columns if any(upper[col] > 0.95)]
            if len(high_corr_cols) > 0:
                X = X.drop(columns=high_corr_cols)
        self.logger.info(f"ğŸ”§ ç‰¹å¾µè³ªé‡éæ¿¾: åŸå§‹={len(original_cols)}, å¸¸é‡={len(constant_cols)}, é«˜ç¼ºå¤±={len(high_missing_cols)}, é«˜ç›¸é—œ={len(original_cols) - X.shape[1] - len(constant_cols) - len(high_missing_cols)}")
        return X

    def _rebalance_labels(self, labels: pd.Series, lag: int, profit_quantile: float,
                           loss_quantile: float, lookback_window: int) -> pd.Series:
        target = 0.20
        max_iter = 3
        step = 0.02
        cur_profit, cur_loss = profit_quantile, loss_quantile
        for _ in range(max_iter):
            label_counts = labels.value_counts().sort_index()
            total = int(labels.notna().sum())
            if len(label_counts) != 3 or total == 0:
                break
            sell_ratio = label_counts.get(0, 0) / total
            buy_ratio = label_counts.get(2, 0) / total
            improved = False
            if buy_ratio < target:
                cur_profit = max(0.60, cur_profit - step)
                improved = True
            if sell_ratio < target:
                cur_loss = min(0.40, cur_loss + step)
                improved = True
            if not improved:
                break
            self.logger.info(f"ğŸ“Š èª¿æ•´å¾Œ quantile: profit={cur_profit:.3f}, loss={cur_loss:.3f}")
            labels = self._generate_labels(self.close_prices, lag, cur_profit, cur_loss, lookback_window)
        return labels

    def _compute_sample_weights(self, y: pd.Series) -> np.ndarray:
        """è¨ˆç®—å¤šé¡åˆ¥å¹³è¡¡æ¨£æœ¬æ¬Šé‡ï¼Œé¿å…é•·å°¾é¡åˆ¥è¢«å¿½è¦–ã€‚"""
        try:
            y_series = pd.Series(y)
            value_counts = y_series.value_counts().to_dict()
            classes = sorted(value_counts.keys())
            total = float(len(y_series))
            n_classes = float(len(classes))
            class_to_weight = {c: (total / (n_classes * float(value_counts.get(c, 1)))) for c in classes}
            return y_series.map(class_to_weight).astype(float).values
        except Exception as e:
            self.logger.warning(f"æ¨£æœ¬æ¬Šé‡è¨ˆç®—å¤±æ•—ï¼Œæ”¹ç‚ºç­‰æ¬Š: {e}")
            return np.ones(len(y), dtype=float)

    def _search_best_trade_threshold(self, y_true: pd.Series, y_proba: np.ndarray) -> tuple:
        """å°å¤šåˆ†é¡æ©Ÿç‡é€²è¡Œå…©éšæ®µæ±ºç­–çš„é–¾å€¼æœå°‹ï¼šå…ˆåˆ¤æ–·æ˜¯å¦äº¤æ˜“ï¼Œå†åœ¨{sell,buy}ä¸­é¸æœ€å¤§ã€‚"""
        try:
            # å‡è¨­é¡åˆ¥é †åºç‚º [0:sell, 1:hold, 2:buy]
            proba_sell = y_proba[:, 0]
            proba_hold = y_proba[:, 1]
            proba_buy = y_proba[:, 2]
            trade_strength = np.maximum(proba_sell, proba_buy)
            best_tau = 0.5
            best_f1 = -1.0
            y_true_arr = np.asarray(y_true)
            # ç²—æƒ
            tau_min = float(self.flags.get('threshold_tau_min', 0.35))
            tau_max = float(self.flags.get('threshold_tau_max', 0.7))
            tau_step = float(self.flags.get('threshold_tau_step', 0.05))
            tau_grid = np.arange(tau_min, tau_max + 1e-9, tau_step)
            for tau in tau_grid:
                do_trade = trade_strength >= tau
                pred = np.where(do_trade, np.where(proba_sell >= proba_buy, 0, 2), 1)
                f1_m = f1_score(y_true_arr, pred, average='macro', zero_division=0)
                if f1_m > best_f1:
                    best_f1, best_tau, best_pred = f1_m, float(tau), pred
            # ç´°æƒ
            narrow_min = max(tau_min, best_tau - 0.05)
            narrow_max = min(tau_max, best_tau + 0.05)
            narrow_grid = np.arange(narrow_min, narrow_max + 1e-9, 0.01)
            for tau in narrow_grid:
                do_trade = trade_strength >= tau
                pred = np.where(do_trade, np.where(proba_sell >= proba_buy, 0, 2), 1)
                f1_m = f1_score(y_true_arr, pred, average='macro', zero_division=0)
                if f1_m > best_f1:
                    best_f1, best_tau, best_pred = f1_m, float(tau), pred
            return best_tau, best_pred
        except Exception as e:
            self.logger.warning(f"é–¾å€¼æœå°‹éç¨‹å¤±æ•—: {e}")
            # å›é€€ç‚º argmax æ±ºç­–
            try:
                return 0.0, np.argmax(y_proba, axis=1)
            except Exception:
                return 0.0, None

    def _rank_features_by_importance(self, X: pd.DataFrame, y: pd.Series, top_k: int) -> List[str]:
        try:
            if X.shape[1] <= top_k:
                return list(X.columns)
            rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced_subsample')
            rf.fit(X.fillna(0), y)
            importances = pd.Series(rf.feature_importances_, index=X.columns)
            top_features = importances.sort_values(ascending=False).head(top_k).index.tolist()
            self.logger.info(f"ğŸ” é‡è¦åº¦é ç¯©: {X.shape[1]} â†’ {len(top_features)}")
            return top_features
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•è¨ˆç®—ç‰¹å¾µé‡è¦åº¦: {e}")
            return list(X.columns)

    def _drop_highly_correlated(self, X: pd.DataFrame, threshold: float) -> pd.DataFrame:
        if X.shape[1] <= 1:
            return X
        corr_matrix = X.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        if to_drop:
            self.logger.info(f"ğŸ”§ é«˜ç›¸é—œç‰¹å¾µç§»é™¤: {len(to_drop)}")
            X = X.drop(columns=to_drop)
        return X

    def _stability_select_features(self, X: pd.DataFrame, y: pd.Series, coarse_k: int, fine_k: int, threshold: float) -> List[str]:
        if X.shape[1] <= max(fine_k, 20):
            return list(X.columns)
        tscv = TimeSeriesSplit(n_splits=3)
        selection_counts = pd.Series(0, index=X.columns)
        for train_idx, test_idx in tscv.split(X):
            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
            variance_selector = VarianceThreshold(threshold=0.01)
            X_train_var = variance_selector.fit_transform(X_train)
            selected_columns = X_train.columns[variance_selector.get_support(indices=True)]
            if len(selected_columns) == 0:
                continue
            coarse_k_fold = min(coarse_k, len(selected_columns) - 1)
            if coarse_k_fold < 5:
                continue
            coarse_selector = SelectKBest(f_classif, k=coarse_k_fold)
            X_train_coarse = coarse_selector.fit_transform(X_train[selected_columns], y_train)
            selected_coarse = pd.Index(selected_columns)[coarse_selector.get_support(indices=True)]
            fine_k_fold = min(fine_k, len(selected_coarse) - 1)
            if fine_k_fold < 5:
                continue
            fine_selector = SelectKBest(mutual_info_classif, k=fine_k_fold)
            fine_selector.fit(X_train[selected_coarse], y_train)
            selected_fine = selected_coarse[fine_selector.get_support(indices=True)]
            selection_counts[selected_fine] += 1
        selected_features = selection_counts[selection_counts >= threshold * selection_counts.max()].index.tolist()
        self.logger.info(f"ğŸ§® ç©©å®šæ€§é¸æ“‡ä¿ç•™: {len(selected_features)} / {len(selection_counts)}")
        return selected_features if selected_features else list(X.columns)

    def _build_full_feature_matrix(self) -> pd.DataFrame:
        """æ§‹å»ºä¸€æ¬¡æ€§çš„å®Œæ•´ç‰¹å¾µçŸ©é™£ï¼Œå«å¤–éƒ¨ç‰¹å¾µã€æŠ€è¡“æŒ‡æ¨™ã€å¤šæ™‚æ¡†ã€TDã€Wyckoffã€Microèˆ‡è¡ç”Ÿå“ã€‚"""
        ohlcv = self.ohlcv_data
        X = pd.DataFrame(index=ohlcv.index)
 
        if isinstance(self.features, pd.DataFrame) and not self.features.empty:
            X = self._safe_merge(X, self.features, prefix='')
 
        tech = self.generate_technical_features(ohlcv, params={})
        X = self._safe_merge(X, tech, prefix='')
 
        if self.flags.get('enable_td', True):
            X = self._safe_merge(X, self._generate_td_features(ohlcv), prefix='')
        if self.flags.get('enable_wyckoff', True):
            X = self._safe_merge(X, self._generate_wyckoff_features(ohlcv), prefix='')
        if self.flags.get('enable_micro', True):
            X = self._safe_merge(X, self._generate_micro_features_from_ohlcv(ohlcv), prefix='')
 
        deriv = self._load_derivatives_features(ohlcv.index)
        if not deriv.empty:
            X = self._safe_merge(X, deriv, prefix='')
 
        X = self._filter_low_quality_features(X)
        X = X.loc[:, ~X.columns.duplicated()]
        return X.astype('float32').fillna(0)


def main():
    import sys

    if len(sys.argv) == 1:
        print("ğŸš€ å–®ä¸€æ™‚æ¡†å„ªåŒ– (15m)")
        optimizer = FeatureOptimizer(data_path='../data', config_path='../configs', timeframe='15m')
        result = optimizer.optimize(n_trials=50)
        print(f"âœ… ç‰¹å¾µå„ªåŒ–å®Œæˆ: {result['best_score']:.4f}")

    elif len(sys.argv) == 2 and sys.argv[1] == 'multi':
        print("ğŸš€ å¤šæ™‚æ¡†å„ªåŒ– (15m, 1h, 4h)")
        optimizer = FeatureOptimizer(data_path='../data', config_path='../configs')
        multi_result = optimizer.optimize_multi_timeframes(['15m', '1h', '4h'], n_trials=30)

        print(f"\nğŸ“Š å¤šæ™‚æ¡†å„ªåŒ–çµæœ:")
        for tf, config in multi_result['best_configs'].items():
            if 'error' not in config:
                print(f"   {tf}: F1={config['best_score']:.4f}, {config['feature_range']}")

    else:
        print("ç”¨æ³•:")
        print("  python optuna_feature.py        # å–®ä¸€æ™‚æ¡†å„ªåŒ–")
        print("  python optuna_feature.py multi  # å¤šæ™‚æ¡†å„ªåŒ–")


if __name__ == "__main__":
    main()