# -*- coding: utf-8 -*-
"""
Meta Quality Optimizer (Layer 1B)
è³ªé‡è©•ä¼°å™¨ï¼šè©•ä¼° Primary ä¿¡è™Ÿæ˜¯å¦å€¼å¾—åŸ·è¡Œï¼ˆäºŒåˆ†é¡ï¼‰

Meta-Labeling æ¶æ§‹çš„ç¬¬äºŒå±¤ï¼šMeta Model
- ç›®æ¨™ï¼šè©•ä¼° Primary ä¿¡è™Ÿè³ªé‡ï¼ˆåŸ·è¡Œ vs è·³éï¼‰
- è¼¸å…¥ï¼šPrimary ä¿¡è™Ÿ (1/-1) + å¸‚å ´ç‰¹å¾µ
- è¼¸å‡ºï¼š1 (åŸ·è¡Œ) / 0 (è·³é)

åƒè€ƒæ–‡ç»ï¼š
- Marcos LÃ³pez de Prado (2018), "Advances in Financial Machine Learning", Ch.3
"""
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple
import numpy as np
import optuna
from optuna.samplers import NSGAIISampler
import pandas as pd

from optuna_system.utils.io_utils import read_dataframe


class MetaQualityOptimizer:
    """
    Layer 1B: Meta Model - è³ªé‡è©•ä¼°å™¨
    
    ç›®æ¨™ï¼šè©•ä¼° Primary ä¿¡è™Ÿè³ªé‡ï¼ˆåŸ·è¡Œ vs è·³éï¼‰
    è¼¸å…¥ï¼šPrimary ä¿¡è™Ÿ (1/-1) + å¸‚å ´ç‰¹å¾µ
    è¼¸å‡ºï¼š1 (åŸ·è¡Œ) / 0 (è·³é)
    """
    
    def __init__(
        self,
        data_path: str,
        config_path: str = "configs/",
        symbol: str = "BTCUSDT",
        timeframe: str = "15m",
        scaled_config: Dict = None
    ):
        """åˆå§‹åŒ– Meta Model å„ªåŒ–å™¨"""
        self.logger = logging.getLogger(__name__)
        self.data_path = Path(data_path)
        self.config_path = Path(config_path)
        self.symbol = symbol
        self.timeframe = timeframe
        self.scaled_config = scaled_config or {}
        
        # ç”±å¤–éƒ¨è¨­å®šï¼ˆMetaLabelOptimizer èª¿ç”¨ï¼‰
        self.primary_signals = None
        self.price_data = None
    
    def set_primary_signals(self, signals: pd.Series):
        """è¨­å®š Primary ä¿¡è™Ÿï¼ˆç”± MetaLabelOptimizer èª¿ç”¨ï¼‰"""
        self.primary_signals = signals
        self.logger.info(f"âœ… Primary ä¿¡è™Ÿå·²è¨­å®š: {len(signals)} ç­†")
    
    def _compute_final_labels(
        self, 
        primary_signals: pd.Series,  # 1/-1
        meta_quality: pd.Series       # 1/0
    ) -> pd.Series:
        """
        è¨ˆç®—æœ€çµ‚çš„ä¸‰åˆ†é¡æ¨™ç±¤ï¼ˆèˆ‡ Coordinator çš„é‚è¼¯ä¸€è‡´ï¼‰
        
        Args:
            primary_signals: Primary Model çš„äºŒåˆ†é¡ä¿¡è™Ÿ (1=è²·å…¥, -1=è³£å‡º)
            meta_quality: Meta Model çš„è³ªé‡è©•ä¼° (1=åŸ·è¡Œ, 0=è·³é)
            
        Returns:
            pd.Series: æœ€çµ‚ä¸‰åˆ†é¡æ¨™ç±¤ (0=è³£å‡º, 1=æŒæœ‰, 2=è²·å…¥)
        """
        final_label = pd.Series(1, index=primary_signals.index)  # é»˜èªæŒæœ‰
        
        # åªæœ‰ Meta é€šéçš„ä¿¡è™Ÿæ‰åŸ·è¡Œ
        final_label[(primary_signals == 1) & (meta_quality == 1)] = 2   # è²·å…¥
        final_label[(primary_signals == -1) & (meta_quality == 1)] = 0  # è³£å‡º
        
        return final_label
    
    def _select_best_from_pareto(self, study: optuna.Study) -> optuna.Trial:
        """
        å¾ Pareto å‰æ²¿é¸æ“‡æœ€ä½³æŠ˜è¡·è§£ï¼ˆä½¿ç”¨è†é»æ³•ï¼‰
        
        Args:
            study: Optuna å¤šç›®æ¨™å„ªåŒ–çš„ Study å°è±¡
            
        Returns:
            optuna.Trial: é¸ä¸­çš„æœ€ä½³ trial
        """
        # ç²å– Pareto å‰æ²¿çš„æ‰€æœ‰ trials
        pareto_trials = study.best_trials
        
        if len(pareto_trials) == 0:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ° Pareto å‰æ²¿è§£ï¼Œè¿”å›æ‰€æœ‰è©¦é©—ä¸­çš„æœ€ä½³")
            return max(study.trials, key=lambda t: t.values[0] if t.values else -999)
        
        self.logger.info(f"ğŸ“Š Pareto å‰æ²¿åŒ…å« {len(pareto_trials)} å€‹è§£")
        
        # æ–¹æ³•ï¼šè†é»æ³•ï¼ˆKnee Pointï¼‰- æ‰¾åˆ°æ›²ç·šè½‰æŠ˜æœ€å¤§çš„é»
        # æ¨™æº–åŒ–ç›®æ¨™å€¼åˆ° [0, 1]
        obj1_values = np.array([t.values[0] for t in pareto_trials])  # æ€§èƒ½ï¼ˆæœ€å¤§åŒ–ï¼‰
        obj2_values = np.array([t.values[1] for t in pareto_trials])  # åå·®ï¼ˆæœ€å°åŒ–ï¼‰
        
        obj1_norm = (obj1_values - obj1_values.min()) / (obj1_values.max() - obj1_values.min() + 1e-6)
        obj2_norm = (obj2_values - obj2_values.min()) / (obj2_values.max() - obj2_values.min() + 1e-6)
        
        # ç†æƒ³é»ï¼šæœ€å¤§æ€§èƒ½ + æœ€å°åå·®
        ideal_point = np.array([1.0, 0.0])
        
        # è¨ˆç®—æ¯å€‹è§£åˆ°ç†æƒ³é»çš„è·é›¢
        distances = np.sqrt(
            (obj1_norm - ideal_point[0])**2 + 
            (obj2_norm - ideal_point[1])**2
        )
        
        # é¸æ“‡è·é›¢ç†æƒ³é»æœ€è¿‘çš„è§£
        best_idx = np.argmin(distances)
        best_trial = pareto_trials[best_idx]
        
        self.logger.info(
            f"âœ… é¸ä¸­æŠ˜è¡·è§£: æ€§èƒ½={best_trial.values[0]:.4f}, "
            f"æ¨™ç±¤åå·®={best_trial.values[1]:.2%}"
        )
        
        # è¨˜éŒ„ Pareto å‰æ²¿çš„çµ±è¨ˆä¿¡æ¯
        self.logger.info(
            f"ğŸ“ˆ Pareto å‰æ²¿çµ±è¨ˆ:\n"
            f"   æ€§èƒ½ç¯„åœ: {obj1_values.min():.4f} ~ {obj1_values.max():.4f}\n"
            f"   åå·®ç¯„åœ: {obj2_values.min():.2%} ~ {obj2_values.max():.2%}"
        )
        
        return best_trial
    
    def _calculate_atr(self, close: pd.Series, period: int = 14) -> pd.Series:
        """è¨ˆç®— ATRï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        if self.price_data is None:
            # ä½¿ç”¨åƒ¹æ ¼æ¨™æº–å·®ä¼°ç®—
            returns = close.pct_change().abs()
            atr = returns.rolling(period).mean() * close
            return atr
        
        try:
            high = self.price_data['high']
            low = self.price_data['low']
            
            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))
            
            tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr = tr.rolling(period).mean()
            
            return atr
        except Exception as e:
            self.logger.warning(f"âš ï¸ ATR è¨ˆç®—å¤±æ•—: {e}ï¼Œä½¿ç”¨ç°¡åŒ–æ–¹æ³•")
            returns = close.pct_change().abs()
            atr = returns.rolling(period).mean() * close
            return atr
    
    def generate_meta_features(
        self,
        primary_signals: pd.Series,
        price_data: pd.DataFrame,
        params: Dict
    ) -> pd.DataFrame:
        """
        ç”Ÿæˆå…ƒç‰¹å¾µï¼ˆç”¨æ–¼è©•ä¼°ä¿¡è™Ÿè³ªé‡ï¼‰
        
        å…ƒç‰¹å¾µé¡å‹ï¼š
        1. ä¿¡è™Ÿå¼·åº¦ï¼šATR å€æ•¸ã€æ­¢ç›ˆè·é›¢
        2. å¸‚å ´ç’°å¢ƒï¼šæ³¢å‹•ç‡ã€è¶¨å‹¢å¼·åº¦ã€æˆäº¤é‡
        3. ä¿¡è™Ÿä¸€è‡´æ€§ï¼šåƒ¹æ ¼å‹•é‡
        4. æ­·å²è¡¨ç¾ï¼šè¿‘æœŸå‹ç‡ï¼ˆæ»¾å‹•çª—å£ï¼‰
        
        Returns:
            pd.DataFrame: å…ƒç‰¹å¾µçŸ©é™£
        """
        meta_features = pd.DataFrame(index=primary_signals.index)
        close = price_data['close']
        
        # === ç‰¹å¾µçµ„ 1ï¼šä¿¡è™Ÿå¼·åº¦ç‰¹å¾µ ===
        atr_period = params.get('atr_period', 14)
        profit_multiplier = params.get('profit_multiplier', 2.0)
        
        # 1.1 ä¿¡è™Ÿå¼·åº¦ï¼ˆæ­¢ç›ˆè·é›¢ / åƒ¹æ ¼ï¼‰
        atr = self._calculate_atr(close, atr_period)
        atr = atr.reindex(primary_signals.index).fillna(method='ffill').fillna(close.std() * 0.02)
        meta_features['signal_strength'] = (atr * profit_multiplier) / close
        
        # 1.2 é¢¨éšªå›å ±æ¯”
        stop_multiplier = params.get('stop_multiplier', 1.5)
        meta_features['risk_reward_ratio'] = profit_multiplier / stop_multiplier
        
        # === ç‰¹å¾µçµ„ 2ï¼šå¸‚å ´ç’°å¢ƒç‰¹å¾µ ===
        # 2.1 æ³¢å‹•ç‡ï¼ˆ20 æœŸæ¨™æº–å·®ï¼‰
        returns = close.pct_change()
        meta_features['volatility'] = returns.rolling(20).std()
        
        # 2.2 è¶¨å‹¢å¼·åº¦ï¼ˆåƒ¹æ ¼ / 50 æœŸå‡ç·š - 1ï¼‰
        sma_50 = close.rolling(50).mean()
        meta_features['trend_strength'] = (close / sma_50 - 1)
        
        # 2.3 æˆäº¤é‡æ¯”ç‡ï¼ˆç•¶å‰æˆäº¤é‡ / 20 æœŸå‡é‡ï¼‰
        if 'volume' in price_data.columns:
            volume = price_data['volume']
            avg_volume = volume.rolling(20).mean()
            meta_features['volume_ratio'] = volume / avg_volume
        else:
            meta_features['volume_ratio'] = 1.0
        
        # === ç‰¹å¾µçµ„ 3ï¼šåƒ¹æ ¼å‹•é‡ç‰¹å¾µ ===
        # 3.1 çŸ­æœŸå‹•é‡ï¼ˆ5 æœŸæ”¶ç›Šç‡ï¼‰
        meta_features['momentum_5'] = close.pct_change(5)
        
        # 3.2 ä¸­æœŸå‹•é‡ï¼ˆ20 æœŸæ”¶ç›Šç‡ï¼‰
        meta_features['momentum_20'] = close.pct_change(20)
        
        # === ç‰¹å¾µçµ„ 4ï¼šæ­·å²è¡¨ç¾ç‰¹å¾µ ===
        # 4.1 è¿‘æœŸå‹ç‡ï¼ˆæ»¾å‹•çª—å£ï¼‰
        meta_features['recent_winrate'] = self._calculate_rolling_winrate(
            primary_signals, price_data, window=20, lag=params.get('lag', 12)
        )
        
        # 4.2 ä¿¡è™Ÿèˆ‡å‹•é‡ä¸€è‡´æ€§
        signal_direction = primary_signals  # 1 æˆ– -1
        momentum_direction = np.sign(meta_features['momentum_5'])
        meta_features['signal_momentum_alignment'] = (signal_direction == momentum_direction).astype(int)
        
        # å¡«å……ç¼ºå¤±å€¼
        meta_features = meta_features.fillna(method='ffill').fillna(0)
        
        return meta_features
    
    def _calculate_rolling_winrate(
        self,
        signals: pd.Series,
        price_data: pd.DataFrame,
        window: int = 20,
        lag: int = 12
    ) -> pd.Series:
        """
        è¨ˆç®—æ»¾å‹•å‹ç‡ï¼ˆé¿å…æœªä¾†æ•¸æ“šæ´©éœ²ï¼‰
        
        Args:
            signals: Primary ä¿¡è™Ÿ (1/-1)
            price_data: åƒ¹æ ¼æ•¸æ“š
            window: æ»¾å‹•çª—å£å¤§å°
            lag: æœªä¾†æ”¶ç›Šçš„ lag æœŸæ•¸
        
        Returns:
            pd.Series: æ»¾å‹•å‹ç‡
        """
        close = price_data['close']
        
        # è¨ˆç®—æœªä¾†æ”¶ç›Š
        future_returns = close.pct_change(lag).shift(-lag)
        
        # è¨ˆç®—ä¿¡è™Ÿæ”¶ç›Š
        signal_returns = signals.shift(1) * future_returns
        
        # å‹è² åˆ¤æ–·ï¼ˆæ”¶ç›Š > 0 ç‚ºå‹ï¼‰
        wins = (signal_returns > 0).astype(int)
        
        # æ»¾å‹•å‹ç‡
        rolling_winrate = wins.rolling(window, min_periods=5).mean()
        
        return rolling_winrate
    
    def generate_meta_labels(
        self,
        primary_signals: pd.Series,
        price_data: pd.DataFrame,
        params: Dict
    ) -> pd.Series:
        """
        ç”Ÿæˆå…ƒæ¨™ç±¤ï¼ˆè¨“ç·´ç›®æ¨™ï¼‰
        
        é‚è¼¯ï¼š
        - åŸ·è¡Œ Primary ä¿¡è™Ÿèƒ½ç²åˆ© â†’ 1 (åŸ·è¡Œ)
        - å¦å‰‡ â†’ 0 (è·³é)
        
        Returns:
            pd.Series: 1=å¥½ä¿¡è™Ÿï¼ˆåŸ·è¡Œï¼‰ï¼Œ0=å£ä¿¡è™Ÿï¼ˆè·³éï¼‰
        """
        close = price_data['close']
        lag = params.get('lag', 12)
        
        # è¨ˆç®—æœªä¾†æ”¶ç›Š
        future_returns = close.pct_change(lag).shift(-lag)
        
        # è¨ˆç®—ä¿¡è™Ÿæ”¶ç›Š
        signal_returns = primary_signals.shift(1) * future_returns
        
        # å®šç¾©ã€Œå¥½ä¿¡è™Ÿã€ï¼šç²åˆ©è¶…é 2Ã— äº¤æ˜“æˆæœ¬
        transaction_cost = params.get('transaction_cost_bps', 10) / 10000
        
        # å…ƒæ¨™ç±¤ï¼šä¿¡è™Ÿæ”¶ç›Š > é›™é‚Šäº¤æ˜“æˆæœ¬ â†’ 1ï¼ˆåŸ·è¡Œï¼‰
        meta_labels = (signal_returns > transaction_cost * 2).astype(int)
        
        return meta_labels
    
    def evaluate_quality(
        self,
        meta_features: pd.DataFrame,
        params: Dict
    ) -> pd.Series:
        """
        ä½¿ç”¨å…ƒç‰¹å¾µè©•ä¼°è³ªé‡ï¼ˆé–¾å€¼åˆ†é¡å™¨ï¼‰
        
        è©•åˆ†å…¬å¼ï¼š
        quality_score = w1 Ã— signal_strength 
                      + w2 Ã— |trend_strength|
                      + w3 Ã— recent_winrate
                      + w4 Ã— signal_momentum_alignment
        
        Returns:
            pd.Series: 1=é«˜è³ªé‡ï¼ˆåŸ·è¡Œï¼‰ï¼Œ0=ä½è³ªé‡ï¼ˆè·³éï¼‰
        """
        # æå–æ¬Šé‡
        strength_weight = params.get('strength_weight', 0.3)
        trend_weight = params.get('trend_weight', 0.3)
        winrate_weight = params.get('winrate_weight', 0.2)
        alignment_weight = params.get('alignment_weight', 0.2)
        
        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        quality_score = (
            meta_features['signal_strength'] * strength_weight +
            meta_features['trend_strength'].abs() * trend_weight +
            meta_features['recent_winrate'] * winrate_weight +
            meta_features['signal_momentum_alignment'] * alignment_weight
        )
        
        # äºŒåˆ†é¡ï¼šåˆ†æ•¸ > é–¾å€¼ â†’ åŸ·è¡Œ
        quality_threshold = params.get('quality_threshold', 0.5)
        quality_labels = (quality_score > quality_threshold).astype(int)
        
        return quality_labels
    
    def objective(self, trial: optuna.Trial) -> Tuple[float, float]:
        """
        å¤šç›®æ¨™å„ªåŒ–ç›®æ¨™å‡½æ•¸ï¼ˆNSGA-IIï¼‰
        
        å„ªåŒ–ç›®æ¨™ï¼š
        1. æœ€å¤§åŒ–ï¼šæ¨¡å‹æ€§èƒ½ï¼ˆF1 + Sharpeï¼‰
        2. æœ€å°åŒ–ï¼šæ¨™ç±¤åˆ†å¸ƒåå·®
        
        Returns:
            Tuple[float, float]: (æ€§èƒ½å¾—åˆ†, æ¨™ç±¤åå·®)
        """
        if self.primary_signals is None or self.price_data is None:
            self.logger.error("âŒ å¿…é ˆå…ˆè¨­å®š Primary ä¿¡è™Ÿå’Œåƒ¹æ ¼æ•¸æ“š")
            return -999.0, 999.0
        
        # ğŸ”§ åƒæ•¸æœç´¢ç©ºé–“ï¼ˆæ”¶çª„è³ªé‡é–¾å€¼ç¯„åœï¼‰
        params = {
            'lag': 12,  # å¾ Primary Model ç¹¼æ‰¿
            'atr_period': 14,
            'profit_multiplier': 2.0,
            'stop_multiplier': 1.5,
            'transaction_cost_bps': trial.suggest_float('transaction_cost_bps', 5.0, 15.0),
            
            # Meta Model ç‰¹å®šåƒæ•¸
            'strength_weight': trial.suggest_float('strength_weight', 0.1, 0.5),
            'trend_weight': trial.suggest_float('trend_weight', 0.1, 0.4),
            'winrate_weight': trial.suggest_float('winrate_weight', 0.1, 0.4),
            'alignment_weight': trial.suggest_float('alignment_weight', 0.05, 0.3),
            
            # ğŸ¯ P0ä¿®å¾©ï¼šé™ä½quality_thresholdé¿å…Metaéåº¦ä¿å®ˆ
            # å•é¡Œåˆ†æï¼ˆv63æ¸¬è©¦ï¼‰ï¼š
            # - èˆŠç¯„åœ0.32-0.48å°è‡´90%ä¿¡è™Ÿè®ŠæŒæœ‰ï¼ŒåŸ·è¡Œç‡åƒ…9.5%
            # - å¬å›ç‡åƒ…13.1%ï¼ŒéŒ¯éå¤§é‡äº¤æ˜“æ©Ÿæœƒ
            # 
            # ä¿®å¾©æ–¹æ¡ˆï¼š
            # - é™ä½åˆ°0.20-0.35ç¯„åœ
            # - é æœŸåŸ·è¡Œç‡æå‡åˆ°15-25%
            # - æŒæœ‰æ¯”ä¾‹å¾90%é™åˆ°70-80%
            # - ä¿æŒé«˜ç²¾åº¦ï¼ˆ77.6%ï¼‰åŒæ™‚æå‡å¬å›ç‡
            # 
            # å­¸è¡“ä¾æ“šï¼šMeta-Labelingæ‡‰éæ¿¾ä½è³ªé‡ä¿¡è™Ÿï¼Œä½†ä¸æ‡‰éåº¦ä¿å®ˆ
            # åƒè€ƒï¼šLÃ³pez de Prado (2018) Ch.3 - Meta-Labeling
            'quality_threshold': trial.suggest_float('quality_threshold', 0.20, 0.35),
        }
        
        try:
            # ç”Ÿæˆå…ƒç‰¹å¾µå’Œå…ƒæ¨™ç±¤
            meta_features = self.generate_meta_features(
                self.primary_signals, self.price_data, params
            )
            meta_labels = self.generate_meta_labels(
                self.primary_signals, self.price_data, params
            )
            
            # é æ¸¬è³ªé‡
            predicted_quality = self.evaluate_quality(meta_features, params)
            
            # ç¢ºä¿æœ‰è¶³å¤ çš„æ¨£æœ¬ï¼ˆæ”¾å®½çº¦æŸï¼š100â†’50ï¼‰
            if len(predicted_quality) < 50:
                self.logger.warning(f"âš ï¸ æ¨£æœ¬æ•¸éå°‘: {len(predicted_quality)} < 50")
                return -999.0, 999.0
            
            # ğŸ¯ ç›®æ¨™ 1ï¼šæ¨¡å‹æ€§èƒ½ï¼ˆF1 + Sharpeï¼‰
            tp = ((predicted_quality == 1) & (meta_labels == 1)).sum()
            fp = ((predicted_quality == 1) & (meta_labels == 0)).sum()
            fn = ((predicted_quality == 0) & (meta_labels == 1)).sum()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall + 1e-6)
            
            # Sharpe è¨ˆç®—
            filtered_signals = self.primary_signals.copy()
            filtered_signals[predicted_quality == 0] = 0
            
            lag = params['lag']
            future_returns = self.price_data['close'].pct_change(lag).shift(-lag)
            future_returns = future_returns.loc[filtered_signals.index]
            
            returns = filtered_signals.shift(1) * future_returns
            sharpe = (returns.mean() / (returns.std() + 1e-6)) * np.sqrt(252)
            
            # ç¶œåˆæ€§èƒ½å¾—åˆ†ï¼ˆåˆå§‹ï¼‰
            performance_score_base = (
                f1 * 0.5 +
                max(0, min(sharpe, 10)) * 0.05  # Sharpe è²¢ç»é™ä½ï¼Œä¸Šé™10
            )
            
            # ğŸ¯ ç›®æ¨™ 2ï¼šæ¨™ç±¤åˆ†å¸ƒåå·®ï¼ˆéœ€è¦æœ€å°åŒ–ï¼‰
            execution_ratio = (predicted_quality == 1).sum() / len(predicted_quality)
            
            # ğŸ”§ P0ä¿®å¤v2ï¼šæ”¹ä¸ºè½¯çº¦æŸï¼ˆæƒ©ç½šè€Œéæ‹’ç»ï¼‰
            # é—®é¢˜åˆ†æï¼š
            # - ç¡¬çº¦æŸï¼ˆè¿”å›-999ï¼‰å¯¼è‡´Optunaæ— è§£ç©ºé—´
            # - å³ä½¿execution_ratio=7.6%åœ¨èŒƒå›´å†…ï¼Œä»å¯èƒ½å› ä¸ºå…¶ä»–åŸå› å¤±è´¥
            # 
            # ä¿®å¤æ–¹æ¡ˆï¼šè½¯çº¦æŸï¼ˆå…è®¸æ¢ç´¢ï¼Œä½†æƒ©ç½šåç¦»ï¼‰
            # - ç†æƒ³èŒƒå›´ï¼š5-40%
            # - åç¦»æƒ©ç½šï¼šçº¿æ€§é™ä½performance_score
            # - ä¸å†ç›´æ¥è¿”å›-999
            
            # è®¡ç®—execution_ratioåç¦»æƒ©ç½š
            exec_penalty = 0.0
            if execution_ratio < 0.05:
                # å¤ªä½ï¼ˆ<5%ï¼‰ï¼šæƒ©ç½šåŠ›åº¦éšåç¦»å¢å¤§
                exec_penalty = (0.05 - execution_ratio) * 2.0  # æœ€å¤š-0.1
            elif execution_ratio > 0.40:
                # å¤ªé«˜ï¼ˆ>40%ï¼‰ï¼šæƒ©ç½šåŠ›åº¦éšåç¦»å¢å¤§
                exec_penalty = (execution_ratio - 0.40) * 1.0  # æœ€å¤š-0.6
            
            # è®°å½•çº¦æŸçŠ¶æ€ï¼ˆè½¯çº¦æŸï¼Œä¸å†ç¡¬æ‹’ç»ï¼‰
            trial.set_user_attr("execution_ratio", execution_ratio)
            trial.set_user_attr("exec_penalty", exec_penalty)
            trial.set_user_attr("constraint_violated", False)  # è½¯çº¦æŸä¸ç®—è¿å
            
            # è¨ˆç®—æœ€çµ‚ä¸‰åˆ†é¡æ¨™ç±¤çš„åˆ†å¸ƒ
            final_labels = self._compute_final_labels(self.primary_signals, predicted_quality)
            
            label_dist = {}
            for i in [0, 1, 2]:
                label_dist[i] = (final_labels == i).sum() / len(final_labels)
            
            # èˆ‡ç›®æ¨™åˆ†å¸ƒçš„æœ€å¤§åå·®
            target_dist = [0.25, 0.50, 0.25]
            max_deviation = max(
                abs(label_dist[0] - target_dist[0]),
                abs(label_dist[1] - target_dist[1]),
                abs(label_dist[2] - target_dist[2])
            )
            
            # åº”ç”¨è½¯çº¦æŸæƒ©ç½š
            performance_score = performance_score_base - exec_penalty
            
            # è¨˜éŒ„è©³ç´°ä¿¡æ¯
            trial.set_user_attr("f1_score", f1)
            trial.set_user_attr("precision", precision)
            trial.set_user_attr("recall", recall)
            trial.set_user_attr("sharpe", sharpe)
            trial.set_user_attr("execution_ratio", execution_ratio)
            trial.set_user_attr("label_deviation", max_deviation)
            trial.set_user_attr("buy_pct", label_dist[2])
            trial.set_user_attr("hold_pct", label_dist[1])
            trial.set_user_attr("sell_pct", label_dist[0])
            trial.set_user_attr("performance_score_base", performance_score_base)
            trial.set_user_attr("performance_score_final", performance_score)
            trial.set_user_attr("constraint_violated", False)
            
            return performance_score, max_deviation
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Meta Model è©•ä¼°å¤±æ•—: {e}")
            return -999.0, 999.0
    
    def optimize(self, n_trials: int = 100) -> Dict:
        """
        åŸ·è¡Œå¤šç›®æ¨™å„ªåŒ–ï¼ˆNSGA-IIï¼‰
        
        Args:
            n_trials: è©¦é©—æ¬¡æ•¸ï¼ˆæ¨è–¦ 400-600ï¼‰
            
        Returns:
            Dict: å„ªåŒ–çµæœï¼ŒåŒ…å«æœ€ä½³åƒæ•¸å’Œæ€§èƒ½æŒ‡æ¨™
        """
        self.logger.info("ğŸš€ Meta Model å¤šç›®æ¨™å„ªåŒ–é–‹å§‹ï¼ˆNSGA-IIï¼‰...")
        self.logger.info(f"   ç›®æ¨™1: æœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½ï¼ˆF1 + Sharpeï¼‰")
        self.logger.info(f"   ç›®æ¨™2: æœ€å°åŒ–æ¨™ç±¤åˆ†å¸ƒåå·®")
        self.logger.info(f"   ç¸½è©¦é©—æ•¸: {n_trials}")
        
        if self.primary_signals is None:
            raise ValueError("âŒ å¿…é ˆå…ˆè¨­å®š Primary ä¿¡è™Ÿ (set_primary_signals)")
        
        # ğŸ¯ ä½¿ç”¨ NSGA-II æ¡æ¨£å™¨
        # population_size: å»ºè­°ç‚º 30-40ï¼Œè©¦é©—æ•¸è¶Šå¤šå¯ä»¥ç¨å¤§
        population_size = min(40, max(30, n_trials // 15))
        
        sampler = NSGAIISampler(
            population_size=population_size,
            mutation_prob=None,  # è‡ªå‹•èª¿æ•´
            crossover_prob=0.9,
            swapping_prob=0.5,
            seed=42  # å¯é‡ç¾æ€§
        )
        
        study = optuna.create_study(
            directions=['maximize', 'minimize'],  # æœ€å¤§åŒ–æ€§èƒ½ï¼Œæœ€å°åŒ–åå·®
            sampler=sampler,
            study_name=f'meta_quality_multiobjective_{self.timeframe}'
        )
        
        self.logger.info(f"   ç¨®ç¾¤å¤§å°: {population_size}")
        self.logger.info(f"   é è¨ˆä»£æ•¸: {n_trials // population_size}")
        
        # åŸ·è¡Œå„ªåŒ–
        study.optimize(self.objective, n_trials=n_trials, show_progress_bar=False)
        
        # å¾ Pareto å‰æ²¿é¸æ“‡æœ€ä½³æŠ˜è¡·è§£
        best_trial = self._select_best_from_pareto(study)
        
        best_params = best_trial.params
        performance_score = best_trial.values[0]
        label_deviation = best_trial.values[1]
        
        self.logger.info(f"âœ… Meta Model å„ªåŒ–å®Œæˆ!")
        self.logger.info(f"ğŸ“‹ æœ€ä½³åƒæ•¸: {best_params}")
        self.logger.info(f"ğŸ“Š æ€§èƒ½å¾—åˆ†: {performance_score:.4f}")
        self.logger.info(f"ğŸ“Š æ¨™ç±¤åå·®: {label_deviation:.2%}")
        
        # çµ±è¨ˆ Pareto å‰æ²¿
        pareto_count = len(study.best_trials)
        self.logger.info(f"ğŸ¯ Pareto å‰æ²¿è§£æ•¸é‡: {pareto_count}/{n_trials}")
        
        # è¿”å›çµæœ
        return {
            'best_params': best_params,
            'best_score': performance_score,
            'label_deviation': label_deviation,
            'n_trials': n_trials,
            'pareto_front_size': pareto_count,
            'study': study,
            'f1_score': best_trial.user_attrs.get('f1_score', 0),
            'precision': best_trial.user_attrs.get('precision', 0),
            'recall': best_trial.user_attrs.get('recall', 0),
            'sharpe': best_trial.user_attrs.get('sharpe', 0),
            'execution_ratio': best_trial.user_attrs.get('execution_ratio', 0),
            'buy_pct': best_trial.user_attrs.get('buy_pct', 0),
            'hold_pct': best_trial.user_attrs.get('hold_pct', 0),
            'sell_pct': best_trial.user_attrs.get('sell_pct', 0)
        }
    
    def apply_transform(self, data: pd.DataFrame, params: Dict) -> pd.DataFrame:
        """çµ±ä¸€ç‰©åŒ–æ¥å£ï¼ˆCoordinator èª¿ç”¨ï¼‰"""
        if self.primary_signals is None:
            raise ValueError("å¿…é ˆå…ˆèª¿ç”¨ set_primary_signals() è¨­ç½® Primary ä¿¡è™Ÿ")
        
        meta_features = self.generate_meta_features(self.primary_signals, data, params)
        meta_quality = self.evaluate_quality(meta_features, params)
        
        result = data.loc[meta_quality.index].copy()
        result['meta_quality'] = meta_quality
        return result


if __name__ == "__main__":
    # ç¨ç«‹æ¸¬è©¦è…³æœ¬
    import sys
    sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))
    
    print("ğŸš€ Meta Quality Optimizer æ¸¬è©¦")
    print("âš ï¸ éœ€è¦å…ˆé‹è¡Œ Primary Model ç”Ÿæˆä¿¡è™Ÿ")
    
    # é€™è£¡éœ€è¦å¾ Primary Model ç²å–ä¿¡è™Ÿ
    # å¯¦éš›ä½¿ç”¨æ™‚ç”± MetaLabelOptimizer å”èª¿
    
    print("âœ… Meta Model æ¡†æ¶å·²æº–å‚™å°±ç·’")
    print("ğŸ“‹ å¾…æ•´åˆåˆ° MetaLabelOptimizer ä¸­ä½¿ç”¨")
