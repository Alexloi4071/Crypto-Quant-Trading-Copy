# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¶…åƒæ•¸å„ªåŒ–å™¨ (ç¬¬3å±¤) - å¤šæ¨¡å‹é›†æˆç‰ˆæœ¬
åˆ†åˆ¥å„ªåŒ–LightGBMã€XGBoostã€CatBoostä¸¦ä¿å­˜æ‰€æœ‰é æ¸¬çµæœ
"""
import json
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import lightgbm as lgb
import numpy as np
import optuna
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss
from sklearn.model_selection import TimeSeriesSplit

from optuna_system.utils.io_utils import read_dataframe

try:
    import xgboost as xgb
except ImportError:  # pragma: no cover - å¯é¸ä¾è³´
    xgb = None

try:
    from catboost import CatBoostClassifier
except ImportError:  # pragma: no cover
    CatBoostClassifier = None

warnings.filterwarnings('ignore')


class ModelOptimizer:
    """æ¨¡å‹è¶…åƒæ•¸å„ªåŒ–å™¨ - ç¬¬3å±¤å„ªåŒ–ï¼ˆå¤šæ¨¡å‹é›†æˆç‰ˆæœ¬ï¼‰"""

    def __init__(self, data_path: str, config_path: str = "configs/",
                 symbol: str = "BTCUSDT", timeframe: str = "15m",
                 results_path: str = None):
        self.data_path = Path(data_path)
        self.config_path = Path(config_path)
        self.config_path.mkdir(exist_ok=True)
        
        # è¨­ç½®çµæœä¿å­˜è·¯å¾‘
        if results_path:
            self.results_path = Path(results_path)
        else:
            self.results_path = Path("optuna_system/results") / f"{symbol}_{timeframe}"
        self.results_path.mkdir(parents=True, exist_ok=True)
        
        self.symbol = symbol
        self.timeframe = timeframe

        # ä½¿ç”¨é›†ä¸­æ—¥èªŒ
        self.logger = logging.getLogger(__name__)
        
        # å®šç¾©è¦è¨“ç·´çš„æ¨¡å‹ï¼ˆå®Œæ•´5æ¨¡å‹æ–¹æ¡ˆï¼‰
        self.models_to_train = ['lightgbm', 'xgboost', 'catboost', 'randomforest', 'extratrees']

    def get_available_models(self) -> List[str]:
        available_models: List[str] = []
        if 'lightgbm' in self.models_to_train:
            available_models.append('lightgbm')
        if 'xgboost' in self.models_to_train and xgb is not None:
            available_models.append('xgboost')
        if 'catboost' in self.models_to_train and CatBoostClassifier is not None:
            available_models.append('catboost')
        if 'randomforest' in self.models_to_train:
            available_models.append('randomforest')
        if 'extratrees' in self.models_to_train:
            available_models.append('extratrees')
        return available_models

    def load_data(self) -> Tuple[pd.DataFrame, pd.Series, Optional[pd.Series]]:
        """ä½¿ç”¨èˆ‡Layer2ç›¸åŒçš„æ•¸æ“šåŠ è¼‰æ–¹å¼"""
        try:
            feature_candidates = [
                self.config_path / f"selected_features_{self.timeframe}.parquet",
                self.config_path / f"selected_features_{self.timeframe}.pkl",
                self.config_path / "selected_features.parquet",
                self.config_path / "selected_features.pkl"
            ]
            feature_file = next((p for p in feature_candidates if p.exists()), None)
            if feature_file is None:
                self.logger.error("ç‰¹å¾µæ–‡ä»¶ä¸å­˜åœ¨æ–¼ configs ç›®éŒ„ï¼Œè«‹å…ˆåŸ·è¡Œ Layer2")
                return pd.DataFrame(), pd.Series(), None

            features_df = read_dataframe(feature_file)
            # é˜²æ´©æ¼ï¼šç‰©åŒ–ç‰¹å¾µé›†å¯èƒ½å« labelï¼ŒLayer3 è¼‰å…¥å¾Œå…ˆç§»é™¤
            if 'label' in features_df.columns:
                features_df = features_df.drop(columns=['label'])
                self.logger.warning("Dropped target column 'label' from features_df to prevent leakage")
            self.logger.info(f"æˆåŠŸåŠ è¼‰ç‰¹å¾µæ–‡ä»¶: {feature_file}")

            label_candidates = [
                self.config_path / f"labels_{self.timeframe}.parquet",
                self.config_path / f"labels_{self.timeframe}.pkl",
                self.config_path / "labels.parquet",
                self.config_path / "labels.pkl"
            ]
            label_file = next((p for p in label_candidates if p.exists()), None)
            if label_file is None:
                self.logger.error("æ¨™ç±¤æ–‡ä»¶ä¸å­˜åœ¨æ–¼ configs ç›®éŒ„ï¼Œè«‹å…ˆåŸ·è¡Œ Layer1")
                return pd.DataFrame(), pd.Series(), None

            labels_df = read_dataframe(label_file)
            labels = labels_df['label'] if 'label' in labels_df.columns else labels_df.iloc[:, 0]
            self.logger.info(f"æˆåŠŸåŠ è¼‰æ¨™ç±¤æ–‡ä»¶: {label_file}")

            cleaned_candidates = [
                self.config_path / f"cleaned_ohlcv_{self.timeframe}.parquet",
                self.config_path / f"cleaned_ohlcv_{self.timeframe}.pkl",
                Path("data/processed/cleaned") / f"{self.symbol}_{self.timeframe}" / "cleaned_ohlcv.pkl",
                Path("data/processed/cleaned") / f"{self.symbol}_{self.timeframe}" / "cleaned_ohlcv.parquet"
            ]
            cleaned_file = next((p for p in cleaned_candidates if p.exists()), None)
            returns_series = None
            if cleaned_file is not None and cleaned_file.exists():
                ohlcv_df = read_dataframe(cleaned_file)
                if 'close' in ohlcv_df.columns:
                    returns_series = ohlcv_df['close'].pct_change()
                    self.logger.info(f"æˆåŠŸè¨ˆç®—æ”¶ç›Šç‡")

            # å°é½Šç´¢å¼•
            common_index = features_df.index.intersection(labels.index)
            if len(common_index) == 0:
                self.logger.error("ç‰¹å¾µå’Œæ¨™ç±¤ç´¢å¼•æ²’æœ‰äº¤é›†")
                return pd.DataFrame(), pd.Series(), None

            features_df = features_df.loc[common_index]
            labels = labels.loc[common_index]
            if returns_series is not None:
                returns_series = returns_series.loc[returns_series.index.intersection(common_index)]

            self.logger.info(f"æ•¸æ“šåŠ è¼‰å®Œæˆ: {features_df.shape[0]} æ¨£æœ¬, {features_df.shape[1]} ç‰¹å¾µ")
            self.logger.info(f"æ¨™ç±¤åˆ†ä½ˆ: {labels.value_counts().to_dict()}")

            return features_df, labels, returns_series

        except Exception as e:
            self.logger.error(f"æ•¸æ“šåŠ è¼‰å¤±æ•—: {e}")
            return pd.DataFrame(), pd.Series(), None

    def suggest_model_params(self, trial: optuna.Trial, model_type: str) -> Dict:
        """ç‚ºæŒ‡å®šæ¨¡å‹é¡å‹ç”Ÿæˆè¶…åƒæ•¸"""

        if model_type == 'lightgbm':
            max_depth = trial.suggest_int('max_depth', 3, 10)
            # å‹•æ…‹è¨ˆç®— num_leaves çš„ä¸Šä¸‹ç•Œï¼Œé¿å… low > high çš„ç„¡æ•ˆå€é–“
            upper_num_leaves = min((2 ** max_depth) - 1, 256)
            lower_num_leaves = 16 if upper_num_leaves >= 16 else max(2, upper_num_leaves)
            num_leaves = trial.suggest_int('num_leaves', lower_num_leaves, upper_num_leaves)
            return {
                'type': 'lightgbm',
                'params': {
                    'objective': 'multiclass',
                    'num_class': 3,
                    'metric': 'multi_logloss',
                    'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),
                    'verbosity': -1,
                    'seed': 42,
                    'num_leaves': num_leaves,
                    'max_depth': max_depth,
                    'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1),
                    'n_estimators': trial.suggest_int('n_estimators', 400, 1000),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
                    'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),
                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                    'min_child_samples': trial.suggest_int('min_child_samples', 10, 120),
                    'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 5.0),
                    'class_weight': 'balanced',
                    'num_threads': 0,
                    'deterministic': True
                }
            }

        elif model_type == 'xgboost' and xgb is not None:
            max_depth = trial.suggest_int('max_depth', 3, 10)
            return {
                'type': 'xgboost',
                'params': {
                    'objective': 'multi:softprob',
                    'num_class': 3,
                    'eval_metric': 'mlogloss',
                    'tree_method': trial.suggest_categorical('tree_method', ['hist', 'approx']),
                    'max_depth': max_depth,
                    'eta': trial.suggest_float('eta', 0.03, 0.1),
                    'n_estimators': trial.suggest_int('n_estimators', 400, 1000),
                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                    'lambda': trial.suggest_float('lambda', 0.0, 5.0),
                    'alpha': trial.suggest_float('alpha', 0.0, 5.0),
                    'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),
                    'gamma': trial.suggest_float('gamma', 0.0, 5.0),
                    'seed': 42
                }
            }

        elif model_type == 'catboost' and CatBoostClassifier is not None:
            depth = trial.suggest_int('depth', 4, 10)
            return {
                'type': 'catboost',
                'params': {
                    'loss_function': 'MultiClass',
                    'eval_metric': 'TotalF1',
                    'iterations': trial.suggest_int('iterations', 500, 1500),
                    'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.3),
                    'depth': depth,
                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 6.0),
                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 5.0),
                    'random_strength': trial.suggest_float('random_strength', 0.0, 2.0),
                    'border_count': trial.suggest_int('border_count', 32, 255),
                    'verbose': False,
                    'random_seed': 42
                }
            }

        elif model_type == 'randomforest':
            return {
                'type': 'randomforest',
                'params': {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                    'class_weight': 'balanced',
                    'random_state': 42,
                    'n_jobs': -1,
                    'bootstrap': True
                }
            }

        elif model_type == 'extratrees':
            return {
                'type': 'extratrees',
                'params': {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10),
                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
                    'class_weight': 'balanced',
                    'random_state': 42,
                    'n_jobs': -1,
                    'bootstrap': False
                }
            }
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹æˆ–ç¼ºå°‘ä¾è³´: {model_type}")

    def fit_and_predict(self, model_info: Dict, X_train: pd.DataFrame, y_train: pd.Series, 
                       X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """è¨“ç·´æ¨¡å‹ä¸¦ç”Ÿæˆé æ¸¬"""
        model_type = model_info['type']
        params = model_info['params']

        try:
            if model_type == 'lightgbm':
                model = lgb.LGBMClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test)
                proba = model.predict_proba(X_test)

            elif model_type == 'xgboost':
                model = xgb.XGBClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test)
                proba = model.predict_proba(X_test)

            elif model_type == 'catboost':
                model = CatBoostClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test).flatten()
                proba = model.predict_proba(X_test)

            elif model_type == 'randomforest':
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test)
                proba = model.predict_proba(X_test)

            elif model_type == 'extratrees':
                from sklearn.ensemble import ExtraTreesClassifier
                model = ExtraTreesClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test)
                proba = model.predict_proba(X_test)

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_type}")

            return preds, proba

        except Exception as e:
            self.logger.error(f"æ¨¡å‹è¨“ç·´å¤±æ•— ({model_type}): {e}")
            # è¿”å›é»˜èªé æ¸¬
            preds = np.ones(len(X_test))
            proba = np.ones((len(X_test), 3)) / 3
            return preds, proba

    def purged_walk_forward_cv(self, X: pd.DataFrame, y: pd.Series, model_info: Dict,
                               n_splits: int = 4, embargo_pct: float = 0.02,
                               purge_pct: float = 0.01,
                               returns: Optional[pd.Series] = None) -> Dict:
        """å¯¦ç¾å¸¶ purged & embargo çš„ Walk-Forward CV"""

        total_len = len(X)
        split_size = max(1, total_len // (n_splits + 1))
        embargo = max(1, int(total_len * embargo_pct))
        purge = max(1, int(total_len * purge_pct))

        metrics = {
            'f1_weighted': [],
            'f1_macro': [],
            'accuracy': [],
            'precision_weighted': [],
            'recall_weighted': [],
            'auc_macro': [],
            'logloss': [],
            'strategy_return': [],
            'sharpe': [],
            'win_rate': []
        }

        for i in range(n_splits):
            train_end = (i + 1) * split_size
            train_end_purged = max(purge, train_end - purge)

            test_start = min(total_len, train_end + embargo)
            test_end = min(total_len, test_start + split_size)

            if test_end - test_start < 20 or train_end_purged <= purge:
                continue

            train_idx = list(range(purge, train_end_purged))
            test_idx = list(range(test_start, test_end))

            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            preds, proba = self.fit_and_predict(model_info, X_train, y_train, X_test)

            metrics['f1_weighted'].append(f1_score(y_test, preds, average='weighted'))
            metrics['f1_macro'].append(f1_score(y_test, preds, average='macro'))
            metrics['accuracy'].append(accuracy_score(y_test, preds))
            metrics['precision_weighted'].append(precision_score(y_test, preds, average='weighted', zero_division=0))
            metrics['recall_weighted'].append(recall_score(y_test, preds, average='weighted', zero_division=0))
            metrics['logloss'].append(log_loss(y_test, proba))

            try:
                metrics['auc_macro'].append(roc_auc_score(y_test, proba, multi_class='ovo', average='macro'))
            except Exception:
                metrics['auc_macro'].append(0.5)

            if returns is not None:
                aligned_idx = returns.index.intersection(y_test.index)
                if len(aligned_idx) > 10:
                    positions = pd.Series(preds, index=y_test.index).reindex(aligned_idx).fillna(1) - 1
                    strategy_ret = positions * returns.reindex(aligned_idx).shift(-1)
                    strategy_ret = strategy_ret.dropna()
                    if len(strategy_ret) > 0:
                        metrics['strategy_return'].append(strategy_ret.sum())
                        std = strategy_ret.std()
                        sharpe = (strategy_ret.mean() / std * np.sqrt(252)) if std > 0 else 0
                        metrics['sharpe'].append(sharpe)
                        metrics['win_rate'].append((strategy_ret > 0).mean())

        summary = {f'{k}_mean': np.mean(v) if v else 0 for k, v in metrics.items()}
        summary.update({f'{k}_std': np.std(v) if v else 0 for k, v in metrics.items()})
        summary['folds'] = len(metrics['f1_weighted'])
        
        # Sharpeæ¯”ç‡å½’ä¸€åŒ–åˆ°0-1åŒºé—´
        def normalize_sharpe(sharpe, min_val=-2.0, max_val=5.0):
            """å°†Sharpeæ¯”ç‡å½’ä¸€åŒ–åˆ°0-1åŒºé—´"""
            if sharpe is None or (isinstance(sharpe, float) and sharpe != sharpe):  # Noneæˆ–NaN
                return 0.0
            return max(0.0, min(1.0, (sharpe - min_val) / (max_val - min_val)))
        
        sharpe_mean = summary.get('sharpe_mean', 0)
        normalized_sharpe = normalize_sharpe(sharpe_mean)
        
        # ä¿®æ”¹åçš„composite_scoreï¼ˆä½¿ç”¨å½’ä¸€åŒ–çš„Sharpeï¼ŒèŒƒå›´0-1ï¼‰
        summary['composite_score'] = (
            summary['f1_weighted_mean'] * 0.40 +      # æé«˜F1æƒé‡
            summary['f1_macro_mean'] * 0.25 +         # å‡è¡¡æ€§
            summary['accuracy_mean'] * 0.10 +         # æ•´ä½“å‡†ç¡®æ€§
            summary['auc_macro_mean'] * 0.15 +        # æ’åºèƒ½åŠ›
            normalized_sharpe * 0.08 +                # å½’ä¸€åŒ–Sharpe
            summary['win_rate_mean'] * 0.02           # èƒœç‡
        )
        
        # ä¿å­˜åŸå§‹å’Œå½’ä¸€åŒ–çš„Sharpeä¾›å‚è€ƒ
        summary['sharpe_raw'] = sharpe_mean
        summary['sharpe_normalized'] = normalized_sharpe
        
        return summary

    def objective(self, trial: optuna.Trial, model_type: str) -> float:
        """Optunaç›®æ¨™å‡½æ•¸ - é‡å°æŒ‡å®šæ¨¡å‹é¡å‹"""

        try:
            # åŠ è¼‰æ•¸æ“š
            features_df, labels, returns_series = self.load_data()

            if len(features_df) == 0 or len(labels) == 0:
                return -999.0

            model_info = self.suggest_model_params(trial, model_type)
            cv_results = self.purged_walk_forward_cv(
                features_df,
                labels,
                model_info,
                n_splits=trial.suggest_int('n_cv_splits', 3, 5),
                embargo_pct=trial.suggest_float('embargo_pct', 0.01, 0.05),
                purge_pct=trial.suggest_float('purge_pct', 0.005, 0.03),
                returns=returns_series
            )

            return cv_results.get('composite_score', -999.0)

        except Exception as e:
            self.logger.error(f"æ¨¡å‹å„ªåŒ–éç¨‹å‡ºéŒ¯ ({model_type}): {e}")
            return -999.0

    def optimize_single_model(self, model_type: str, n_trials: int = 50) -> Dict:
        """å„ªåŒ–å–®å€‹æ¨¡å‹çš„è¶…åƒæ•¸"""
        self.logger.info(f"é–‹å§‹å„ªåŒ– {model_type} æ¨¡å‹...")

        # å‰µå»ºç ”ç©¶
        study = optuna.create_study(
            direction='maximize',
            study_name=f'{model_type}_optimization_layer3',
            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=5)
        )

        # åŸ·è¡Œå„ªåŒ– - ä½¿ç”¨lambdaåŒ…è£objective
        study.optimize(lambda trial: self.objective(trial, model_type), n_trials=n_trials)

        # ç²å–æœ€å„ªåƒæ•¸
        best_params = study.best_params
        best_score = study.best_value

        self.logger.info(f"{model_type} å„ªåŒ–å®Œæˆ! æœ€ä½³å¾—åˆ†: {best_score:.4f}")
        self.logger.info(f"{model_type} æœ€å„ªåƒæ•¸: {best_params}")

        return {
            'model_type': model_type,
            'best_params': best_params,
            'best_score': best_score,
            'n_trials': n_trials
        }

    def train_and_save_predictions(self, model_type: str, best_params: Dict) -> None:
        """ä½¿ç”¨æœ€å„ªåƒæ•¸è¨“ç·´æ¨¡å‹ä¸¦ä¿å­˜é æ¸¬çµæœ"""
        self.logger.info(f"é–‹å§‹è¨“ç·´ {model_type} ä¸¦ç”Ÿæˆé æ¸¬...")

        try:
            features_df, labels, returns_series = self.load_data()
            
            if len(features_df) == 0 or len(labels) == 0:
                self.logger.error(f"{model_type}: æ•¸æ“šåŠ è¼‰å¤±æ•—")
                return

            # ä½¿ç”¨æœ€å„ªåƒæ•¸é‡å»ºæ¨¡å‹ä¿¡æ¯
            fixed_trial = optuna.trial.FixedTrial(best_params)
            model_info = self.suggest_model_params(fixed_trial, model_type)

            # ä½¿ç”¨Walk-Forwardæ–¹å¼ç”Ÿæˆå…¨é‡é æ¸¬
            total_len = len(features_df)
            n_splits = 5
            split_size = total_len // (n_splits + 1)
            
            all_predictions = []
            all_probabilities = []
            all_indices = []

            for i in range(n_splits):
                train_end = (i + 1) * split_size
                test_start = train_end
                test_end = min(total_len, test_start + split_size)

                if test_end - test_start < 20:
                    continue

                train_idx = list(range(0, train_end))
                test_idx = list(range(test_start, test_end))

                X_train, X_test = features_df.iloc[train_idx], features_df.iloc[test_idx]
                y_train = labels.iloc[train_idx]

                preds, proba = self.fit_and_predict(model_info, X_train, y_train, X_test)

                all_predictions.extend(preds)
                all_probabilities.extend(proba)
                all_indices.extend(features_df.index[test_idx])

            # ä¿å­˜é æ¸¬çµæœ
            predictions_df = pd.DataFrame({
                'prediction': all_predictions,
                'proba_class_0': [p[0] for p in all_probabilities],
                'proba_class_1': [p[1] for p in all_probabilities],
                'proba_class_2': [p[2] for p in all_probabilities]
            }, index=all_indices)

            # æ ¹æ“šæ¨¡å‹é¡å‹ç¢ºå®šæ–‡ä»¶å
            filename_map = {
                'lightgbm': 'lgb_predictions.parquet',
                'xgboost': 'xgb_predictions.parquet',
                'catboost': 'cat_predictions.parquet',
                'randomforest': 'rf_predictions.parquet',
                'extratrees': 'et_predictions.parquet'
            }
            
            output_file = self.results_path / filename_map[model_type]
            predictions_df.to_parquet(output_file)
            
            self.logger.info(f"âœ… {model_type} é æ¸¬å·²ä¿å­˜: {output_file}")
            self.logger.info(f"   é æ¸¬æ¨£æœ¬æ•¸: {len(predictions_df)}")

        except Exception as e:
            self.logger.error(f"{model_type} è¨“ç·´å’Œé æ¸¬å¤±æ•—: {e}")

    def optimize(self, n_trials: int = 50) -> Dict:
        """åŸ·è¡Œæ‰€æœ‰æ¨¡å‹çš„è¶…åƒæ•¸å„ªåŒ–å’Œé æ¸¬ç”Ÿæˆ"""
        self.logger.info("=" * 80)
        self.logger.info("é–‹å§‹å¤šæ¨¡å‹é›†æˆå„ªåŒ–ï¼ˆç¬¬3å±¤ï¼‰...")
        self.logger.info("=" * 80)

        all_results = {}
        
        # æª¢æŸ¥å¯ç”¨çš„æ¨¡å‹
        available_models = self.get_available_models()
        
        if not available_models:
            self.logger.error("æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è«‹æª¢æŸ¥ä¾è³´å®‰è£ã€‚")
            return {}

        self.logger.info(f"å¯ç”¨æ¨¡å‹: {available_models}")

        # ç‚ºæ¯å€‹æ¨¡å‹ç¨ç«‹å„ªåŒ–
        for model_type in available_models:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"è™•ç†æ¨¡å‹: {model_type.upper()}")
            self.logger.info(f"{'='*60}")
            
            # æ­¥é©Ÿ1: å„ªåŒ–è¶…åƒæ•¸
            result = self.optimize_single_model(model_type, n_trials)
            all_results[model_type] = result
            
            # æ­¥é©Ÿ2: è¨“ç·´ä¸¦ä¿å­˜é æ¸¬
            self.train_and_save_predictions(model_type, result['best_params'])

            # æ­¥é©Ÿ3: å³æ™‚å¯«å‡ºç´¯ç©çš„ model_params.jsonï¼Œé¿å…ä¸­é€”åœæ­¢ç„¡ç¸½çµ
            try:
                incremental_path = self.config_path / "model_params.json"
                with open(incremental_path, 'w', encoding='utf-8') as f:
                    json.dump(all_results, f, indent=2, ensure_ascii=False)
                self.logger.info(f"ğŸ“ å·²æ›´æ–°: {incremental_path}ï¼ˆç´¯ç© {len(all_results)} å€‹æ¨¡å‹çµæœï¼‰")
            except Exception as e:
                self.logger.warning(f"âš ï¸ å¯«å‡º model_params.json å¤±æ•—: {e}")

        # ä¿å­˜æ•´åˆçµæœ
        output_file = self.config_path / "model_params.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        self.logger.info("\n" + "=" * 80)
        self.logger.info("âœ… å¤šæ¨¡å‹å„ªåŒ–å®Œæˆ!")
        self.logger.info(f"âœ… çµæœå·²ä¿å­˜è‡³: {output_file}")
        self.logger.info(f"âœ… é æ¸¬æ–‡ä»¶ä¿å­˜åœ¨: {self.results_path}")
        self.logger.info("=" * 80)

        # è¿”å›çµ±ä¸€æ ¼å¼çš„çµæœï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
        best_model = max(all_results.items(), key=lambda x: x[1]['best_score'])
        return {
            'best_params': best_model[1]['best_params'],
            'best_score': best_model[1]['best_score'],
            'best_model_type': best_model[0],
            'all_models': all_results,
            'n_trials': n_trials
        }


def main():
    """ä¸»å‡½æ•¸"""
    optimizer = ModelOptimizer(
        data_path='data',
        config_path='configs',
        results_path='optuna_system/results/BTCUSDT_15m'
    )
    result = optimizer.optimize(n_trials=50)
    print(f"\næœ€ä½³æ¨¡å‹: {result.get('best_model_type', 'N/A')}")
    print(f"æœ€ä½³å¾—åˆ†: {result.get('best_score', 0):.4f}")


if __name__ == "__main__":
    main()
