"""
å¯¹æŠ—æ€§éªŒè¯æ¨¡å— (Adversarial Validation)

ç”¨äºæ£€æµ‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œè¿™æ˜¯ç³»ç»Ÿæ€§åå·®çš„é‡è¦æŒ‡æ ‡ã€‚

åŸºäºå­¦æœ¯æ–‡çŒ®:
- Kaufman, S., Rosset, S., & Perlich, C. (2012): "Leakage in Data Mining: 
  Formulation, Detection, and Avoidance"
- Zhuang, F. et al. (2020): "A Comprehensive Survey on Transfer Learning"

ä½œè€…: Optuna System Team
æ—¥æœŸ: 2025-10-31
"""

import logging
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

logger = logging.getLogger(__name__)


class AdversarialValidator:
    """
    å¯¹æŠ—æ€§éªŒè¯å™¨
    
    é€šè¿‡è®­ç»ƒåˆ†ç±»å™¨åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¯„ä¼°æ•°æ®åˆ†å¸ƒå·®å¼‚ã€‚
    å¦‚æœåˆ†ç±»å™¨èƒ½è½»æ˜“åŒºåˆ†ä¸¤ä¸ªé›†åˆï¼ˆAUCè¿œç¦»0.5ï¼‰ï¼Œè¯´æ˜å­˜åœ¨åˆ†å¸ƒåç§»ã€‚
    
    å­¦æœ¯ä¾æ®:
    - AUC < 0.55: åˆ†å¸ƒç›¸ä¼¼ï¼Œä½é£é™©
    - 0.55 <= AUC < 0.65: è½»åº¦åç§»ï¼Œä¸­ç­‰é£é™©
    - 0.65 <= AUC < 0.75: ä¸­åº¦åç§»ï¼Œé«˜é£é™©
    - AUC >= 0.75: ä¸¥é‡åç§»ï¼Œæé«˜é£é™©
    """
    
    def __init__(self, 
                 model_type: str = 'rf',
                 n_cv_splits: int = 5,
                 random_state: int = 42):
        """
        åˆå§‹åŒ–å¯¹æŠ—æ€§éªŒè¯å™¨
        
        Args:
            model_type: åˆ†ç±»å™¨ç±»å‹ ('rf', 'gb', 'lr')
            n_cv_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­
        """
        self.model_type = model_type
        self.n_cv_splits = n_cv_splits
        self.random_state = random_state
        self.logger = logger
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨
        self.model = self._get_model()
        self.scaler = StandardScaler()
        
        # å­˜å‚¨ç»“æœ
        self.results_ = None
        self.feature_importances_ = None
    
    def _get_model(self):
        """è·å–åˆ†ç±»å™¨"""
        if self.model_type == 'rf':
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=self.random_state,
                n_jobs=-1
            )
        elif self.model_type == 'gb':
            return GradientBoostingClassifier(
                n_estimators=100,
                max_depth=3,
                learning_rate=0.1,
                random_state=self.random_state
            )
        elif self.model_type == 'lr':
            return LogisticRegression(
                max_iter=1000,
                random_state=self.random_state,
                n_jobs=-1
            )
        else:
            raise ValueError(f"Unknown model_type: {self.model_type}")
    
    def validate(self,
                 train_features: pd.DataFrame,
                 test_features: pd.DataFrame,
                 sample_weight: Optional[np.ndarray] = None) -> Dict:
        """
        æ‰§è¡Œå¯¹æŠ—æ€§éªŒè¯
        
        Args:
            train_features: è®­ç»ƒé›†ç‰¹å¾
            test_features: æµ‹è¯•é›†ç‰¹å¾
            sample_weight: æ ·æœ¬æƒé‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            DictåŒ…å«:
                - train_test_auc: è®­ç»ƒ/æµ‹è¯•AUC
                - cv_auc_mean: äº¤å‰éªŒè¯å¹³å‡AUC
                - cv_auc_std: äº¤å‰éªŒè¯AUCæ ‡å‡†å·®
                - distribution_shift: åˆ†å¸ƒåç§»ç¨‹åº¦
                - overfitting_risk: è¿‡æ‹Ÿåˆé£é™©ç­‰çº§
                - top_discriminative_features: æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ” æ‰§è¡Œå¯¹æŠ—æ€§éªŒè¯...")
        
        # æ•°æ®å‡†å¤‡
        X_train = train_features.copy()
        X_test = test_features.copy()
        
        # ç¡®ä¿ç‰¹å¾ä¸€è‡´
        common_features = list(set(X_train.columns) & set(X_test.columns))
        if len(common_features) < len(X_train.columns):
            self.logger.warning(f"âš ï¸ ç‰¹å¾ä¸ä¸€è‡´: trainæœ‰{len(X_train.columns)}ä¸ª, testæœ‰{len(X_test.columns)}ä¸ª, å…±åŒ{len(common_features)}ä¸ª")
        
        X_train = X_train[common_features]
        X_test = X_test[common_features]
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆ0=train, 1=testï¼‰
        y_train = np.zeros(len(X_train))
        y_test = np.ones(len(X_test))
        
        # åˆå¹¶æ•°æ®
        X_combined = pd.concat([X_train, X_test], axis=0, ignore_index=True)
        y_combined = np.concatenate([y_train, y_test])
        
        # æ ‡å‡†åŒ–ï¼ˆå¯¹äºé€»è¾‘å›å½’å¾ˆé‡è¦ï¼‰
        if self.model_type == 'lr':
            X_combined_scaled = self.scaler.fit_transform(X_combined)
            X_combined = pd.DataFrame(
                X_combined_scaled,
                columns=X_combined.columns,
                index=X_combined.index
            )
        
        # äº¤å‰éªŒè¯
        self.logger.info(f"  ğŸ“Š å¼€å§‹{self.n_cv_splits}æŠ˜äº¤å‰éªŒè¯...")
        cv_aucs = []
        cv_accs = []
        
        skf = StratifiedKFold(
            n_splits=self.n_cv_splits,
            shuffle=True,
            random_state=self.random_state
        )
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_combined, y_combined)):
            X_fold_train = X_combined.iloc[train_idx]
            y_fold_train = y_combined[train_idx]
            X_fold_val = X_combined.iloc[val_idx]
            y_fold_val = y_combined[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            model = self._get_model()
            model.fit(X_fold_train, y_fold_train)
            
            # é¢„æµ‹
            y_pred_proba = model.predict_proba(X_fold_val)[:, 1]
            y_pred = model.predict(X_fold_val)
            
            # è¯„ä¼°
            fold_auc = roc_auc_score(y_fold_val, y_pred_proba)
            fold_acc = accuracy_score(y_fold_val, y_pred)
            
            cv_aucs.append(fold_auc)
            cv_accs.append(fold_acc)
            
            self.logger.debug(f"    Fold {fold+1}: AUC={fold_auc:.4f}, Acc={fold_acc:.4f}")
        
        cv_auc_mean = np.mean(cv_aucs)
        cv_auc_std = np.std(cv_aucs)
        
        # åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒï¼ˆç”¨äºç‰¹å¾é‡è¦æ€§ï¼‰
        self.logger.info("  ğŸ“ˆ åœ¨å…¨éƒ¨æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        self.model.fit(X_combined, y_combined)
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importances_ = pd.Series(
                self.model.feature_importances_,
                index=common_features
            ).sort_values(ascending=False)
            
            top_features = self.feature_importances_.head(10)
            self.logger.info(f"  ğŸ” Top 10åŒºåˆ†æ€§ç‰¹å¾:")
            for feat, imp in top_features.items():
                self.logger.info(f"     {feat}: {imp:.4f}")
        
        # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ï¼ˆè¯Šæ–­ç”¨ï¼‰
        y_pred_proba = self.model.predict_proba(X_combined)[:, 1]
        train_test_auc = roc_auc_score(y_combined, y_pred_proba)
        
        # è¯„ä¼°åˆ†å¸ƒåç§»
        distribution_shift = self._assess_distribution_shift(cv_auc_mean)
        overfitting_risk = self._assess_overfitting_risk(cv_auc_mean, train_test_auc)
        
        # å­˜å‚¨ç»“æœ
        self.results_ = {
            'train_test_auc': train_test_auc,
            'cv_auc_mean': cv_auc_mean,
            'cv_auc_std': cv_auc_std,
            'cv_aucs': cv_aucs,
            'distribution_shift': distribution_shift,
            'overfitting_risk': overfitting_risk,
            'n_train_samples': len(X_train),
            'n_test_samples': len(X_test),
            'n_features': len(common_features),
            'top_discriminative_features': top_features.to_dict() if hasattr(self.model, 'feature_importances_') else {}
        }
        
        # æ‰“å°æ€»ç»“
        self._print_summary()
        
        return self.results_
    
    def _assess_distribution_shift(self, auc: float) -> str:
        """è¯„ä¼°åˆ†å¸ƒåç§»ç¨‹åº¦"""
        if auc < 0.55:
            return 'minimal'
        elif auc < 0.65:
            return 'mild'
        elif auc < 0.75:
            return 'moderate'
        else:
            return 'severe'
    
    def _assess_overfitting_risk(self, cv_auc: float, train_test_auc: float) -> str:
        """è¯„ä¼°è¿‡æ‹Ÿåˆé£é™©"""
        if cv_auc < 0.55:
            return 'low'
        elif cv_auc < 0.65:
            return 'medium'
        elif cv_auc < 0.75:
            return 'high'
        else:
            return 'very_high'
    
    def _print_summary(self):
        """æ‰“å°éªŒè¯æ€»ç»“"""
        if self.results_ is None:
            return
        
        r = self.results_
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š å¯¹æŠ—æ€§éªŒè¯ç»“æœ:")
        self.logger.info(f"  ğŸ¯ äº¤å‰éªŒè¯AUC: {r['cv_auc_mean']:.4f} Â± {r['cv_auc_std']:.4f}")
        self.logger.info(f"  ğŸ“ˆ è®­ç»ƒ/æµ‹è¯•AUC: {r['train_test_auc']:.4f}")
        self.logger.info(f"  ğŸ“ è®­ç»ƒæ ·æœ¬: {r['n_train_samples']:,}")
        self.logger.info(f"  ğŸ“ æµ‹è¯•æ ·æœ¬: {r['n_test_samples']:,}")
        self.logger.info(f"  ğŸ”¢ ç‰¹å¾æ•°é‡: {r['n_features']}")
        
        # åˆ†å¸ƒåç§»è¯„ä¼°
        shift = r['distribution_shift']
        if shift == 'minimal':
            self.logger.info(f"  âœ… åˆ†å¸ƒåç§»: {shift.upper()} (AUC < 0.55)")
            self.logger.info("     ä½é£é™©ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒç›¸ä¼¼")
        elif shift == 'mild':
            self.logger.info(f"  âš ï¸  åˆ†å¸ƒåç§»: {shift.upper()} (0.55 <= AUC < 0.65)")
            self.logger.info("     ä¸­ç­‰é£é™©ï¼Œå­˜åœ¨è½»å¾®åˆ†å¸ƒå·®å¼‚")
        elif shift == 'moderate':
            self.logger.info(f"  âš ï¸  åˆ†å¸ƒåç§»: {shift.upper()} (0.65 <= AUC < 0.75)")
            self.logger.info("     é«˜é£é™©ï¼Œåˆ†å¸ƒå·®å¼‚æ˜¾è‘—")
        else:  # severe
            self.logger.info(f"  ğŸš¨ åˆ†å¸ƒåç§»: {shift.upper()} (AUC >= 0.75)")
            self.logger.info("     æé«˜é£é™©ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒå·®å¼‚å·¨å¤§")
        
        # è¿‡æ‹Ÿåˆé£é™©
        risk = r['overfitting_risk']
        if risk == 'low':
            self.logger.info(f"  âœ… è¿‡æ‹Ÿåˆé£é™©: {risk.upper()}")
        elif risk == 'medium':
            self.logger.info(f"  âš ï¸  è¿‡æ‹Ÿåˆé£é™©: {risk.upper()}")
        elif risk == 'high':
            self.logger.info(f"  ğŸš¨ è¿‡æ‹Ÿåˆé£é™©: {risk.upper()}")
        else:  # very_high
            self.logger.info(f"  ğŸš¨ è¿‡æ‹Ÿåˆé£é™©: {risk.upper()}")
            self.logger.info("     å¼ºçƒˆå»ºè®®é‡æ–°åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†æˆ–æ”¶é›†æ›´å¤šæ•°æ®")
        
        self.logger.info("=" * 60)
    
    def get_feature_importance_report(self, top_n: int = 20) -> pd.DataFrame:
        """
        è·å–ç‰¹å¾é‡è¦æ€§æŠ¥å‘Š
        
        Args:
            top_n: è¿”å›å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾
            
        Returns:
            DataFrameåŒ…å«ç‰¹å¾åå’Œé‡è¦æ€§åˆ†æ•°
        """
        if self.feature_importances_ is None:
            self.logger.warning("âš ï¸ ç‰¹å¾é‡è¦æ€§æœªè®¡ç®—ï¼Œè¯·å…ˆè¿è¡Œvalidate()")
            return pd.DataFrame()
        
        report = pd.DataFrame({
            'feature': self.feature_importances_.head(top_n).index,
            'importance': self.feature_importances_.head(top_n).values
        })
        
        # æ·»åŠ ç´¯ç§¯é‡è¦æ€§
        report['cumulative_importance'] = report['importance'].cumsum() / report['importance'].sum()
        
        return report


def quick_adversarial_check(train_features: pd.DataFrame,
                            test_features: pd.DataFrame,
                            model_type: str = 'rf') -> Dict:
    """
    å¿«é€Ÿå¯¹æŠ—æ€§éªŒè¯æ£€æŸ¥
    
    ä¾¿æ·å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿè¯„ä¼°è®­ç»ƒ/æµ‹è¯•åˆ†å¸ƒå·®å¼‚ã€‚
    
    Args:
        train_features: è®­ç»ƒé›†ç‰¹å¾
        test_features: æµ‹è¯•é›†ç‰¹å¾
        model_type: åˆ†ç±»å™¨ç±»å‹
        
    Returns:
        éªŒè¯ç»“æœå­—å…¸
        
    Example:
        >>> result = quick_adversarial_check(X_train, X_test)
        >>> if result['cv_auc_mean'] > 0.65:
        >>>     print("âš ï¸ è­¦å‘Š: å­˜åœ¨æ˜¾è‘—çš„åˆ†å¸ƒåç§»!")
    """
    validator = AdversarialValidator(model_type=model_type)
    result = validator.validate(train_features, test_features)
    return result


def detect_covariate_shift(features_by_time: List[pd.DataFrame],
                           window_size: int = 5) -> List[Dict]:
    """
    æ£€æµ‹æ—¶é—´åºåˆ—ä¸­çš„åå˜é‡åç§»ï¼ˆCovariate Shiftï¼‰
    
    ä½¿ç”¨æ»‘åŠ¨çª—å£æ¯”è¾ƒç›¸é‚»æ—¶é—´æ®µçš„ç‰¹å¾åˆ†å¸ƒã€‚
    
    Args:
        features_by_time: æŒ‰æ—¶é—´æ’åºçš„ç‰¹å¾DataFrameåˆ—è¡¨
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆæ—¶é—´æ®µæ•°ï¼‰
        
    Returns:
        æ¯ä¸ªçª—å£å¯¹çš„éªŒè¯ç»“æœåˆ—è¡¨
        
    Example:
        >>> # å‡è®¾æœ‰æŒ‰æœˆä»½åˆ†ç»„çš„ç‰¹å¾æ•°æ®
        >>> monthly_features = [df_jan, df_feb, df_mar, ...]
        >>> shifts = detect_covariate_shift(monthly_features, window_size=3)
        >>> for i, shift in enumerate(shifts):
        >>>     if shift['cv_auc_mean'] > 0.6:
        >>>         print(f"æ—¶é—´æ®µ {i} åˆ° {i+3} å­˜åœ¨åˆ†å¸ƒåç§»")
    """
    if len(features_by_time) < window_size + 1:
        raise ValueError(f"éœ€è¦è‡³å°‘{window_size + 1}ä¸ªæ—¶é—´æ®µï¼Œä½†åªæä¾›äº†{len(features_by_time)}ä¸ª")
    
    results = []
    validator = AdversarialValidator(model_type='rf', n_cv_splits=3)
    
    for i in range(len(features_by_time) - window_size):
        # å‰çª—å£ä½œä¸º"è®­ç»ƒé›†"
        train_window = pd.concat(
            features_by_time[i:i+window_size],
            axis=0,
            ignore_index=True
        )
        
        # åçª—å£ä½œä¸º"æµ‹è¯•é›†"
        test_window = features_by_time[i + window_size]
        
        logger.info(f"æ£€æµ‹æ—¶é—´æ®µ {i} åˆ° {i+window_size} çš„åˆ†å¸ƒåç§»...")
        
        result = validator.validate(train_window, test_window)
        result['time_window_start'] = i
        result['time_window_end'] = i + window_size
        
        results.append(result)
    
    return results


if __name__ == '__main__':
    # ç®€å•æµ‹è¯•
    print("å¯¹æŠ—æ€§éªŒè¯æ¨¡å—æµ‹è¯•")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 20
    
    # è®­ç»ƒé›†ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰
    X_train = pd.DataFrame(
        np.random.normal(0, 1, (n_samples, n_features)),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # æµ‹è¯•é›†ï¼ˆè½»å¾®åç§»ï¼‰
    X_test = pd.DataFrame(
        np.random.normal(0.2, 1.1, (n_samples // 2, n_features)),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # æ‰§è¡ŒéªŒè¯
    result = quick_adversarial_check(X_train, X_test)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ")
    print(f"   AUC: {result['cv_auc_mean']:.4f}")
    print(f"   åˆ†å¸ƒåç§»: {result['distribution_shift']}")

