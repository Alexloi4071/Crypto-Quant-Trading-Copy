"""
æ¨¡å‹é›†æˆæ¡†æ¶ (Model Ensemble)

å®ç°å¤šç§æ¨¡å‹èŒƒå¼çš„é›†æˆï¼Œæé«˜é¢„æµ‹ç¨³å¥æ€§å¹¶é™ä½å•ä¸€æ¨¡å‹çš„ç³»ç»Ÿæ€§åå·®ã€‚

é›†æˆæ–¹æ³•:
1. Votingï¼ˆæŠ•ç¥¨é›†æˆï¼‰
2. Stackingï¼ˆå †å é›†æˆï¼‰
3. Blendingï¼ˆæ··åˆé›†æˆï¼‰
4. Diversity-based Ensembleï¼ˆå¤šæ ·æ€§é›†æˆï¼‰

åŸºäºå­¦æœ¯æ–‡çŒ®:
- Wolpert, D. H. (1992): "Stacked Generalization"
- Breiman, L. (1996): "Bagging Predictors"
- Dietterich, T. G. (2000): "Ensemble Methods in Machine Learning"
- Rokach, L. (2010): "Ensemble-based Classifiers"

ä½œè€…: Optuna System Team
æ—¥æœŸ: 2025-10-31
"""

import logging
from typing import Dict, List, Optional, Union, Tuple, Any
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_predict, StratifiedKFold
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    VotingClassifier,
    StackingClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    roc_auc_score,
    classification_report
)

try:
    from lightgbm import LGBMClassifier
    HAS_LGBM = True
except ImportError:
    HAS_LGBM = False

try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    from catboost import CatBoostClassifier
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False

logger = logging.getLogger(__name__)


class DiverseModelEnsemble:
    """
    å¤šæ ·åŒ–æ¨¡å‹é›†æˆå™¨
    
    é›†æˆä¸åŒèŒƒå¼çš„æ¨¡å‹:
    - æ ‘æ¨¡å‹: Random Forest, LightGBM, XGBoost, CatBoost
    - çº¿æ€§æ¨¡å‹: Logistic Regression
    - éçº¿æ€§æ¨¡å‹: SVM
    
    ç›®çš„: é™ä½å•ä¸€æ¨¡å‹çš„åå·®ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self,
                 ensemble_method: str = 'stacking',
                 use_tree_models: bool = True,
                 use_linear_models: bool = True,
                 use_svm: bool = False,
                 n_cv_folds: int = 5,
                 random_state: int = 42):
        """
        åˆå§‹åŒ–é›†æˆå™¨
        
        Args:
            ensemble_method: é›†æˆæ–¹æ³• ('voting', 'stacking', 'blending')
            use_tree_models: æ˜¯å¦ä½¿ç”¨æ ‘æ¨¡å‹
            use_linear_models: æ˜¯å¦ä½¿ç”¨çº¿æ€§æ¨¡å‹
            use_svm: æ˜¯å¦ä½¿ç”¨SVMï¼ˆè®¡ç®—æ…¢ï¼‰
            n_cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­
        """
        self.ensemble_method = ensemble_method
        self.use_tree_models = use_tree_models
        self.use_linear_models = use_linear_models
        self.use_svm = use_svm
        self.n_cv_folds = n_cv_folds
        self.random_state = random_state
        self.logger = logger
        
        # å­˜å‚¨æ¨¡å‹
        self.base_models_ = None
        self.ensemble_model_ = None
        self.base_predictions_ = None
        self.diversity_scores_ = None
    
    def _create_base_models(self) -> List[Tuple[str, Any]]:
        """åˆ›å»ºåŸºç¡€æ¨¡å‹åˆ—è¡¨"""
        models = []
        
        # æ ‘æ¨¡å‹
        if self.use_tree_models:
            # Random Forest
            models.append((
                'rf',
                RandomForestClassifier(
                    n_estimators=100,
                    max_depth=8,
                    min_samples_split=10,
                    min_samples_leaf=4,
                    random_state=self.random_state,
                    n_jobs=-1
                )
            ))
            
            # LightGBM
            if HAS_LGBM:
                models.append((
                    'lgbm',
                    LGBMClassifier(
                        n_estimators=100,
                        max_depth=8,
                        learning_rate=0.05,
                        num_leaves=31,
                        subsample=0.8,
                        colsample_bytree=0.8,
                        random_state=self.random_state,
                        verbose=-1
                    )
                ))
            
            # XGBoost
            if HAS_XGB:
                models.append((
                    'xgb',
                    XGBClassifier(
                        n_estimators=100,
                        max_depth=8,
                        learning_rate=0.05,
                        subsample=0.8,
                        colsample_bytree=0.8,
                        random_state=self.random_state,
                        verbosity=0
                    )
                ))
            
            # CatBoost
            if HAS_CATBOOST:
                models.append((
                    'catboost',
                    CatBoostClassifier(
                        iterations=100,
                        depth=6,
                        learning_rate=0.05,
                        random_state=self.random_state,
                        verbose=False
                    )
                ))
            
            # Gradient Boosting
            models.append((
                'gb',
                GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=5,
                    learning_rate=0.05,
                    subsample=0.8,
                    random_state=self.random_state
                )
            ))
        
        # çº¿æ€§æ¨¡å‹
        if self.use_linear_models:
            models.append((
                'lr',
                LogisticRegression(
                    max_iter=1000,
                    random_state=self.random_state,
                    n_jobs=-1
                )
            ))
        
        # SVM
        if self.use_svm:
            models.append((
                'svm',
                SVC(
                    kernel='rbf',
                    probability=True,
                    random_state=self.random_state
                )
            ))
        
        self.logger.info(f"  ğŸ¤– åˆ›å»ºäº†{len(models)}ä¸ªåŸºç¡€æ¨¡å‹: {[name for name, _ in models]}")
        
        return models
    
    def fit(self,
            X_train: pd.DataFrame,
            y_train: pd.Series,
            X_val: Optional[pd.DataFrame] = None,
            y_val: Optional[pd.Series] = None) -> 'DiverseModelEnsemble':
        """
        è®­ç»ƒé›†æˆæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_val: éªŒè¯ç‰¹å¾ï¼ˆBlendingéœ€è¦ï¼‰
            y_val: éªŒè¯æ ‡ç­¾ï¼ˆBlendingéœ€è¦ï¼‰
            
        Returns:
            self
        """
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸš€ è®­ç»ƒ{self.ensemble_method.upper()}é›†æˆæ¨¡å‹...")
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹
        self.base_models_ = self._create_base_models()
        
        if self.ensemble_method == 'voting':
            self._fit_voting(X_train, y_train)
        
        elif self.ensemble_method == 'stacking':
            self._fit_stacking(X_train, y_train)
        
        elif self.ensemble_method == 'blending':
            if X_val is None or y_val is None:
                raise ValueError("Blendingéœ€è¦æä¾›éªŒè¯é›†")
            self._fit_blending(X_train, y_train, X_val, y_val)
        
        else:
            raise ValueError(f"Unknown ensemble_method: {self.ensemble_method}")
        
        self.logger.info("  âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return self
    
    def _fit_voting(self, X_train: pd.DataFrame, y_train: pd.Series):
        """è®­ç»ƒæŠ•ç¥¨é›†æˆ"""
        self.logger.info("  ğŸ“Š ä½¿ç”¨Votingé›†æˆï¼ˆè½¯æŠ•ç¥¨ï¼‰...")
        
        self.ensemble_model_ = VotingClassifier(
            estimators=self.base_models_,
            voting='soft',
            n_jobs=-1
        )
        
        self.ensemble_model_.fit(X_train, y_train)
    
    def _fit_stacking(self, X_train: pd.DataFrame, y_train: pd.Series):
        """è®­ç»ƒå †å é›†æˆ"""
        self.logger.info("  ğŸ“Š ä½¿ç”¨Stackingé›†æˆï¼ˆå…ƒå­¦ä¹ å™¨: Logistic Regressionï¼‰...")
        
        self.ensemble_model_ = StackingClassifier(
            estimators=self.base_models_,
            final_estimator=LogisticRegression(max_iter=1000, random_state=self.random_state),
            cv=self.n_cv_folds,
            n_jobs=-1
        )
        
        self.ensemble_model_.fit(X_train, y_train)
    
    def _fit_blending(self,
                     X_train: pd.DataFrame,
                     y_train: pd.Series,
                     X_val: pd.DataFrame,
                     y_val: pd.Series):
        """è®­ç»ƒæ··åˆé›†æˆ"""
        self.logger.info("  ğŸ“Š ä½¿ç”¨Blendingé›†æˆ...")
        
        # 1. åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹
        trained_models = []
        val_predictions = []
        
        for name, model in self.base_models_:
            self.logger.info(f"     è®­ç»ƒ {name}...")
            model.fit(X_train, y_train)
            trained_models.append((name, model))
            
            # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
            val_pred = model.predict_proba(X_val)
            val_predictions.append(val_pred)
        
        # 2. ä½¿ç”¨éªŒè¯é›†é¢„æµ‹è®­ç»ƒå…ƒå­¦ä¹ å™¨
        val_predictions_stacked = np.hstack(val_predictions)
        
        meta_learner = LogisticRegression(max_iter=1000, random_state=self.random_state)
        meta_learner.fit(val_predictions_stacked, y_val)
        
        self.ensemble_model_ = {
            'base_models': trained_models,
            'meta_learner': meta_learner
        }
    
    def predict(self, X_test: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        if self.ensemble_method == 'blending':
            # Blendingéœ€è¦ç‰¹æ®Šå¤„ç†
            base_predictions = []
            for name, model in self.ensemble_model_['base_models']:
                pred = model.predict_proba(X_test)
                base_predictions.append(pred)
            
            stacked_predictions = np.hstack(base_predictions)
            return self.ensemble_model_['meta_learner'].predict(stacked_predictions)
        
        else:
            return self.ensemble_model_.predict(X_test)
    
    def predict_proba(self, X_test: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        if self.ensemble_method == 'blending':
            base_predictions = []
            for name, model in self.ensemble_model_['base_models']:
                pred = model.predict_proba(X_test)
                base_predictions.append(pred)
            
            stacked_predictions = np.hstack(base_predictions)
            return self.ensemble_model_['meta_learner'].predict_proba(stacked_predictions)
        
        else:
            return self.ensemble_model_.predict_proba(X_test)
    
    def evaluate(self,
                X_test: pd.DataFrame,
                y_test: pd.Series,
                metrics: Optional[List[str]] = None) -> Dict:
        """
        è¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½
        
        Args:
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š è¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹
        y_pred = self.predict(X_test)
        y_pred_proba = self.predict_proba(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        if metrics is None:
            metrics = ['accuracy', 'f1_macro', 'roc_auc']
        
        results = {}
        
        if 'accuracy' in metrics:
            results['accuracy'] = accuracy_score(y_test, y_pred)
        
        if 'f1_macro' in metrics:
            results['f1_macro'] = f1_score(y_test, y_pred, average='macro')
        
        if 'roc_auc' in metrics:
            if y_pred_proba.shape[1] == 2:
                results['roc_auc'] = roc_auc_score(y_test, y_pred_proba[:, 1])
            else:
                results['roc_auc'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
        
        # æ‰“å°ç»“æœ
        self.logger.info("  ğŸ¯ é›†æˆæ¨¡å‹æ€§èƒ½:")
        for metric, value in results.items():
            self.logger.info(f"     {metric}: {value:.4f}")
        
        # è¯„ä¼°åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        results['base_models'] = self._evaluate_base_models(X_test, y_test)
        
        # è®¡ç®—å¤šæ ·æ€§
        results['diversity'] = self._calculate_diversity(X_test, y_test)
        
        return results
    
    def _evaluate_base_models(self,
                             X_test: pd.DataFrame,
                             y_test: pd.Series) -> Dict:
        """è¯„ä¼°å„ä¸ªåŸºç¡€æ¨¡å‹"""
        self.logger.info("  ğŸ“Š è¯„ä¼°åŸºç¡€æ¨¡å‹...")
        
        base_results = {}
        
        if self.ensemble_method == 'blending':
            models = self.ensemble_model_['base_models']
        else:
            # Voting/Stackingçš„åŸºç¡€æ¨¡å‹å·²è®­ç»ƒ
            models = [(name, estimator) for name, estimator in self.ensemble_model_.named_estimators_.items()]
        
        for name, model in models:
            y_pred = model.predict(X_test)
            
            base_results[name] = {
                'accuracy': accuracy_score(y_test, y_pred),
                'f1_macro': f1_score(y_test, y_pred, average='macro')
            }
            
            self.logger.info(f"     {name}: F1={base_results[name]['f1_macro']:.4f}, Acc={base_results[name]['accuracy']:.4f}")
        
        return base_results
    
    def _calculate_diversity(self,
                            X_test: pd.DataFrame,
                            y_test: pd.Series) -> Dict:
        """
        è®¡ç®—æ¨¡å‹å¤šæ ·æ€§
        
        ä½¿ç”¨Qç»Ÿè®¡é‡å’Œä¸ä¸€è‡´åº¦é‡
        """
        self.logger.info("  ğŸ” è®¡ç®—æ¨¡å‹å¤šæ ·æ€§...")
        
        # è·å–å„ä¸ªæ¨¡å‹çš„é¢„æµ‹
        if self.ensemble_method == 'blending':
            models = self.ensemble_model_['base_models']
        else:
            models = [(name, estimator) for name, estimator in self.ensemble_model_.named_estimators_.items()]
        
        predictions = {}
        for name, model in models:
            predictions[name] = model.predict(X_test)
        
        # è®¡ç®—Qç»Ÿè®¡é‡ï¼ˆä¸¤ä¸¤æ¨¡å‹ä¹‹é—´ï¼‰
        model_names = list(predictions.keys())
        n_models = len(model_names)
        
        q_statistics = []
        disagreement_counts = []
        
        for i in range(n_models):
            for j in range(i+1, n_models):
                name_i = model_names[i]
                name_j = model_names[j]
                
                pred_i = predictions[name_i]
                pred_j = predictions[name_j]
                
                # Qç»Ÿè®¡é‡
                n11 = np.sum((pred_i == y_test) & (pred_j == y_test))
                n00 = np.sum((pred_i != y_test) & (pred_j != y_test))
                n10 = np.sum((pred_i == y_test) & (pred_j != y_test))
                n01 = np.sum((pred_i != y_test) & (pred_j == y_test))
                
                if (n11 * n00 + n10 * n01) > 0:
                    q = (n11 * n00 - n10 * n01) / (n11 * n00 + n10 * n01)
                else:
                    q = 0
                
                q_statistics.append(q)
                
                # ä¸ä¸€è‡´åº¦
                disagreement = np.sum(pred_i != pred_j) / len(y_test)
                disagreement_counts.append(disagreement)
        
        avg_q = np.mean(q_statistics) if q_statistics else 0
        avg_disagreement = np.mean(disagreement_counts) if disagreement_counts else 0
        
        self.logger.info(f"     å¹³å‡Qç»Ÿè®¡é‡: {avg_q:.4f} (è¶Šå°è¶Šå¥½ï¼Œ<0è¡¨ç¤ºé«˜å¤šæ ·æ€§)")
        self.logger.info(f"     å¹³å‡ä¸ä¸€è‡´åº¦: {avg_disagreement:.4f} (è¶Šå¤§è¶Šå¥½)")
        
        diversity_assessment = 'high' if avg_q < 0.2 and avg_disagreement > 0.3 else 'medium' if avg_q < 0.5 else 'low'
        
        self.logger.info(f"     å¤šæ ·æ€§è¯„ä¼°: {diversity_assessment.upper()}")
        
        return {
            'avg_q_statistic': avg_q,
            'avg_disagreement': avg_disagreement,
            'diversity_assessment': diversity_assessment,
            'q_statistics': q_statistics,
            'disagreement_counts': disagreement_counts
        }
    
    def get_model_weights(self) -> Optional[Dict]:
        """è·å–æ¨¡å‹æƒé‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰"""
        if self.ensemble_method == 'stacking':
            # Stackingçš„å…ƒå­¦ä¹ å™¨æƒé‡
            if hasattr(self.ensemble_model_.final_estimator_, 'coef_'):
                weights = self.ensemble_model_.final_estimator_.coef_[0]
                model_names = [name for name, _ in self.base_models_]
                
                weight_dict = {name: float(weight) for name, weight in zip(model_names, weights)}
                
                self.logger.info("  âš–ï¸  Stackingæ¨¡å‹æƒé‡:")
                for name, weight in weight_dict.items():
                    self.logger.info(f"     {name}: {weight:.4f}")
                
                return weight_dict
        
        return None


if __name__ == '__main__':
    # ç®€å•æµ‹è¯•
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    
    print("æ¨¡å‹é›†æˆæ¨¡å—æµ‹è¯•")
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=10,
        random_state=42
    )
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
    y = pd.Series(y)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # åˆ›å»ºé›†æˆå™¨
    ensemble = DiverseModelEnsemble(
        ensemble_method='stacking',
        use_tree_models=True,
        use_linear_models=True,
        use_svm=False
    )
    
    # è®­ç»ƒ
    ensemble.fit(X_train, y_train)
    
    # è¯„ä¼°
    results = ensemble.evaluate(X_test, y_test)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ")
    print(f"   é›†æˆF1: {results['f1_macro']:.4f}")
    print(f"   å¤šæ ·æ€§: {results['diversity']['diversity_assessment']}")

