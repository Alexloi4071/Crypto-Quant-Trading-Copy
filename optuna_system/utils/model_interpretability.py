"""
æ¨¡å‹å¯è§£é‡Šæ€§æ¡†æ¶

é›†æˆSHAPã€LIMEå’Œæ’åˆ—é‡è¦æ€§ç­‰å¤šç§å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œ
ç”¨äºç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹å’Œæ£€æµ‹æ½œåœ¨åå·®ã€‚

åŸºäºå­¦æœ¯æ–‡çŒ®:
- Lundberg, S. M., & Lee, S. I. (2017): "A Unified Approach to Interpreting Model Predictions" (SHAP)
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016): "Why Should I Trust You?" (LIME)
- Breiman, L. (2001): "Random Forests" (Permutation Importance)
- Molnar, C. (2020): "Interpretable Machine Learning"

ä½œè€…: Optuna System Team
æ—¥æœŸ: 2025-10-31
"""

import logging
from typing import Dict, List, Optional, Union, Tuple, Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance
from sklearn.base import BaseEstimator

logger = logging.getLogger(__name__)

# å¯é€‰ä¾èµ–
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False
    logger.warning("âš ï¸ SHAPæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å®‰è£…: pip install shap")

try:
    from lime import lime_tabular
    HAS_LIME = True
except ImportError:
    HAS_LIME = False
    logger.warning("âš ï¸ LIMEæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å®‰è£…: pip install lime")


class ModelInterpreter:
    """
    æ¨¡å‹å¯è§£é‡Šæ€§è§£é‡Šå™¨
    
    æ”¯æŒå¤šç§è§£é‡Šæ–¹æ³•:
    1. SHAP (SHapley Additive exPlanations)
    2. LIME (Local Interpretable Model-agnostic Explanations)
    3. Permutation Importance
    4. Feature Interaction Detection
    """
    
    def __init__(self,
                 model: BaseEstimator,
                 X_train: pd.DataFrame,
                 feature_names: Optional[List[str]] = None,
                 random_state: int = 42):
        """
        åˆå§‹åŒ–è§£é‡Šå™¨
        
        Args:
            model: å·²è®­ç»ƒçš„æ¨¡å‹
            X_train: è®­ç»ƒæ•°æ®ï¼ˆç”¨äºå»ºç«‹åŸºçº¿ï¼‰
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            random_state: éšæœºç§å­
        """
        self.model = model
        self.X_train = X_train
        self.feature_names = feature_names if feature_names is not None else X_train.columns.tolist()
        self.random_state = random_state
        self.logger = logger
        
        # å­˜å‚¨è§£é‡Šç»“æœ
        self.shap_values_ = None
        self.shap_explainer_ = None
        self.lime_explainer_ = None
        self.perm_importance_ = None
    
    def explain_with_shap(self,
                         X_test: pd.DataFrame,
                         method: str = 'auto',
                         max_samples: int = 100) -> Dict:
        """
        ä½¿ç”¨SHAPè§£é‡Šæ¨¡å‹é¢„æµ‹
        
        Args:
            X_test: æµ‹è¯•æ•°æ®
            method: SHAPæ–¹æ³• ('auto', 'tree', 'kernel', 'deep', 'linear')
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆé¿å…è®¡ç®—å¤ªæ…¢ï¼‰
            
        Returns:
            DictåŒ…å«SHAPå€¼å’Œæ€»ç»“
        """
        if not HAS_SHAP:
            self.logger.error("âŒ SHAPæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½")
            return {}
        
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ” ä½¿ç”¨SHAPè§£é‡Šæ¨¡å‹ (method={method})...")
        
        # é™åˆ¶æ ·æœ¬æ•°
        if len(X_test) > max_samples:
            self.logger.info(f"  ğŸ“Š æ ·æœ¬æ•°è¿‡å¤šï¼Œéšæœºé‡‡æ ·{max_samples}ä¸ª")
            X_test_sample = X_test.sample(n=max_samples, random_state=self.random_state)
        else:
            X_test_sample = X_test
        
        # é€‰æ‹©SHAP explainer
        if method == 'auto':
            # è‡ªåŠ¨é€‰æ‹©ï¼ˆåŸºäºæ¨¡å‹ç±»å‹ï¼‰
            if hasattr(self.model, 'tree_'):
                method = 'tree'
            elif hasattr(self.model, 'coef_'):
                method = 'linear'
            else:
                method = 'kernel'
            self.logger.info(f"  ğŸ¤– è‡ªåŠ¨é€‰æ‹©SHAPæ–¹æ³•: {method}")
        
        try:
            if method == 'tree':
                # TreeExplainerï¼ˆé€‚ç”¨äºæ ‘æ¨¡å‹ï¼‰
                self.shap_explainer_ = shap.TreeExplainer(self.model)
                self.shap_values_ = self.shap_explainer_.shap_values(X_test_sample)
            
            elif method == 'kernel':
                # KernelExplainerï¼ˆæ¨¡å‹æ— å…³ï¼‰
                self.shap_explainer_ = shap.KernelExplainer(
                    self.model.predict_proba,
                    shap.sample(self.X_train, min(100, len(self.X_train)))
                )
                self.shap_values_ = self.shap_explainer_.shap_values(X_test_sample)
            
            elif method == 'linear':
                # LinearExplainerï¼ˆçº¿æ€§æ¨¡å‹ï¼‰
                self.shap_explainer_ = shap.LinearExplainer(
                    self.model,
                    self.X_train
                )
                self.shap_values_ = self.shap_explainer_.shap_values(X_test_sample)
            
            else:
                raise ValueError(f"Unsupported SHAP method: {method}")
            
            # å¤„ç†å¤šç±»åˆ«æƒ…å†µ
            if isinstance(self.shap_values_, list):
                # å¤šç±»åˆ«ï¼šå–ç¬¬ä¸€ä¸ªç±»åˆ«æˆ–å¹³å‡
                shap_vals = np.abs(self.shap_values_[1])  # é€šå¸¸å…³æ³¨æ­£ç±»
            else:
                shap_vals = np.abs(self.shap_values_)
            
            # è®¡ç®—å…¨å±€ç‰¹å¾é‡è¦æ€§
            global_importance = np.mean(shap_vals, axis=0)
            
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': global_importance
            }).sort_values('importance', ascending=False)
            
            self.logger.info(f"  ğŸ” Top 10 SHAPé‡è¦ç‰¹å¾:")
            for i, row in importance_df.head(10).iterrows():
                self.logger.info(f"     {row['feature']}: {row['importance']:.4f}")
            
            result = {
                'shap_values': self.shap_values_,
                'global_importance': importance_df,
                'method': method,
                'n_samples': len(X_test_sample)
            }
            
            return result
        
        except Exception as e:
            self.logger.error(f"âŒ SHAPè§£é‡Šå¤±è´¥: {e}")
            return {}
    
    def explain_with_lime(self,
                         X_test: pd.DataFrame,
                         instance_idx: int = 0,
                         n_features: int = 10) -> Dict:
        """
        ä½¿ç”¨LIMEè§£é‡Šå•ä¸ªé¢„æµ‹
        
        Args:
            X_test: æµ‹è¯•æ•°æ®
            instance_idx: è¦è§£é‡Šçš„å®ä¾‹ç´¢å¼•
            n_features: æ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡
            
        Returns:
            LIMEè§£é‡Šç»“æœ
        """
        if not HAS_LIME:
            self.logger.error("âŒ LIMEæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æ­¤åŠŸèƒ½")
            return {}
        
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ” ä½¿ç”¨LIMEè§£é‡Šå•ä¸ªé¢„æµ‹ (instance={instance_idx})...")
        
        try:
            # åˆ›å»ºLIME explainer
            if self.lime_explainer_ is None:
                self.lime_explainer_ = lime_tabular.LimeTabularExplainer(
                    self.X_train.values,
                    feature_names=self.feature_names,
                    class_names=['class_0', 'class_1'],
                    mode='classification',
                    random_state=self.random_state
                )
            
            # è§£é‡Šå•ä¸ªå®ä¾‹
            instance = X_test.iloc[instance_idx].values
            explanation = self.lime_explainer_.explain_instance(
                instance,
                self.model.predict_proba,
                num_features=n_features
            )
            
            # æå–ç‰¹å¾æƒé‡
            feature_weights = explanation.as_list()
            
            self.logger.info(f"  ğŸ“Š LIMEç‰¹å¾è´¡çŒ® (Top {n_features}):")
            for feat, weight in feature_weights:
                self.logger.info(f"     {feat}: {weight:+.4f}")
            
            result = {
                'explanation': explanation,
                'feature_weights': feature_weights,
                'instance_idx': instance_idx,
                'prediction': self.model.predict_proba([instance])[0]
            }
            
            return result
        
        except Exception as e:
            self.logger.error(f"âŒ LIMEè§£é‡Šå¤±è´¥: {e}")
            return {}
    
    def explain_with_permutation(self,
                                X_test: pd.DataFrame,
                                y_test: pd.Series,
                                n_repeats: int = 10,
                                scoring: str = 'f1_macro') -> Dict:
        """
        ä½¿ç”¨æ’åˆ—é‡è¦æ€§è§£é‡Šæ¨¡å‹
        
        Args:
            X_test: æµ‹è¯•æ•°æ®
            y_test: æµ‹è¯•æ ‡ç­¾
            n_repeats: æ’åˆ—é‡å¤æ¬¡æ•°
            scoring: è¯„åˆ†æŒ‡æ ‡
            
        Returns:
            æ’åˆ—é‡è¦æ€§ç»“æœ
        """
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ” ä½¿ç”¨æ’åˆ—é‡è¦æ€§è§£é‡Šæ¨¡å‹ (n_repeats={n_repeats})...")
        
        try:
            # è®¡ç®—æ’åˆ—é‡è¦æ€§
            perm_result = permutation_importance(
                self.model,
                X_test,
                y_test,
                n_repeats=n_repeats,
                random_state=self.random_state,
                n_jobs=-1,
                scoring=scoring
            )
            
            # æ•´ç†ç»“æœ
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance_mean': perm_result.importances_mean,
                'importance_std': perm_result.importances_std
            }).sort_values('importance_mean', ascending=False)
            
            self.perm_importance_ = importance_df
            
            self.logger.info(f"  ğŸ” Top 10æ’åˆ—é‡è¦ç‰¹å¾:")
            for i, row in importance_df.head(10).iterrows():
                self.logger.info(f"     {row['feature']}: {row['importance_mean']:.4f} Â± {row['importance_std']:.4f}")
            
            result = {
                'importance_df': importance_df,
                'raw_importances': perm_result.importances
            }
            
            return result
        
        except Exception as e:
            self.logger.error(f"âŒ æ’åˆ—é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def detect_feature_interactions(self,
                                    X_test: pd.DataFrame,
                                    top_n: int = 5) -> Dict:
        """
        æ£€æµ‹ç‰¹å¾äº¤äº’ï¼ˆåŸºäºSHAPäº¤äº’å€¼ï¼‰
        
        Args:
            X_test: æµ‹è¯•æ•°æ®
            top_n: è¿”å›å‰Nå¯¹äº¤äº’ç‰¹å¾
            
        Returns:
            ç‰¹å¾äº¤äº’ç»“æœ
        """
        if not HAS_SHAP:
            self.logger.error("âŒ SHAPæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹ç‰¹å¾äº¤äº’")
            return {}
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ”— æ£€æµ‹ç‰¹å¾äº¤äº’...")
        
        try:
            # é™åˆ¶æ ·æœ¬æ•°ï¼ˆäº¤äº’è®¡ç®—å¾ˆæ…¢ï¼‰
            max_samples = min(50, len(X_test))
            X_test_sample = X_test.sample(n=max_samples, random_state=self.random_state)
            
            self.logger.info(f"  ğŸ“Š ä½¿ç”¨{max_samples}ä¸ªæ ·æœ¬è®¡ç®—äº¤äº’...")
            
            # è®¡ç®—SHAPäº¤äº’å€¼
            if self.shap_explainer_ is None:
                if hasattr(self.model, 'tree_'):
                    self.shap_explainer_ = shap.TreeExplainer(self.model)
                else:
                    self.logger.warning("âš ï¸ ä»…æ ‘æ¨¡å‹æ”¯æŒå¿«é€Ÿäº¤äº’è®¡ç®—")
                    return {}
            
            shap_interaction_values = self.shap_explainer_.shap_interaction_values(X_test_sample)
            
            # å¤„ç†å¤šç±»åˆ«
            if isinstance(shap_interaction_values, list):
                shap_interaction_values = shap_interaction_values[1]
            
            # è®¡ç®—å¹³å‡äº¤äº’å¼ºåº¦
            n_features = len(self.feature_names)
            interaction_matrix = np.zeros((n_features, n_features))
            
            for i in range(n_features):
                for j in range(n_features):
                    if i != j:
                        interaction_matrix[i, j] = np.abs(shap_interaction_values[:, i, j]).mean()
            
            # æ‰¾åˆ°æœ€å¼ºçš„äº¤äº’
            interactions = []
            for i in range(n_features):
                for j in range(i+1, n_features):
                    interactions.append({
                        'feature_1': self.feature_names[i],
                        'feature_2': self.feature_names[j],
                        'interaction_strength': interaction_matrix[i, j]
                    })
            
            interactions_df = pd.DataFrame(interactions).sort_values(
                'interaction_strength',
                ascending=False
            )
            
            self.logger.info(f"  ğŸ” Top {top_n}ç‰¹å¾äº¤äº’:")
            for i, row in interactions_df.head(top_n).iterrows():
                self.logger.info(f"     {row['feature_1']} Ã— {row['feature_2']}: {row['interaction_strength']:.4f}")
            
            result = {
                'interactions_df': interactions_df,
                'interaction_matrix': interaction_matrix,
                'feature_names': self.feature_names
            }
            
            return result
        
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾äº¤äº’æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def comprehensive_interpretation(self,
                                    X_test: pd.DataFrame,
                                    y_test: Optional[pd.Series] = None) -> Dict:
        """
        æ‰§è¡Œå…¨é¢çš„æ¨¡å‹è§£é‡Šåˆ†æ
        
        åŒ…æ‹¬:
        1. SHAPå…¨å±€è§£é‡Š
        2. æ’åˆ—é‡è¦æ€§
        3. ç‰¹å¾äº¤äº’ï¼ˆå¯é€‰ï¼‰
        
        Args:
            X_test: æµ‹è¯•æ•°æ®
            y_test: æµ‹è¯•æ ‡ç­¾ï¼ˆæ’åˆ—é‡è¦æ€§éœ€è¦ï¼‰
            
        Returns:
            å®Œæ•´è§£é‡Šç»“æœ
        """
        self.logger.info("ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹è§£é‡Šåˆ†æ...")
        
        results = {}
        
        # 1. SHAPè§£é‡Š
        if HAS_SHAP:
            results['shap'] = self.explain_with_shap(X_test)
        
        # 2. æ’åˆ—é‡è¦æ€§
        if y_test is not None:
            results['permutation'] = self.explain_with_permutation(X_test, y_test)
        
        # 3. ç‰¹å¾äº¤äº’ï¼ˆå¯é€‰ï¼Œè®¡ç®—æ…¢ï¼‰
        # results['interactions'] = self.detect_feature_interactions(X_test)
        
        # 4. ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§æ£€æŸ¥
        if 'shap' in results and 'permutation' in results:
            results['consistency'] = self._check_importance_consistency(
                results['shap']['global_importance'],
                results['permutation']['importance_df']
            )
        
        self._print_comprehensive_summary(results)
        
        return results
    
    def _check_importance_consistency(self,
                                     shap_importance: pd.DataFrame,
                                     perm_importance: pd.DataFrame) -> Dict:
        """æ£€æŸ¥ä¸åŒæ–¹æ³•çš„ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ” æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§...")
        
        # è·å–Top 10ç‰¹å¾
        shap_top10 = set(shap_importance.head(10)['feature'])
        perm_top10 = set(perm_importance.head(10)['feature'])
        
        # è®¡ç®—é‡å 
        overlap = shap_top10 & perm_top10
        overlap_pct = len(overlap) / 10 * 100
        
        self.logger.info(f"  ğŸ“Š Top 10ç‰¹å¾é‡å : {len(overlap)}/10 ({overlap_pct:.0f}%)")
        
        if overlap_pct >= 70:
            self.logger.info("  âœ… ä¸€è‡´æ€§é«˜ - ç‰¹å¾é‡è¦æ€§å¯é ")
            consistency_level = 'high'
        elif overlap_pct >= 50:
            self.logger.info("  âš ï¸  ä¸€è‡´æ€§ä¸­ç­‰ - éœ€è¿›ä¸€æ­¥éªŒè¯")
            consistency_level = 'medium'
        else:
            self.logger.info("  ğŸš¨ ä¸€è‡´æ€§ä½ - ç‰¹å¾é‡è¦æ€§ä¸ç¨³å®š")
            consistency_level = 'low'
        
        return {
            'overlap_features': list(overlap),
            'overlap_pct': overlap_pct,
            'consistency_level': consistency_level
        }
    
    def _print_comprehensive_summary(self, results: Dict):
        """æ‰“å°å…¨é¢è§£é‡Šæ€»ç»“"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ† å…¨é¢æ¨¡å‹è§£é‡Šæ€»ç»“:")
        
        if 'shap' in results and results['shap']:
            self.logger.info("  âœ… SHAPè§£é‡Šå®Œæˆ")
        
        if 'permutation' in results and results['permutation']:
            self.logger.info("  âœ… æ’åˆ—é‡è¦æ€§å®Œæˆ")
        
        if 'consistency' in results:
            level = results['consistency']['consistency_level']
            overlap_pct = results['consistency']['overlap_pct']
            self.logger.info(f"  ğŸ“Š ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§: {level.upper()} ({overlap_pct:.0f}%é‡å )")
        
        self.logger.info("=" * 60)
    
    def plot_shap_summary(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶SHAPæ€»ç»“å›¾"""
        if not HAS_SHAP or self.shap_values_ is None:
            self.logger.warning("âš ï¸ è¯·å…ˆè¿è¡Œexplain_with_shap()")
            return
        
        try:
            plt.figure(figsize=(10, 6))
            
            if isinstance(self.shap_values_, list):
                shap.summary_plot(self.shap_values_[1], self.X_train, show=False)
            else:
                shap.summary_plot(self.shap_values_, self.X_train, show=False)
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"  ğŸ’¾ SHAPæ€»ç»“å›¾å·²ä¿å­˜: {save_path}")
            
            plt.close()
        
        except Exception as e:
            self.logger.error(f"âŒ ç»˜å›¾å¤±è´¥: {e}")


if __name__ == '__main__':
    # ç®€å•æµ‹è¯•
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    
    print("æ¨¡å‹å¯è§£é‡Šæ€§æ¨¡å—æµ‹è¯•")
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=500,
        n_features=10,
        n_informative=5,
        random_state=42
    )
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])
    y = pd.Series(y)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    
    # åˆ›å»ºè§£é‡Šå™¨
    interpreter = ModelInterpreter(model, X_train)
    
    # æ‰§è¡Œè§£é‡Š
    results = interpreter.comprehensive_interpretation(X_test, y_test)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ")
    if 'consistency' in results:
        print(f"   ä¸€è‡´æ€§: {results['consistency']['consistency_level']}")

