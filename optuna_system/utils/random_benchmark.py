"""
éšæœºåŸºå‡†æµ‹è¯•æ¨¡å— (Random Benchmark Testing)

é€šè¿‡å¯¹æ ‡ç­¾è¿›è¡Œéšæœºæ‰“ä¹±ï¼ŒéªŒè¯æ¨¡å‹æ˜¯å¦çœŸçš„ä»æ•°æ®ä¸­å­¦ä¹ åˆ°æ¨¡å¼ã€‚
å¦‚æœæ‰“ä¹±æ ‡ç­¾åæ€§èƒ½ä¾ç„¶å¾ˆå¥½ï¼Œè¯´æ˜æ¨¡å‹åœ¨è¿‡æ‹Ÿåˆå™ªå£°æˆ–å­˜åœ¨æ•°æ®æ³„æ¼ã€‚

åŸºäºå­¦æœ¯æ–‡çŒ®:
- Dua, D., & Graff, C. (2017): "UCI Machine Learning Repository"
- Wolpert, D. H., & Macready, W. G. (1997): "No Free Lunch Theorems"
- LÃ³pez de Prado, M. (2018): "Advances in Financial Machine Learning"

ä½œè€…: Optuna System Team
æ—¥æœŸ: 2025-10-31
"""

import logging
from typing import Dict, List, Optional, Callable, Any
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.base import clone
import matplotlib.pyplot as plt
from scipy import stats

logger = logging.getLogger(__name__)


class RandomBenchmarkTester:
    """
    éšæœºåŸºå‡†æµ‹è¯•å™¨
    
    æ‰§è¡Œä»¥ä¸‹æµ‹è¯•:
    1. æ ‡ç­¾éšæœºæ‰“ä¹±æµ‹è¯• (Label Permutation Test)
    2. ç‰¹å¾éšæœºæ‰“ä¹±æµ‹è¯• (Feature Permutation Test)
    3. ä¸éšæœºåŸºçº¿å¯¹æ¯” (Random Baseline Comparison)
    
    åˆ¤æ–­æ ‡å‡†:
    - çœŸå®æ€§èƒ½ >> éšæœºæ€§èƒ½: æ¨¡å‹å­¦åˆ°äº†çœŸå®æ¨¡å¼ âœ…
    - çœŸå®æ€§èƒ½ â‰ˆ éšæœºæ€§èƒ½: æ¨¡å‹æœªå­¦åˆ°æœ‰æ•ˆæ¨¡å¼ âš ï¸
    - çœŸå®æ€§èƒ½ < éšæœºæ€§èƒ½: å¯èƒ½å­˜åœ¨Bugæˆ–æ•°æ®æ³„æ¼ ğŸš¨
    """
    
    def __init__(self,
                 n_permutations: int = 10,
                 n_cv_splits: int = 5,
                 random_state: int = 42,
                 metric: str = 'f1_macro'):
        """
        åˆå§‹åŒ–éšæœºåŸºå‡†æµ‹è¯•å™¨
        
        Args:
            n_permutations: éšæœºæ‰“ä¹±æ¬¡æ•°
            n_cv_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­
            metric: è¯„ä¼°æŒ‡æ ‡ ('f1_macro', 'accuracy', 'roc_auc', etc.)
        """
        self.n_permutations = n_permutations
        self.n_cv_splits = n_cv_splits
        self.random_state = random_state
        self.metric = metric
        self.logger = logger
        
        # å­˜å‚¨ç»“æœ
        self.results_ = None
    
    def test_label_permutation(self,
                               model: Any,
                               X: pd.DataFrame,
                               y: pd.Series,
                               cv: Optional[Any] = None) -> Dict:
        """
        æ ‡ç­¾éšæœºæ‰“ä¹±æµ‹è¯•
        
        å°†æ ‡ç­¾éšæœºæ‰“ä¹±å¤šæ¬¡ï¼Œè®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½ã€‚
        å¦‚æœæ‰“ä¹±åæ€§èƒ½ä¸‹é™ä¸æ˜æ˜¾ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½åœ¨æ‹Ÿåˆå™ªå£°ã€‚
        
        Args:
            model: å¾…æµ‹è¯•çš„æ¨¡å‹ï¼ˆéœ€å®ç°sklearnæ¥å£ï¼‰
            X: ç‰¹å¾çŸ©é˜µ
            y: çœŸå®æ ‡ç­¾
            cv: äº¤å‰éªŒè¯ç­–ç•¥ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤StratifiedKFoldï¼‰
            
        Returns:
            DictåŒ…å«:
                - real_score_mean: çœŸå®æ ‡ç­¾çš„å¹³å‡åˆ†æ•°
                - real_score_std: çœŸå®æ ‡ç­¾çš„æ ‡å‡†å·®
                - permuted_scores_mean: æ‰“ä¹±æ ‡ç­¾çš„å¹³å‡åˆ†æ•°
                - permuted_scores_std: æ‰“ä¹±æ ‡ç­¾çš„æ ‡å‡†å·®
                - p_value: ç»Ÿè®¡æ˜¾è‘—æ€§på€¼
                - is_significant: æ˜¯å¦æ˜¾è‘—å¥½äºéšæœº
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ² æ‰§è¡Œæ ‡ç­¾éšæœºæ‰“ä¹±æµ‹è¯•...")
        
        if cv is None:
            cv = StratifiedKFold(
                n_splits=self.n_cv_splits,
                shuffle=True,
                random_state=self.random_state
            )
        
        # 1. åœ¨çœŸå®æ ‡ç­¾ä¸Šè¯„ä¼°
        self.logger.info("  ğŸ“Š åœ¨çœŸå®æ ‡ç­¾ä¸Šè¯„ä¼°...")
        real_scores = cross_val_score(
            clone(model), X, y,
            cv=cv,
            scoring=self.metric,
            n_jobs=-1
        )
        real_score_mean = np.mean(real_scores)
        real_score_std = np.std(real_scores)
        
        self.logger.info(f"     çœŸå®åˆ†æ•°: {real_score_mean:.4f} Â± {real_score_std:.4f}")
        
        # 2. åœ¨æ‰“ä¹±æ ‡ç­¾ä¸Šè¯„ä¼°
        self.logger.info(f"  ğŸ”€ æ‰§è¡Œ{self.n_permutations}æ¬¡æ ‡ç­¾æ‰“ä¹±æµ‹è¯•...")
        permuted_scores = []
        
        for i in range(self.n_permutations):
            # æ‰“ä¹±æ ‡ç­¾
            y_permuted = y.sample(frac=1, random_state=self.random_state + i).reset_index(drop=True)
            
            # è¯„ä¼°
            scores = cross_val_score(
                clone(model), X, y_permuted,
                cv=cv,
                scoring=self.metric,
                n_jobs=-1
            )
            permuted_score = np.mean(scores)
            permuted_scores.append(permuted_score)
            
            self.logger.debug(f"     æ‰“ä¹± {i+1}: {permuted_score:.4f}")
        
        permuted_scores_mean = np.mean(permuted_scores)
        permuted_scores_std = np.std(permuted_scores)
        
        self.logger.info(f"     æ‰“ä¹±åˆ†æ•°: {permuted_scores_mean:.4f} Â± {permuted_scores_std:.4f}")
        
        # 3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆWilcoxonç¬¦å·ç§©æ£€éªŒï¼‰
        # H0: çœŸå®åˆ†æ•° = éšæœºåˆ†æ•°
        # H1: çœŸå®åˆ†æ•° > éšæœºåˆ†æ•°
        try:
            # æ‰©å±•çœŸå®åˆ†æ•°åˆ°ç›¸åŒé•¿åº¦ä»¥è¿›è¡Œé…å¯¹æ£€éªŒ
            real_scores_extended = np.full(self.n_permutations, real_score_mean)
            statistic, p_value = stats.wilcoxon(
                real_scores_extended,
                permuted_scores,
                alternative='greater'
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç»Ÿè®¡æ£€éªŒå¤±è´¥: {e}")
            p_value = None
        
        # åˆ¤æ–­æ˜¯å¦æ˜¾è‘—
        is_significant = (p_value is not None and p_value < 0.05)
        
        # è®¡ç®—æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
        effect_size = (real_score_mean - permuted_scores_mean) / (
            np.sqrt((real_score_std**2 + permuted_scores_std**2) / 2) + 1e-10
        )
        
        result = {
            'real_score_mean': real_score_mean,
            'real_score_std': real_score_std,
            'real_scores': real_scores.tolist(),
            'permuted_scores_mean': permuted_scores_mean,
            'permuted_scores_std': permuted_scores_std,
            'permuted_scores': permuted_scores,
            'p_value': p_value,
            'is_significant': is_significant,
            'effect_size': effect_size,
            'improvement_pct': ((real_score_mean - permuted_scores_mean) / permuted_scores_mean * 100) if permuted_scores_mean > 0 else 0
        }
        
        self._print_permutation_summary(result)
        
        return result
    
    def test_feature_permutation(self,
                                 model: Any,
                                 X: pd.DataFrame,
                                 y: pd.Series,
                                 n_features_to_shuffle: Optional[int] = None) -> Dict:
        """
        ç‰¹å¾éšæœºæ‰“ä¹±æµ‹è¯•
        
        é€ä¸ªæˆ–æ‰¹é‡æ‰“ä¹±ç‰¹å¾ï¼Œè§‚å¯Ÿæ€§èƒ½ä¸‹é™æƒ…å†µã€‚
        æ€§èƒ½ä¸‹é™è¶Šå¤šï¼Œè¯´æ˜è¯¥ç‰¹å¾è¶Šé‡è¦ã€‚
        
        Args:
            model: å¾…æµ‹è¯•çš„æ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾
            n_features_to_shuffle: è¦æ‰“ä¹±çš„ç‰¹å¾æ•°é‡ï¼ˆNoneåˆ™æ‰“ä¹±æ‰€æœ‰ï¼‰
            
        Returns:
            ç‰¹å¾é‡è¦æ€§å­—å…¸
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ”€ æ‰§è¡Œç‰¹å¾éšæœºæ‰“ä¹±æµ‹è¯•...")
        
        # åŸºçº¿æ€§èƒ½ï¼ˆæ‰€æœ‰ç‰¹å¾æ­£å¸¸ï¼‰
        cv = StratifiedKFold(
            n_splits=self.n_cv_splits,
            shuffle=True,
            random_state=self.random_state
        )
        
        baseline_scores = cross_val_score(
            clone(model), X, y,
            cv=cv,
            scoring=self.metric,
            n_jobs=-1
        )
        baseline_score = np.mean(baseline_scores)
        
        self.logger.info(f"  ğŸ“Š åŸºçº¿åˆ†æ•°ï¼ˆæ‰€æœ‰ç‰¹å¾æ­£å¸¸ï¼‰: {baseline_score:.4f}")
        
        # ç¡®å®šè¦æµ‹è¯•çš„ç‰¹å¾
        if n_features_to_shuffle is None:
            features_to_test = X.columns.tolist()
        else:
            # éšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾
            features_to_test = np.random.choice(
                X.columns,
                size=min(n_features_to_shuffle, len(X.columns)),
                replace=False
            ).tolist()
        
        self.logger.info(f"  ğŸ”¢ æµ‹è¯•{len(features_to_test)}ä¸ªç‰¹å¾...")
        
        feature_importances = {}
        
        for feature in features_to_test:
            # å¤åˆ¶æ•°æ®å¹¶æ‰“ä¹±è¯¥ç‰¹å¾
            X_shuffled = X.copy()
            X_shuffled[feature] = X_shuffled[feature].sample(
                frac=1,
                random_state=self.random_state
            ).reset_index(drop=True)
            
            # è¯„ä¼°æ€§èƒ½
            shuffled_scores = cross_val_score(
                clone(model), X_shuffled, y,
                cv=cv,
                scoring=self.metric,
                n_jobs=-1
            )
            shuffled_score = np.mean(shuffled_scores)
            
            # è®¡ç®—æ€§èƒ½ä¸‹é™
            importance = baseline_score - shuffled_score
            feature_importances[feature] = {
                'importance': importance,
                'score_after_shuffle': shuffled_score,
                'score_drop_pct': (importance / baseline_score * 100) if baseline_score > 0 else 0
            }
            
            self.logger.debug(f"     {feature}: -{importance:.4f} ({feature_importances[feature]['score_drop_pct']:.1f}%)")
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_importances = dict(
            sorted(
                feature_importances.items(),
                key=lambda x: x[1]['importance'],
                reverse=True
            )
        )
        
        # æ‰“å°Top 10
        self.logger.info(f"  ğŸ” Top 10é‡è¦ç‰¹å¾:")
        for i, (feat, info) in enumerate(list(sorted_importances.items())[:10]):
            self.logger.info(f"     {i+1}. {feat}: -{info['importance']:.4f} ({info['score_drop_pct']:.1f}%)")
        
        return {
            'baseline_score': baseline_score,
            'feature_importances': sorted_importances
        }
    
    def test_random_baseline(self,
                            model: Any,
                            X: pd.DataFrame,
                            y: pd.Series,
                            baseline_type: str = 'stratified') -> Dict:
        """
        ä¸éšæœºåŸºçº¿å¯¹æ¯”
        
        æ¯”è¾ƒæ¨¡å‹ä¸ç®€å•éšæœºçŒœæµ‹çš„æ€§èƒ½ã€‚
        
        Args:
            model: å¾…æµ‹è¯•çš„æ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾
            baseline_type: åŸºçº¿ç±»å‹ ('stratified', 'uniform', 'majority')
            
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ¯ ä¸éšæœºåŸºçº¿å¯¹æ¯” (type={baseline_type})...")
        
        cv = StratifiedKFold(
            n_splits=self.n_cv_splits,
            shuffle=True,
            random_state=self.random_state
        )
        
        # æ¨¡å‹æ€§èƒ½
        model_scores = cross_val_score(
            clone(model), X, y,
            cv=cv,
            scoring=self.metric,
            n_jobs=-1
        )
        model_score = np.mean(model_scores)
        
        # è®¡ç®—éšæœºåŸºçº¿
        if baseline_type == 'stratified':
            # æŒ‰ç±»åˆ«åˆ†å¸ƒéšæœºçŒœæµ‹
            class_distribution = y.value_counts(normalize=True)
            if self.metric in ['f1_macro', 'accuracy']:
                baseline_score = 1.0 / len(class_distribution)  # ç®€åŒ–ä¼°è®¡
            else:
                baseline_score = 0.5  # AUCçš„éšæœºåŸºçº¿
        elif baseline_type == 'uniform':
            # å‡åŒ€éšæœºçŒœæµ‹
            n_classes = y.nunique()
            baseline_score = 1.0 / n_classes
        elif baseline_type == 'majority':
            # å§‹ç»ˆçŒœæµ‹å¤šæ•°ç±»
            majority_class_pct = y.value_counts().max() / len(y)
            baseline_score = majority_class_pct if self.metric == 'accuracy' else 0.5
        else:
            raise ValueError(f"Unknown baseline_type: {baseline_type}")
        
        # è®¡ç®—æ”¹è¿›
        improvement = model_score - baseline_score
        improvement_pct = (improvement / baseline_score * 100) if baseline_score > 0 else 0
        
        result = {
            'model_score': model_score,
            'baseline_score': baseline_score,
            'improvement': improvement,
            'improvement_pct': improvement_pct,
            'baseline_type': baseline_type
        }
        
        self._print_baseline_summary(result)
        
        return result
    
    def full_benchmark(self,
                      model: Any,
                      X: pd.DataFrame,
                      y: pd.Series) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„éšæœºåŸºå‡†æµ‹è¯•å¥—ä»¶
        
        åŒ…æ‹¬:
        1. æ ‡ç­¾æ‰“ä¹±æµ‹è¯•
        2. ç‰¹å¾æ‰“ä¹±æµ‹è¯•ï¼ˆå‰10ä¸ªç‰¹å¾ï¼‰
        3. éšæœºåŸºçº¿å¯¹æ¯”
        
        Args:
            model: å¾…æµ‹è¯•çš„æ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾
            
        Returns:
            å®Œæ•´æµ‹è¯•ç»“æœ
        """
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´éšæœºåŸºå‡†æµ‹è¯•å¥—ä»¶...")
        
        results = {}
        
        # 1. æ ‡ç­¾æ‰“ä¹±æµ‹è¯•
        results['label_permutation'] = self.test_label_permutation(model, X, y)
        
        # 2. ç‰¹å¾æ‰“ä¹±æµ‹è¯•
        results['feature_permutation'] = self.test_feature_permutation(
            model, X, y,
            n_features_to_shuffle=min(10, len(X.columns))
        )
        
        # 3. éšæœºåŸºçº¿å¯¹æ¯”
        results['random_baseline'] = self.test_random_baseline(model, X, y)
        
        # ç»¼åˆè¯„ä¼°
        results['overall_assessment'] = self._assess_overall(results)
        
        self.results_ = results
        
        self._print_overall_summary()
        
        return results
    
    def _assess_overall(self, results: Dict) -> str:
        """ç»¼åˆè¯„ä¼°æ¨¡å‹è´¨é‡"""
        label_perm = results['label_permutation']
        baseline = results['random_baseline']
        
        # åˆ¤æ–­æ ‡å‡†
        is_better_than_random = label_perm['is_significant']
        improvement_vs_baseline = baseline['improvement_pct']
        
        if is_better_than_random and improvement_vs_baseline > 50:
            return 'excellent'
        elif is_better_than_random and improvement_vs_baseline > 20:
            return 'good'
        elif is_better_than_random and improvement_vs_baseline > 10:
            return 'acceptable'
        elif is_better_than_random:
            return 'marginal'
        else:
            return 'poor'
    
    def _print_permutation_summary(self, result: Dict):
        """æ‰“å°æ ‡ç­¾æ‰“ä¹±æµ‹è¯•æ€»ç»“"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š æ ‡ç­¾æ‰“ä¹±æµ‹è¯•ç»“æœ:")
        self.logger.info(f"  çœŸå®åˆ†æ•°: {result['real_score_mean']:.4f} Â± {result['real_score_std']:.4f}")
        self.logger.info(f"  æ‰“ä¹±åˆ†æ•°: {result['permuted_scores_mean']:.4f} Â± {result['permuted_scores_std']:.4f}")
        self.logger.info(f"  æ”¹è¿›: +{result['improvement_pct']:.1f}%")
        self.logger.info(f"  æ•ˆåº”é‡: {result['effect_size']:.2f}")
        
        if result['p_value'] is not None:
            self.logger.info(f"  på€¼: {result['p_value']:.4f}")
        
        if result['is_significant']:
            self.logger.info("  âœ… æ¨¡å‹æ˜¾è‘—å¥½äºéšæœº (p < 0.05)")
        else:
            self.logger.info("  âš ï¸  æ¨¡å‹æœªæ˜¾è‘—å¥½äºéšæœº (p >= 0.05)")
            self.logger.info("     å¯èƒ½åŸå› : æ•°æ®æ³„æ¼ã€è¿‡æ‹Ÿåˆå™ªå£°ã€ç‰¹å¾æ— æ•ˆ")
        
        self.logger.info("=" * 60)
    
    def _print_baseline_summary(self, result: Dict):
        """æ‰“å°åŸºçº¿å¯¹æ¯”æ€»ç»“"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š éšæœºåŸºçº¿å¯¹æ¯”ç»“æœ:")
        self.logger.info(f"  æ¨¡å‹åˆ†æ•°: {result['model_score']:.4f}")
        self.logger.info(f"  åŸºçº¿åˆ†æ•°: {result['baseline_score']:.4f} ({result['baseline_type']})")
        self.logger.info(f"  æ”¹è¿›: +{result['improvement_pct']:.1f}%")
        
        if result['improvement_pct'] > 50:
            self.logger.info("  âœ… æ¨¡å‹æ˜¾è‘—ä¼˜äºåŸºçº¿")
        elif result['improvement_pct'] > 20:
            self.logger.info("  âœ… æ¨¡å‹ä¼˜äºåŸºçº¿")
        elif result['improvement_pct'] > 10:
            self.logger.info("  âš ï¸  æ¨¡å‹ç•¥ä¼˜äºåŸºçº¿")
        else:
            self.logger.info("  ğŸš¨ æ¨¡å‹ä»…ç•¥èƒœåŸºçº¿ï¼Œéœ€è¦æ”¹è¿›")
        
        self.logger.info("=" * 60)
    
    def _print_overall_summary(self):
        """æ‰“å°æ•´ä½“æ€»ç»“"""
        if self.results_ is None:
            return
        
        assessment = self.results_['overall_assessment']
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ† å®Œæ•´åŸºå‡†æµ‹è¯•æ€»ä½“è¯„ä¼°:")
        
        if assessment == 'excellent':
            self.logger.info("  âœ… ä¼˜ç§€ - æ¨¡å‹å­¦åˆ°äº†å¼ºæœ‰åŠ›çš„æ¨¡å¼")
        elif assessment == 'good':
            self.logger.info("  âœ… è‰¯å¥½ - æ¨¡å‹æ€§èƒ½å¯é ")
        elif assessment == 'acceptable':
            self.logger.info("  âš ï¸  å¯æ¥å— - æ¨¡å‹æœ‰ä¸€å®šé¢„æµ‹èƒ½åŠ›")
        elif assessment == 'marginal':
            self.logger.info("  âš ï¸  è¾¹ç¼˜ - æ¨¡å‹é¢„æµ‹èƒ½åŠ›è¾ƒå¼±")
        else:  # poor
            self.logger.info("  ğŸš¨ è¾ƒå·® - æ¨¡å‹æœªå­¦åˆ°æœ‰æ•ˆæ¨¡å¼")
            self.logger.info("     å»ºè®®: æ£€æŸ¥æ•°æ®è´¨é‡ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹é€‰æ‹©")
        
        self.logger.info("=" * 60)


if __name__ == '__main__':
    # ç®€å•æµ‹è¯•
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.datasets import make_classification
    
    print("éšæœºåŸºå‡†æµ‹è¯•æ¨¡å—æµ‹è¯•")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=10,
        n_redundant=5,
        random_state=42
    )
    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(20)])
    y = pd.Series(y)
    
    # åˆ›å»ºæ¨¡å‹
    model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)
    
    # æ‰§è¡Œæµ‹è¯•
    tester = RandomBenchmarkTester(n_permutations=5, n_cv_splits=3)
    results = tester.full_benchmark(model, X, y)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ")
    print(f"   æ€»ä½“è¯„ä¼°: {results['overall_assessment'].upper()}")

