#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Layer3 äº”æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–å’Œè®­ç»ƒè„šæœ¬
"""

import sys
import logging
from pathlib import Path
from datetime import datetime
import pandas as pd
import os
import json
import argparse

try:
    # ç¡®ä¿æ§åˆ¶å°ä½¿ç”¨ UTF-8 ç¼–ç ï¼Œå¦åˆ™ä¸­æ–‡æ—¥å¿—ä¼šæŠ¥é”™
    for stream in (sys.stdout, sys.stderr):
        if hasattr(stream, "reconfigure"):
            stream.reconfigure(encoding="utf-8")
except Exception:
    pass

# è®¾ç½®æ—¥å¿—
log_filename = f'layer3_optimization_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

try:
    from optuna_system.utils.io_utils import read_dataframe
except ImportError:
    from optuna_system.utils.io_utils import read_dataframe  # type: ignore


def resolve_layer_json(json_name: str) -> dict:
    json_path = Path("optuna_system/configs") / json_name
    if not json_path.exists():
        return {}
    try:
        with json_path.open('r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}


def get_latest_version() -> str | None:
    try:
        latest_file = Path("optuna_system/results/latest.txt")
        if latest_file.exists():
            v = latest_file.read_text(encoding="utf-8").strip()
            return v if v else None
    except Exception:
        pass
    return None


def load_dataframe_with_fallback(path: Path) -> pd.DataFrame:
    df = read_dataframe(path)
    if not isinstance(df.index, pd.DatetimeIndex):
        try:
            df.index = pd.to_datetime(df.index)
        except Exception:
            pass
    return df

def find_latest_file(directory: Path, pattern: str, prefer_suffix: tuple[str, ...] = (".parquet", ".pkl")) -> Path | None:
    candidates = sorted(directory.rglob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
    if not candidates:
        return None
    for suffix in prefer_suffix:
        for path in candidates:
            if path.suffix.lower() == suffix.lower():
                try:
                    _ = read_dataframe(path)
                    return path
                except Exception:
                    continue
    for path in candidates:
        try:
            _ = read_dataframe(path)
            return path
        except Exception:
            continue
    return None

def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    logger.info("="*80)
    logger.info("æ£€æŸ¥Layer3å‰ç½®æ¡ä»¶...")
    logger.info("="*80)
    
    version_sub = get_latest_version()

    # å„ªå…ˆåœ¨ latest ç‰ˆæœ¬å­ç›®éŒ„ä¸­å°‹æ‰¾ï¼Œå¦å‰‡å›é€€åˆ°æ•´å€‹ç›®éŒ„æƒæ
    base_labels_dir = Path("data/processed/labels/BTCUSDT_15m")
    labels_dir = base_labels_dir / version_sub if version_sub else base_labels_dir
    labels_file = find_latest_file(labels_dir, "labels*", prefer_suffix=(".pkl", ".parquet"))
    if labels_file is None:
        # å›é€€ï¼šåœ¨æ›´é«˜å±¤ç´šæƒæå« 15m çš„æ¨™ç±¤æª”ï¼ˆéè¿´ï¼‰
        labels_file = find_latest_file(Path("data/processed/labels"), "labels*15m*", prefer_suffix=(".pkl", ".parquet"))
    
    if labels_file:
        size_mb = os.path.getsize(labels_file) / (1024*1024)
        mtime = datetime.fromtimestamp(os.path.getmtime(labels_file))
        df = read_dataframe(labels_file)
        logger.info(f"âœ… Layer1 æ ‡ç­¾æ–‡ä»¶:")
        logger.info(f"   è·¯å¾„: {labels_file}")
        logger.info(f"   å¤§å°: {size_mb:.2f}MB")
        logger.info(f"   ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"   å½¢çŠ¶: {df.shape}")
        if 'label' in df.columns:
            label_dist = df['label'].value_counts().to_dict()
            logger.info(f"   æ ‡ç­¾åˆ†å¸ƒ: {label_dist}")
    else:
        logger.error("âŒ æœªæ‰¾åˆ°Layer1æ ‡ç­¾æ–‡ä»¶")
        return False, None, None
    
    base_features_dir = Path("data/processed/features/BTCUSDT_15m")
    features_dir = base_features_dir / version_sub if version_sub else base_features_dir
    features_file = find_latest_file(features_dir, "features*", prefer_suffix=(".parquet", ".pkl"))
    if features_file is None:
        # å›é€€ï¼šåœ¨æ›´é«˜å±¤ç´šæƒæå« 15m çš„ç‰¹å¾µæª”ï¼ˆéè¿´ï¼‰
        features_file = find_latest_file(Path("data/processed/features"), "features*15m*", prefer_suffix=(".parquet", ".pkl"))
    
    if features_file:
        size_mb = os.path.getsize(features_file) / (1024*1024)
        mtime = datetime.fromtimestamp(os.path.getmtime(features_file))
        df = read_dataframe(features_file)
        logger.info(f"âœ… Layer2 ç‰¹å¾æ–‡ä»¶:")
        logger.info(f"   è·¯å¾„: {features_file}")
        logger.info(f"   å¤§å°: {size_mb:.2f}MB")
        logger.info(f"   ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"   å½¢çŠ¶: {df.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)")
    else:
        logger.error("âŒ æœªæ‰¾åˆ°Layer2ç‰¹å¾æ–‡ä»¶")
        return False, None, None
    
    cleaned_candidates = [
        Path("data/processed/cleaned/BTCUSDT_15m/cleaned_ohlcv.pkl"),
        Path("data/processed/cleaned/BTCUSDT_15m/cleaned_ohlcv.parquet")
    ]
    cleaned_file = next((p for p in cleaned_candidates if p.exists()), None)
    if cleaned_file:
        logger.info(f"âœ… Layer0 æ¸…æ´—æ–‡ä»¶: {cleaned_file}")
    else:
        logger.warning("âš ï¸ æœªæ‰¾åˆ°cleaned_ohlcv (æ”¶ç›Šç‡è®¡ç®—å¯èƒ½å—å½±å“)")
    
    # ç‰ˆæœ¬ä¸€è‡´æ€§æç¤º
    try:
        def extract_vn(p: Path) -> str | None:
            parts = [part for part in p.parts if part.startswith('v') and len(part) > 1 and part[1:].isdigit()]
            return parts[-1] if parts else None
        vn_labels = extract_vn(labels_file) if labels_file else None
        vn_features = extract_vn(features_file) if features_file else None
        if vn_labels and vn_features and vn_labels != vn_features:
            logger.warning(f"âš ï¸ æ¨™ç±¤èˆ‡ç‰¹å¾µç‰ˆæœ¬ä¸ä¸€è‡´: labels={vn_labels}, features={vn_features} â€” å»ºè­°é‡æ–°ç‰©åŒ–æˆ–æŒ‡å®šä¸€è‡´ç‰ˆæœ¬")
        if version_sub:
            logger.info(f"ç‰ˆæœ¬å„ªå…ˆç­–ç•¥: ä½¿ç”¨ latest ç‰ˆæœ¬ {version_sub}")
    except Exception:
        pass

    logger.info("\nâœ… æ‰€æœ‰å‰ç½®æ¡ä»¶æ»¡è¶³ï¼å‡†å¤‡å¼€å§‹Layer3ä¼˜åŒ–...\n")
    return True, labels_file, features_file

def prepare_config_files(labels_file, features_file):
    """å‡†å¤‡configsç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶é“¾æ¥"""
    logger.info("å‡†å¤‡é…ç½®æ–‡ä»¶...")
    
    configs_dir = Path("configs")
    configs_dir.mkdir(exist_ok=True)
    
    # å¤åˆ¶æˆ–é“¾æ¥æ–‡ä»¶åˆ°configsç›®å½•
    target_labels = configs_dir / "labels_15m.parquet"
    target_features = configs_dir / "selected_features_15m.parquet"
    
    # è¯»å–å¹¶ä¿å­˜åˆ°configs
    labels_df = read_dataframe(labels_file)
    labels_df.to_parquet(target_labels)
    logger.info(f"âœ… æ ‡ç­¾æ–‡ä»¶å·²å¤åˆ¶åˆ°: {target_labels}")
    
    features_df = read_dataframe(features_file)
    # é˜²æ´©æ¼ï¼šè‹¥ç‰¹å¾µé›†å«æœ‰ labelï¼Œå…ˆç§»é™¤å†å¯«å…¥ configs
    if 'label' in features_df.columns:
        features_df = features_df.drop(columns=['label'])
    features_df.to_parquet(target_features)
    logger.info(f"âœ… ç‰¹å¾æ–‡ä»¶å·²å¤åˆ¶åˆ°: {target_features}")
    
    # å¤åˆ¶cleanedæ–‡ä»¶
    cleaned_priorities = [
        Path("data/processed/cleaned/BTCUSDT_15m/cleaned_ohlcv.pkl"),
        Path("data/processed/cleaned/BTCUSDT_15m/cleaned_ohlcv.parquet")
    ]
    for cleaned_source in cleaned_priorities:
        if cleaned_source.exists():
            cleaned_target = configs_dir / "cleaned_ohlcv_15m.parquet"
            cleaned_df = read_dataframe(cleaned_source)
            cleaned_df.to_parquet(cleaned_target)
            logger.info(f"âœ… æ¸…æ´—æ–‡ä»¶å·²å¤åˆ¶åˆ°: {cleaned_target}")
            break

def run_layer3_optimization(n_trials_per_model: int = 250):
    """è¿è¡ŒLayer3ä¼˜åŒ–"""
    logger.info("="*80)
    logger.info("å¼€å§‹Layer3äº”æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–...")
    logger.info("="*80)
    
    try:
        from optuna_system.optimizers.optuna_model import ModelOptimizer
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        version_sub = get_latest_version()
        results_base = Path("optuna_system/results/BTCUSDT_15m")
        results_path = results_base / version_sub if version_sub else results_base
        results_path.mkdir(parents=True, exist_ok=True)

        optimizer = ModelOptimizer(
            data_path='data',
            config_path='configs',
            results_path=str(results_path),
            symbol='BTCUSDT',
            timeframe='15m'
        )
        
        logger.info(f"\né…ç½®å‚æ•°:")
        logger.info(f"  - æ•°æ®è·¯å¾„: data")
        logger.info(f"  - é…ç½®è·¯å¾„: configs")
        logger.info(f"  - ç»“æœè·¯å¾„: {results_path}")
        logger.info(f"  - äº¤æ˜“å¯¹: BTCUSDT")
        logger.info(f"  - æ—¶é—´æ¡†æ¶: 15m")
        logger.info(f"  - æ¯ä¸ªæ¨¡å‹trials: {n_trials_per_model}")
        logger.info(f"  - æ€»è®¡trials: {n_trials_per_model * len(optimizer.models_to_train)}")
        logger.info(f"  - æ¨¡å‹åˆ—è¡¨: {optimizer.models_to_train}")
        logger.info("")
        
        # å¼€å§‹ä¼˜åŒ–
        start_time = datetime.now()
        logger.info(f"â° å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        result = optimizer.optimize(n_trials=n_trials_per_model)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("="*80)
        logger.info("ğŸ‰ Layer3ä¼˜åŒ–å®Œæˆï¼")
        logger.info("="*80)
        logger.info(f"â° ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration/60:.1f}åˆ†é’Ÿ ({duration:.0f}ç§’)")
        logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {result.get('best_model_type', 'N/A')}")
        logger.info(f"ğŸ“Š æœ€ä½³å¾—åˆ†: {result.get('best_score', 0):.4f}")
        
        if 'all_models' in result:
            logger.info("\nå„æ¨¡å‹æ€§èƒ½:")
            for model_name, model_result in result['all_models'].items():
                logger.info(f"  - {model_name}: {model_result['best_score']:.4f}")
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        logger.info("\nç”Ÿæˆçš„æ–‡ä»¶:")
        results_path = Path("optuna_system/results/BTCUSDT_15m")
        for pred_file in results_path.glob("*_predictions.parquet"):
            size_kb = os.path.getsize(pred_file) / 1024
            df = pd.read_parquet(pred_file)
            logger.info(f"  âœ… {pred_file.name}: {size_kb:.1f}KB, å½¢çŠ¶: {df.shape}")
        
        config_file = Path("configs/model_params.json")
        if config_file.exists():
            size_kb = os.path.getsize(config_file) / 1024
            logger.info(f"  âœ… model_params.json: {size_kb:.1f}KB")
        
        logger.info("\nğŸŠ Layer3ä¼˜åŒ–æˆåŠŸå®Œæˆï¼å¯ä»¥ç»§ç»­æ‰§è¡ŒLayer4-9ã€‚")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Layer3ä¼˜åŒ–å¤±è´¥: {e}", exc_info=True)
        return False

def parse_args():
    parser = argparse.ArgumentParser(description="Layer3 ä¼˜åŒ–è„šæœ¬")
    parser.add_argument("--trials", type=int, default=50,
                        help="æ¯ä¸ªæ¨¡å‹çš„ trials æ•°ï¼ˆé»˜è®¤ 50ï¼‰")
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80)
    logger.info("Layer3 äº”æ¨¡å‹é›†æˆä¼˜åŒ–å¯åŠ¨å™¨")
    logger.info("="*80)
    logger.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"å·¥ä½œç›®å½•: {Path.cwd()}")
    logger.info("")

    args = parse_args()
    trials_per_model = args.trials
    
    # æ­¥éª¤1: æ£€æŸ¥å‰ç½®æ¡ä»¶
    ready, labels_file, features_file = check_prerequisites()
    if not ready:
        logger.error("å‰ç½®æ¡ä»¶ä¸æ»¡è¶³ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    # æ­¥éª¤2: å‡†å¤‡é…ç½®æ–‡ä»¶
    prepare_config_files(labels_file, features_file)
    
    # æ­¥éª¤3: è¿è¡Œä¼˜åŒ–
    logger.info(f"æœ¬æ¬¡ä½¿ç”¨æ¯æ¨¡å‹ trials = {trials_per_model}ï¼Œçº¦ç­‰äºæ€»è®¡ {trials_per_model * len(['lightgbm','xgboost','catboost','randomforest','extratrees'])} trials")
    success = run_layer3_optimization(n_trials_per_model=trials_per_model)
    
    if success:
        logger.info("\n" + "="*80)
        logger.info("âœ… æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼")
        logger.info("="*80)
        logger.info("\nä¸‹ä¸€æ­¥:")
        logger.info("1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†ä¿¡æ¯")
        logger.info("2. æ£€æŸ¥optuna_system/results/BTCUSDT_15m/ç›®å½•ä¸‹çš„é¢„æµ‹æ–‡ä»¶")
        logger.info("3. è¿è¡ŒLayer6é›†æˆä¼˜åŒ–: python run_9layers_optimization.py")
    else:
        logger.error("\n" + "="*80)
        logger.error("âŒ ä¼˜åŒ–è¿‡ç¨‹å‡ºç°é”™è¯¯")
        logger.error("="*80)
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

