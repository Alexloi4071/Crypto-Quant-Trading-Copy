#!/usr/bin/env python3
"""
A/B æ¸¬è©¦ç³»çµ±
å°‡å€‹æ€§åŒ–å„ªåŒ–é…ç½®èˆ‡çµ±ä¸€åŸºæº–æ¯”è¼ƒ
"""

import asyncio
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import sys

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

from src.optimization.advanced_optuna_optimizer import AdvancedOptunaOptimizer
from src.config.config_manager import ConfigManager


@dataclass
class ABTestResult:
    """A/B æ¸¬è©¦çµæœæ•¸æ“šé¡"""
    symbol: str
    timeframe: str
    config_type: str  # 'personalized' æˆ– 'baseline'
    best_score: float
    avg_score: float
    score_std: float
    consistency_score: float
    trial_count: int
    optimization_time: float
    best_params: Dict[str, Any]
    

class ABTester:
    """
    A/B æ¸¬è©¦ç³»çµ±
    æ¯”è¼ƒå€‹æ€§åŒ–é…ç½®é©…å‹•å„ªåŒ– vs çµ±ä¸€åŸºæº–é…ç½®
    """
    
    # çµ±ä¸€åŸºæº–é…ç½®
    BASELINE_CONFIG = {
        'label_lag_range': (1, 10),
        'label_threshold_range': (0.01, 0.05),
        'n_features_range': (20, 50),
        'cv_folds': 5,
        'trials_config': {
            'layer1_total': 100,
            'layer2_total': 80,
            'stage1_trials': 60,
            'stage2_trials': 40
        },
        'model_param_ranges': {
            'num_leaves': (20, 100),
            'max_depth': (4, 15),
            'learning_rate': (0.01, 0.3),
            'n_estimators': (100, 1000),
            'reg_alpha': (0.0, 1.0),
            'reg_lambda': (0.0, 1.0),
            'feature_fraction': (0.6, 1.0),
            'bagging_fraction': (0.6, 1.0),
            'bagging_freq': (1, 7),
            'min_child_samples': (5, 100)
        }
    }
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.results_dir = Path("results/ab_testing")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ§ª A/B æ¸¬è©¦ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ çµæœç›®éŒ„: {self.results_dir}")
    
    async def run_ab_test(self, symbol: str, timeframe: str, 
                         n_trials: int = 100) -> Dict[str, ABTestResult]:
        """
        é‹è¡Œå–®å€‹ symbol-timeframe çš„ A/B æ¸¬è©¦
        
        Args:
            symbol: äº¤æ˜“å°
            timeframe: æ™‚æ¡†
            n_trials: æ¯å€‹é…ç½®çš„è©¦é©—æ¬¡æ•¸
            
        Returns:
            A/B æ¸¬è©¦çµæœå°æ¯”
        """
        print(f"\nğŸ§ª é–‹å§‹ A/B æ¸¬è©¦: {symbol} {timeframe}")
        print("=" * 80)
        
        results = {}
        
        try:
            # Açµ„ï¼šå€‹æ€§åŒ–é…ç½®é©…å‹•å„ªåŒ–
            print("ğŸ…°ï¸ æ¸¬è©¦çµ„A: å€‹æ€§åŒ–é…ç½®é©…å‹•å„ªåŒ–")
            personalized_result = await self._run_personalized_optimization(
                symbol, timeframe, n_trials
            )
            results['personalized'] = personalized_result
            
            # Bçµ„ï¼šçµ±ä¸€åŸºæº–é…ç½®å„ªåŒ–
            print("\nğŸ…±ï¸ æ¸¬è©¦çµ„B: çµ±ä¸€åŸºæº–é…ç½®å„ªåŒ–")  
            baseline_result = await self._run_baseline_optimization(
                symbol, timeframe, n_trials
            )
            results['baseline'] = baseline_result
            
            # ç”Ÿæˆå°æ¯”å ±å‘Š
            self._generate_comparison_report(results, symbol, timeframe)
            
            return results
            
        except Exception as e:
            print(f"âŒ A/B æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    async def _run_personalized_optimization(self, symbol: str, timeframe: str, 
                                           n_trials: int) -> ABTestResult:
        """é‹è¡Œå€‹æ€§åŒ–é…ç½®é©…å‹•å„ªåŒ–"""
        start_time = datetime.now()
        
        # ä½¿ç”¨é…ç½®é©…å‹•çš„å„ªåŒ–å™¨
        optimizer = AdvancedOptunaOptimizer(symbol, timeframe)
        
        print(f"ğŸ“Š å€‹æ€§åŒ–é…ç½®åƒæ•¸:")
        print(f"   æ»¯å¾ŒæœŸç¯„åœ: {optimizer.get_label_lag_range()}")
        print(f"   é–¾å€¼ç¯„åœ: {optimizer.get_label_threshold_range()}")
        print(f"   ç‰¹å¾µæ•¸ç¯„åœ: {optimizer.get_feature_count_range()}")
        print(f"   CVæŠ˜æ•¸: {optimizer.get_cv_folds()}")
        
        # é‹è¡Œå„ªåŒ–
        results = await optimizer.run_complete_optimization()
        
        end_time = datetime.now()
        optimization_time = (end_time - start_time).total_seconds()
        
        # æå–çµæœ
        layer1_results = results.get('layer1', {})
        layer2_results = results.get('layer2', {})
        
        return ABTestResult(
            symbol=symbol,
            timeframe=timeframe,
            config_type='personalized',
            best_score=layer2_results.get('best_score', 0.0),
            avg_score=layer1_results.get('cv_scores', [0.0])[0] if layer1_results.get('cv_scores') else 0.0,
            score_std=np.std(layer1_results.get('cv_scores', [0.0])),
            consistency_score=-np.std(layer1_results.get('cv_scores', [0.0])),
            trial_count=layer1_results.get('total_trials', n_trials),
            optimization_time=optimization_time,
            best_params={**layer1_results.get('best_params', {}), 
                        **layer2_results.get('best_params', {})}
        )
    
    async def _run_baseline_optimization(self, symbol: str, timeframe: str, 
                                       n_trials: int) -> ABTestResult:
        """é‹è¡Œçµ±ä¸€åŸºæº–é…ç½®å„ªåŒ–"""
        start_time = datetime.now()
        
        # å‰µå»ºä½¿ç”¨åŸºæº–é…ç½®çš„å„ªåŒ–å™¨
        optimizer = BaselineOptimizer(symbol, timeframe, self.BASELINE_CONFIG)
        
        print(f"ğŸ“Š çµ±ä¸€åŸºæº–é…ç½®åƒæ•¸:")
        print(f"   æ»¯å¾ŒæœŸç¯„åœ: {self.BASELINE_CONFIG['label_lag_range']}")
        print(f"   é–¾å€¼ç¯„åœ: {self.BASELINE_CONFIG['label_threshold_range']}")
        print(f"   ç‰¹å¾µæ•¸ç¯„åœ: {self.BASELINE_CONFIG['n_features_range']}")
        print(f"   CVæŠ˜æ•¸: {self.BASELINE_CONFIG['cv_folds']}")
        
        # é‹è¡Œå„ªåŒ–
        results = await optimizer.run_baseline_optimization(n_trials)
        
        end_time = datetime.now()
        optimization_time = (end_time - start_time).total_seconds()
        
        return ABTestResult(
            symbol=symbol,
            timeframe=timeframe,
            config_type='baseline',
            best_score=results.get('best_score', 0.0),
            avg_score=results.get('avg_cv_score', 0.0),
            score_std=results.get('score_std', 0.0),
            consistency_score=-results.get('score_std', 0.0),
            trial_count=results.get('trial_count', n_trials),
            optimization_time=optimization_time,
            best_params=results.get('best_params', {})
        )
    
    def _generate_comparison_report(self, results: Dict[str, ABTestResult], 
                                   symbol: str, timeframe: str):
        """ç”Ÿæˆè©³ç´°çš„å°æ¯”å ±å‘Š"""
        personalized = results.get('personalized')
        baseline = results.get('baseline')
        
        if not personalized or not baseline:
            print("âš ï¸ çµæœä¸å®Œæ•´ï¼Œç„¡æ³•ç”Ÿæˆå°æ¯”å ±å‘Š")
            return
        
        print(f"\nğŸ“Š A/B æ¸¬è©¦å°æ¯”å ±å‘Š: {symbol} {timeframe}")
        print("=" * 80)
        
        # æ€§èƒ½å°æ¯”
        score_improvement = personalized.best_score - baseline.best_score
        score_improvement_pct = (score_improvement / baseline.best_score * 100) if baseline.best_score > 0 else 0
        
        consistency_improvement = personalized.consistency_score - baseline.consistency_score
        
        print(f"ğŸ¯ **æ€§èƒ½æŒ‡æ¨™å°æ¯”**")
        print(f"   å€‹æ€§åŒ–æœ€ä½³åˆ†æ•¸: {personalized.best_score:.6f}")
        print(f"   åŸºæº–æœ€ä½³åˆ†æ•¸: {baseline.best_score:.6f}")
        print(f"   åˆ†æ•¸æå‡: {score_improvement:+.6f} ({score_improvement_pct:+.2f}%)")
        
        print(f"\nğŸ“ˆ **ä¸€è‡´æ€§å°æ¯”**")
        print(f"   å€‹æ€§åŒ–ä¸€è‡´æ€§: {personalized.consistency_score:.6f}")
        print(f"   åŸºæº–ä¸€è‡´æ€§: {baseline.consistency_score:.6f}")
        print(f"   ä¸€è‡´æ€§æå‡: {consistency_improvement:+.6f}")
        
        print(f"\nâ±ï¸ **æ•ˆç‡å°æ¯”**")
        print(f"   å€‹æ€§åŒ–å„ªåŒ–æ™‚é–“: {personalized.optimization_time:.1f} ç§’")
        print(f"   åŸºæº–å„ªåŒ–æ™‚é–“: {baseline.optimization_time:.1f} ç§’")
        print(f"   æ™‚é–“å·®ç•°: {personalized.optimization_time - baseline.optimization_time:+.1f} ç§’")
        
        # çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ
        print(f"\nğŸ”¬ **çµ±è¨ˆåˆ†æ**")
        
        if score_improvement_pct > 5:
            significance = "ğŸš€ é¡¯è‘—å„ªå‹¢"
        elif score_improvement_pct > 2:
            significance = "âœ… æ˜é¡¯æå‡" 
        elif score_improvement_pct > 0:
            significance = "ğŸ“ˆ è¼•å¾®æå‡"
        else:
            significance = "âŒ ç„¡æ˜é¡¯å„ªå‹¢"
        
        print(f"   çµè«–: {significance}")
        print(f"   ç½®ä¿¡åº¦: {self._calculate_confidence(personalized, baseline):.1f}%")
        
        # ä¿å­˜è©³ç´°çµæœ
        self._save_ab_test_results(results, symbol, timeframe)
    
    def _calculate_confidence(self, personalized: ABTestResult, 
                             baseline: ABTestResult) -> float:
        """è¨ˆç®—çµ±è¨ˆç½®ä¿¡åº¦ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        # ç°¡åŒ–çš„ç½®ä¿¡åº¦è¨ˆç®—ï¼ŒåŸºæ–¼åˆ†æ•¸å·®ç•°å’Œæ¨™æº–å·®
        score_diff = abs(personalized.best_score - baseline.best_score)
        combined_std = (personalized.score_std + baseline.score_std) / 2
        
        if combined_std == 0:
            return 99.9 if score_diff > 0 else 50.0
            
        # ç°¡åŒ–çš„ t-test è¿‘ä¼¼
        t_stat = score_diff / combined_std
        confidence = min(95.0 + t_stat * 10, 99.9)  # ç°¡åŒ–æ˜ å°„
        
        return confidence
    
    def _save_ab_test_results(self, results: Dict[str, ABTestResult], 
                             symbol: str, timeframe: str):
        """ä¿å­˜ A/B æ¸¬è©¦çµæœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ab_test_{symbol}_{timeframe}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        # åºåˆ—åŒ–çµæœ
        serializable_results = {}
        for key, result in results.items():
            serializable_results[key] = {
                'symbol': result.symbol,
                'timeframe': result.timeframe,
                'config_type': result.config_type,
                'best_score': result.best_score,
                'avg_score': result.avg_score,
                'score_std': result.score_std,
                'consistency_score': result.consistency_score,
                'trial_count': result.trial_count,
                'optimization_time': result.optimization_time,
                'best_params': result.best_params
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ A/B æ¸¬è©¦çµæœå·²ä¿å­˜: {filepath}")
    
    async def run_batch_ab_test(self, symbol_timeframe_pairs: List[Tuple[str, str]], 
                               n_trials: int = 100) -> Dict[str, Dict[str, ABTestResult]]:
        """æ‰¹é‡é‹è¡Œå¤šå€‹ A/B æ¸¬è©¦"""
        print(f"ğŸ§ª é–‹å§‹æ‰¹é‡ A/B æ¸¬è©¦: {len(symbol_timeframe_pairs)} å€‹çµ„åˆ")
        
        all_results = {}
        
        for symbol, timeframe in symbol_timeframe_pairs:
            test_key = f"{symbol}_{timeframe}"
            print(f"\nğŸ”„ è™•ç† {test_key}...")
            
            try:
                results = await self.run_ab_test(symbol, timeframe, n_trials)
                all_results[test_key] = results
                print(f"âœ… {test_key} å®Œæˆ")
            except Exception as e:
                print(f"âŒ {test_key} å¤±æ•—: {e}")
                all_results[test_key] = {}
        
        # ç”Ÿæˆæ‰¹é‡ç¸½çµå ±å‘Š
        self._generate_batch_summary(all_results)
        
        return all_results
    
    def _generate_batch_summary(self, all_results: Dict[str, Dict[str, ABTestResult]]):
        """ç”Ÿæˆæ‰¹é‡æ¸¬è©¦ç¸½çµå ±å‘Š"""
        print(f"\nğŸ“Š æ‰¹é‡ A/B æ¸¬è©¦ç¸½çµå ±å‘Š")
        print("=" * 100)
        
        wins = 0
        total = 0
        total_improvement = 0
        
        print(f"{'çµ„åˆ':15} {'å€‹æ€§åŒ–åˆ†æ•¸':12} {'åŸºæº–åˆ†æ•¸':12} {'æå‡å¹…åº¦':12} {'çµè«–':15}")
        print("-" * 80)
        
        for test_key, results in all_results.items():
            if not results:
                continue
                
            personalized = results.get('personalized')
            baseline = results.get('baseline')
            
            if personalized and baseline:
                improvement = personalized.best_score - baseline.best_score
                improvement_pct = (improvement / baseline.best_score * 100) if baseline.best_score > 0 else 0
                total_improvement += improvement_pct
                total += 1
                
                if improvement > 0:
                    wins += 1
                    conclusion = "âœ… å€‹æ€§åŒ–å‹å‡º"
                else:
                    conclusion = "âŒ åŸºæº–å‹å‡º"
                
                print(f"{test_key:15} {personalized.best_score:.6f}   {baseline.best_score:.6f}   "
                      f"{improvement_pct:+8.2f}%    {conclusion}")
        
        print("-" * 80)
        if total > 0:
            win_rate = wins / total * 100
            avg_improvement = total_improvement / total
            print(f"ç¸½çµ: {wins}/{total} å‹å‡º (å‹ç‡: {win_rate:.1f}%)")
            print(f"å¹³å‡æå‡: {avg_improvement:+.2f}%")
        
        print("=" * 100)


class BaselineOptimizer:
    """åŸºæº–é…ç½®å„ªåŒ–å™¨"""
    
    def __init__(self, symbol: str, timeframe: str, baseline_config: Dict):
        self.symbol = symbol
        self.timeframe = timeframe
        self.config = baseline_config
        self.optimizer = AdvancedOptunaOptimizer(symbol, timeframe)
        
        # å¼·åˆ¶è¦†è“‹é…ç½®ç‚ºåŸºæº–é…ç½®
        self._override_config()
    
    def _override_config(self):
        """è¦†è“‹å„ªåŒ–å™¨é…ç½®ç‚ºåŸºæº–é…ç½®"""
        # é€™è£¡éœ€è¦è‡¨æ™‚è¦†è“‹é…ç½®æ–¹æ³•
        self.optimizer.strategy_config = self.config
        
    async def run_baseline_optimization(self, n_trials: int) -> Dict[str, Any]:
        """ä½¿ç”¨åŸºæº–é…ç½®é‹è¡Œå„ªåŒ–"""
        # ç›´æ¥èª¿ç”¨å„ªåŒ–å™¨çš„å®Œæ•´å„ªåŒ–ï¼Œä½†ä½¿ç”¨åŸºæº–é…ç½®
        results = await self.optimizer.run_complete_optimization()
        
        # è¨ˆç®—é¡å¤–çš„çµ±è¨ˆä¿¡æ¯
        layer1_results = results.get('layer1', {})
        cv_scores = layer1_results.get('cv_scores', [])
        
        return {
            'best_score': results.get('layer2', {}).get('best_score', 0.0),
            'avg_cv_score': np.mean(cv_scores) if cv_scores else 0.0,
            'score_std': np.std(cv_scores) if cv_scores else 0.0,
            'trial_count': n_trials,
            'best_params': {**layer1_results.get('best_params', {}),
                          **results.get('layer2', {}).get('best_params', {})}
        }


# ä½¿ç”¨ç¯„ä¾‹å’Œæ¸¬è©¦å‡½æ•¸
async def test_ab_system():
    """æ¸¬è©¦ A/B ç³»çµ±"""
    tester = ABTester()
    
    # å–®å€‹æ¸¬è©¦
    results = await tester.run_ab_test('BTCUSDT', '1h', n_trials=50)
    
    # æ‰¹é‡æ¸¬è©¦
    pairs = [('BTCUSDT', '1h'), ('ETHUSDT', '1h')]
    batch_results = await tester.run_batch_ab_test(pairs, n_trials=30)
    
    return results, batch_results


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    results, batch_results = asyncio.run(test_ab_system())
    print("A/B æ¸¬è©¦ç³»çµ±æ¸¬è©¦å®Œæˆ")
