#!/usr/bin/env python3
"""
ä¸»å„ªåŒ–å™¨ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒ ML æµç¨‹ç®¡ç†
åŸ Optuna å„ªåŒ–åŠŸèƒ½å·²é·ç§»è‡³ optuna_system/

è² è²¬ï¼š
- çµ±ä¸€ç®¡ç†æ¨™ç±¤ç”Ÿæˆã€ç‰¹å¾µé¸æ“‡ã€æ¨¡å‹è¨“ç·´æµç¨‹
- æ•¸æ“šé è™•ç†å’Œå¾Œè™•ç†
- çµæœä¿å­˜å’Œå ±å‘Šç”Ÿæˆ
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

try:
    from .config import get_config
    from .label_optimizer import LabelGenerator
    from .feature_selector import FeatureSelector
    from .model_optimizer import ModelTrainer
except ImportError:
    from config import get_config
    from label_optimizer import LabelGenerator
    from feature_selector import FeatureSelector
    from model_optimizer import ModelTrainer


class MLPipelineManager:
    """ML æµç¨‹ç®¡ç†å™¨ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒæµç¨‹"""

    def __init__(self, symbol: str, timeframe: str, version: str = None):
        self.symbol = symbol
        self.timeframe = timeframe
        self.version = version or f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ğŸš€ åˆå§‹åŒ– ML æµç¨‹ç®¡ç†å™¨ - {symbol} {timeframe}")
        print(f"ğŸ“Š ç‰ˆæœ¬: {self.version}")
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.label_generator = LabelGenerator(symbol, timeframe)
        self.feature_selector = FeatureSelector(symbol, timeframe)
        self.model_trainer = ModelTrainer(symbol, timeframe)
        
        # è¨­ç½®æ•¸æ“šè·¯å¾‘
        self.setup_data_paths()
        
        # çµæœå­˜å„²
        self.results = {}

    def setup_data_paths(self):
        """è¨­ç½®æ•¸æ“šè·¯å¾‘"""
        self.base_dir = Path("data")
        self.raw_dir = self.base_dir / "raw" / self.symbol
        self.processed_dir = self.base_dir / "processed"
        self.features_dir = self.processed_dir / "features" / f"{self.symbol}_{self.timeframe}" / self.version
        self.labels_dir = self.processed_dir / "labels" / f"{self.symbol}_{self.timeframe}" / self.version
        self.models_dir = Path("results") / "models" / f"{self.symbol}_{self.timeframe}_{self.version}"
        
        # å‰µå»ºç›®éŒ„
        for dir_path in [self.features_dir, self.labels_dir, self.models_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def load_ohlcv_data(self) -> pd.DataFrame:
        """åŠ è¼‰ OHLCV æ•¸æ“š"""
        try:
            ohlcv_file = self.raw_dir / f"{self.symbol}_{self.timeframe}_ohlcv.parquet"
            
            if not ohlcv_file.exists():
                raise FileNotFoundError(f"OHLCV æ–‡ä»¶ä¸å­˜åœ¨: {ohlcv_file}")
            
            print(f"ğŸ“Š åŠ è¼‰ OHLCV æ•¸æ“š: {ohlcv_file}")
            data = pd.read_parquet(ohlcv_file)
            
            # ç¢ºä¿ç´¢å¼•æ˜¯æ™‚é–“æˆ³
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index)
            
            print(f"âœ… OHLCV æ•¸æ“šåŠ è¼‰å®Œæˆ: {len(data)} æ¢è¨˜éŒ„")
            
            return data
            
        except Exception as e:
            print(f"âŒ OHLCV æ•¸æ“šåŠ è¼‰å¤±æ•—: {e}")
            raise e

    def generate_features_from_raw(self, ohlcv_data: pd.DataFrame = None) -> pd.DataFrame:
        """å¾åŸå§‹æ•¸æ“šç”ŸæˆæŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ"""
        try:
            if ohlcv_data is None:
                ohlcv_data = self.load_ohlcv_data()
            
            print(f"ğŸ”§ ç”ŸæˆæŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ...")
            
            # åŸºæœ¬åƒ¹æ ¼ç‰¹å¾µ
            features = pd.DataFrame(index=ohlcv_data.index)
            
            # åƒ¹æ ¼è®ŠåŒ–ç‡
            features['returns'] = ohlcv_data['close'].pct_change()
            features['high_low_ratio'] = ohlcv_data['high'] / ohlcv_data['low']
            features['open_close_ratio'] = ohlcv_data['open'] / ohlcv_data['close']
            
            # ç§»å‹•å¹³å‡ç·š
            for window in [5, 10, 20, 50]:
                features[f'sma_{window}'] = ohlcv_data['close'].rolling(window).mean()
                features[f'ema_{window}'] = ohlcv_data['close'].ewm(span=window).mean()
                features[f'price_sma_{window}_ratio'] = ohlcv_data['close'] / features[f'sma_{window}']
            
            # å¸ƒæ—å¸¶
            for window in [20]:
                sma = ohlcv_data['close'].rolling(window).mean()
                std = ohlcv_data['close'].rolling(window).std()
                features[f'bb_upper_{window}'] = sma + (2 * std)
                features[f'bb_lower_{window}'] = sma - (2 * std)
                features[f'bb_width_{window}'] = (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}']) / sma
                features[f'bb_position_{window}'] = (ohlcv_data['close'] - features[f'bb_lower_{window}']) / (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}'])
            
            # RSI
            for window in [14, 21]:
                delta = ohlcv_data['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
                rs = gain / loss
                features[f'rsi_{window}'] = 100 - (100 / (1 + rs))
            
            # æˆäº¤é‡ç‰¹å¾µ
            features['volume_sma_ratio'] = ohlcv_data['volume'] / ohlcv_data['volume'].rolling(20).mean()
            features['price_volume'] = ohlcv_data['close'] * ohlcv_data['volume']
            
            # æ³¢å‹•ç‡
            for window in [10, 20]:
                features[f'volatility_{window}'] = ohlcv_data['close'].pct_change().rolling(window).std()
            
            # ç§»é™¤ç„¡æ•ˆå€¼
            features = features.fillna(method='bfill').fillna(method='ffill')
            features = features.replace([np.inf, -np.inf], np.nan).fillna(0)
            
            print(f"âœ… æŠ€è¡“æŒ‡æ¨™ç”Ÿæˆå®Œæˆ: {features.shape[1]} å€‹ç‰¹å¾µ")
            
            # ä¿å­˜ç‰¹å¾µ
            features_file = self.features_dir / f"{self.symbol}_{self.timeframe}_features_full.parquet"
            features.to_parquet(features_file)
            print(f"ğŸ’¾ ç‰¹å¾µå·²ä¿å­˜: {features_file}")
            
            return features
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")
            raise e

    def run_complete_pipeline(self, 
                            label_params: Dict[str, Any] = None,
                            feature_params: Dict[str, Any] = None,
                            model_params: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        é‹è¡Œå®Œæ•´çš„ ML æµç¨‹
        
        Args:
            label_params: æ¨™ç±¤ç”Ÿæˆåƒæ•¸
            feature_params: ç‰¹å¾µé¸æ“‡åƒæ•¸
            model_params: æ¨¡å‹è¨“ç·´åƒæ•¸
        
        Returns:
            å®Œæ•´çµæœå­—å…¸
        """
        try:
            print(f"ğŸš€ é–‹å§‹å®Œæ•´ ML æµç¨‹ - {self.symbol} {self.timeframe}")
            
            # 1. åŠ è¼‰æ•¸æ“š
            ohlcv_data = self.load_ohlcv_data()
            
            # 2. ç”Ÿæˆç‰¹å¾µ
            print(f"\nğŸ“ æ­¥é©Ÿ 1: ç”Ÿæˆç‰¹å¾µ")
            features = self.generate_features_from_raw(ohlcv_data)
            
            # 3. ç”Ÿæˆæ¨™ç±¤
            print(f"\nğŸ“ æ­¥é©Ÿ 2: ç”Ÿæˆæ¨™ç±¤")
            default_label_params = {
                'lag': 3,
                'profit_threshold': 0.005,
                'loss_threshold': -0.005,
                'label_type': 'ternary',
                'threshold_method': 'fixed'
            }
            
            if label_params:
                default_label_params.update(label_params)
            
            labels = self.label_generator.generate_labels(
                price_data=ohlcv_data['close'],
                **default_label_params
            )
            
            # ä¿å­˜æ¨™ç±¤
            labels_file = self.labels_dir / f"{self.symbol}_{self.timeframe}_labels.parquet"
            labels.to_frame('label').to_parquet(labels_file)
            
            # 4. æ•¸æ“šå°é½Š
            print(f"\nğŸ“ æ­¥é©Ÿ 3: æ•¸æ“šå°é½Š")
            common_index = features.index.intersection(labels.index)
            X = features.loc[common_index]
            y = labels.loc[common_index]
            
            print(f"ğŸ“Š å°é½Šå¾Œæ•¸æ“š: {len(X)} æ¢è¨˜éŒ„, {X.shape[1]} å€‹ç‰¹å¾µ")
            
            # 5. ç‰¹å¾µé¸æ“‡
            print(f"\nğŸ“ æ­¥é©Ÿ 4: ç‰¹å¾µé¸æ“‡")
            default_feature_params = {
                'n_features': 20,
                'method': 'lgb'
            }
            
            if feature_params:
                default_feature_params.update(feature_params)
            
            # ç›¸é—œæ€§éæ¿¾
            X_filtered = self.feature_selector.remove_correlated_features(X, threshold=0.95)
            
            # ç‰¹å¾µé¸æ“‡
            selected_features = self.feature_selector.select_features_by_importance(
                X_filtered, y, **default_feature_params
            )
            
            X_selected = X_filtered[selected_features]
            
            # ä¿å­˜é¸ä¸­çš„ç‰¹å¾µ
            selected_features_file = self.features_dir / f"{self.symbol}_{self.timeframe}_selected_features.parquet"
            X_selected.to_parquet(selected_features_file)
            
            # ä¿å­˜ç‰¹å¾µé¸æ“‡åƒæ•¸
            feature_selection_params = {
                'selected_features': selected_features,
                'n_features': len(selected_features),
                'selection_method': default_feature_params['method'],
                'correlation_threshold': 0.95
            }
            
            params_file = self.features_dir / f"{self.symbol}_{self.timeframe}_feature_selection_params.json"
            with open(params_file, 'w', encoding='utf-8') as f:
                json.dump(feature_selection_params, f, indent=2, ensure_ascii=False)
            
            # 6. æ¨¡å‹è¨“ç·´
            print(f"\nğŸ“ æ­¥é©Ÿ 5: æ¨¡å‹è¨“ç·´")
            
            model, metrics = self.model_trainer.train_with_validation(
                X_selected, y, model_params=model_params
            )
            
            # 7. ä¿å­˜æ¨¡å‹
            print(f"\nğŸ“ æ­¥é©Ÿ 6: ä¿å­˜çµæœ")
            
            model_path = self.model_trainer.save_model(
                model=model,
                save_dir=str(self.models_dir),
                model_params=model_params,
                feature_names=selected_features,
                metrics=metrics
            )
            
            # 8. ç”Ÿæˆå ±å‘Š
            label_report = self.label_generator.generate_report(y)
            feature_report = self.feature_selector.get_feature_importance_report(X_selected, y, selected_features)
            model_report = self.model_trainer.generate_report(model, metrics, selected_features)
            
            # 9. æ•´åˆçµæœ
            results = {
                'version': self.version,
                'symbol': self.symbol,
                'timeframe': self.timeframe,
                'data_info': {
                    'total_samples': len(X),
                    'feature_count': len(selected_features),
                    'selected_features': selected_features
                },
                'label_params': default_label_params,
                'feature_params': default_feature_params,
                'model_params': model_params or self.model_trainer._get_default_params(),
                'metrics': metrics,
                'model_path': model_path,
                'reports': {
                    'labels': label_report,
                    'features': feature_report,
                    'model': model_report
                }
            }
            
            # ä¿å­˜å®Œæ•´çµæœ
            results_file = self.models_dir / "pipeline_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"âœ… å®Œæ•´ ML æµç¨‹å®Œæˆ!")
            print(f"ğŸ“Š æœ€çµ‚ F1 åˆ†æ•¸: {metrics.get('f1', 0):.4f}")
            print(f"ğŸ’¾ çµæœå·²ä¿å­˜: {self.models_dir}")
            
            self.results = results
            return results
            
        except Exception as e:
            print(f"âŒ ML æµç¨‹å¤±æ•—: {e}")
            raise e

    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæµç¨‹ç¸½çµå ±å‘Š"""
        if not self.results:
            return "å°šæœªé‹è¡Œå®Œæ•´æµç¨‹"
        
        results = self.results
        
        report = f"""
ğŸ¯ ML æµç¨‹ç¸½çµå ±å‘Š
{'='*60}
ğŸ“Š åŸºæœ¬ä¿¡æ¯:
â”œâ”€ äº¤æ˜“å°: {results['symbol']}
â”œâ”€ æ™‚é–“æ¡†æ¶: {results['timeframe']}
â”œâ”€ ç‰ˆæœ¬: {results['version']}
â””â”€ é‹è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ˆ æ•¸æ“šçµ±è¨ˆ:
â”œâ”€ ç¸½æ¨£æœ¬æ•¸: {results['data_info']['total_samples']:,}
â”œâ”€ ç‰¹å¾µæ•¸é‡: {results['data_info']['feature_count']}
â””â”€ æ¨™ç±¤é¡å‹: {results['label_params']['label_type']}

ğŸ¯ æ€§èƒ½æŒ‡æ¨™:"""

        for metric, value in results['metrics'].items():
            report += f"\nâ”œâ”€ {metric.upper()}: {value:.4f}"

        report += f"""

ğŸ’¾ è¼¸å‡ºæ–‡ä»¶:
â”œâ”€ æ¨¡å‹: {results['model_path']}
â”œâ”€ ç‰¹å¾µ: {self.features_dir}
â””â”€ æ¨™ç±¤: {self.labels_dir}

ğŸ”§ åƒæ•¸è¨­ç½®:
â”œâ”€ æ¨™ç±¤åƒæ•¸: {results['label_params']}
â”œâ”€ ç‰¹å¾µåƒæ•¸: {results['feature_params']}
â””â”€ æ¨¡å‹åƒæ•¸æ•¸é‡: {len(results['model_params'])}å€‹
"""

        return report


# å‘å¾Œå…¼å®¹æ€§
ModularOptunaOptimizer = MLPipelineManager  # åˆ¥åï¼Œä¿æŒå‘å¾Œå…¼å®¹
