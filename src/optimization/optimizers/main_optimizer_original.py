#!/usr/bin/env python3
"""
ä¸»æ§åˆ¶å™¨ - æ¨¡çµ„åŒ–Optunaå„ªåŒ–ç³»çµ±

çµ±ä¸€ç®¡ç†æ¨™ç±¤å„ªåŒ–ã€ç‰¹å¾µé¸æ“‡ã€æ¨¡å‹è¶…åƒæ•¸å„ªåŒ–ä¸‰å€‹æ¨¡çµ„
å¯¦ç¾ç«¯åˆ°ç«¯çš„è‡ªå‹•åŒ–å„ªåŒ–æµç¨‹ï¼Œæ”¯æŒæ–¹æ¡ˆBåˆ°æ–¹æ¡ˆCçš„å¹³æ»‘å‡ç´š
å¢å¼·å ±å‘ŠåŠŸèƒ½èˆ‡éåº¦æ“¬åˆæª¢æ¸¬
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import lightgbm as lgb
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import f1_score
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

# å°å…¥æ¨¡çµ„
try:
    from .config import OptimizationConfig, get_config
    from .label_optimizer import LabelOptimizer
    from .feature_selector import FeatureSelector
    from .model_optimizer import ModelOptimizer
except ImportError:
    # è™•ç†ç›¸å°å°å…¥å•é¡Œ
    from config import OptimizationConfig, get_config
    from label_optimizer import LabelOptimizer
    from feature_selector import FeatureSelector
    from model_optimizer import ModelOptimizer

# ğŸ”§ ç§»é™¤æ—§ç³»ç»Ÿå¯¼å…¥ - ä½¿ç”¨æ–°çš„optimizationæ¨¡å—


class ModularOptunaOptimizer:
    """æ¨¡çµ„åŒ–Optunaå„ªåŒ–ç³»çµ±ä¸»æ§åˆ¶å™¨"""

    def __init__(self, symbol: str, timeframe: str, version: str = None, auto_version: bool = True, 
                 use_saved_params: bool = True):
        self.symbol = symbol
        self.timeframe = timeframe
        
        # ğŸ”¢ ç‰ˆæœ¬ç®¡ç†
        if version:
            self.version = version
        elif auto_version:
            self.version = self.get_next_version()
        else:
            self.version = "v1"
        
        # åˆå§‹åŒ–é…ç½®
        self.config = OptimizationConfig()
        
        # ğŸ”§ è¼‰å…¥å·²ä¿å­˜çš„Optunaæœ€ä½³åƒæ•¸ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.saved_params = {}
        if use_saved_params:
            self.saved_params = self._load_saved_optuna_params()
            if self.saved_params:
                print(f"ğŸ”§ è¼‰å…¥å·²ä¿å­˜çš„Optunaåƒæ•¸: {len(self.saved_params)}å€‹çµ„ä»¶")
                for component, params in self.saved_params.items():
                    print(f"   - {component}: {len(params)}å€‹åƒæ•¸")
        
        # é©—è­‰é…ç½®
        if not self.config.validate_config(symbol, timeframe):
            print("âš ï¸ é…ç½®é©—è­‰å¤±æ•—ï¼Œä½¿ç”¨é»˜èªé…ç½®")
        
        # å‰µå»ºç›®éŒ„çµæ§‹
        self.config.create_directories()
        
        # ğŸ”§ ç§»é™¤æ—§ç³»ç»Ÿåˆå§‹åŒ– - ä¼˜åŒ–æ¨¡å—å·²è¶³å¤Ÿ
        
        # åˆå§‹åŒ–å„ªåŒ–æ¨¡çµ„ï¼ˆä½¿ç”¨å·²ä¿å­˜çš„åƒæ•¸ï¼‰
        label_params = self.saved_params.get('labels', {})
        feature_params = self.saved_params.get('features', {})
        model_params = self.saved_params.get('model', {})
        
        self.label_optimizer = LabelOptimizer(symbol, timeframe, custom_params=label_params)
        self.feature_selector = FeatureSelector(symbol, timeframe, custom_params=feature_params)
        self.model_optimizer = ModelOptimizer(symbol, timeframe, custom_params=model_params)
        
        # çµæœå­˜å„²
        self.optimization_results = {}
        self.execution_log = []
        
        # æ•¸æ“šè·¯å¾‘
        self.setup_data_paths()
        
        print(f"ğŸš€ åˆå§‹åŒ–æ¨¡çµ„åŒ–Optunaå„ªåŒ–å™¨ - {symbol} {timeframe}")
        print(f"ğŸ“Š ç•¶å‰ç‰ˆæœ¬: {self.version}")
        print(f"âœ… å·²è¼‰å…¥æ–°çš„optimizationå„ªåŒ–æ¨¡çµ„")
    
    def _load_saved_optuna_params(self) -> Dict[str, Dict[str, Any]]:
        """è¼‰å…¥å·²ä¿å­˜çš„Optunaæœ€ä½³åƒæ•¸"""
        saved_params = {}
        
        try:
            # åƒæ•¸ç›®éŒ„çµæ§‹: results/optimal_params/SYMBOL/TIMEFRAME/COMPONENT/
            base_dir = f"results/optimal_params/{self.symbol}/{self.timeframe}"
            
            for component in ["labels", "features", "model"]:
                component_dir = f"{base_dir}/{component}"
                latest_file = f"{component_dir}/{component}_latest.json"
                
                if os.path.exists(latest_file):
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        param_data = json.load(f)
                        saved_params[component] = param_data.get('best_params', {})
                        print(f"   âœ… è¼‰å…¥{component}åƒæ•¸: {len(saved_params[component])}å€‹")
                else:
                    print(f"   âš ï¸  {component}åƒæ•¸æª”æ¡ˆä¸å­˜åœ¨: {latest_file}")
            
            return saved_params
            
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥å·²ä¿å­˜åƒæ•¸å¤±æ•—: {e}")
            return {}
    
    def save_optuna_params(self, component: str, params: Dict[str, Any], score: float):
        """ä¿å­˜Optunaæœ€ä½³åƒæ•¸åˆ°çµæ§‹åŒ–ç›®éŒ„"""
        try:
            # å‰µå»ºç›®éŒ„çµæ§‹
            component_dir = f"results/optimal_params/{self.symbol}/{self.timeframe}/{component}"
            os.makedirs(component_dir, exist_ok=True)
            
            # å‰µå»ºåƒæ•¸è¨˜éŒ„
            param_record = {
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "component": component,
                "timestamp": datetime.now().isoformat(),
                "best_params": params,
                "best_score": score,
                "metadata": {"source": "main_optimizer", "version": self.version}
            }
            
            # ä¿å­˜åˆ°latestæª”æ¡ˆ
            latest_file = f"{component_dir}/{component}_latest.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(param_record, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ {component}åƒæ•¸å·²ä¿å­˜: {len(params)}å€‹åƒæ•¸")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜{component}åƒæ•¸å¤±æ•—: {e}")
            return False
    
    def run_labels_optuna_optimization(self, n_trials: int = 200, 
                                      features_df: pd.DataFrame = None, ohlcv_df: pd.DataFrame = None) -> Dict[str, Any]:
        """åŸ·è¡ŒLabelsåƒæ•¸çš„Optunaå„ªåŒ–"""
        print(f"ğŸ¯ é–‹å§‹Labels Optunaå„ªåŒ– - {self.symbol} {self.timeframe}")
        print(f"Trialæ•¸é‡: {n_trials}")
        
        # ğŸ”§ å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œè‡ªå‹•è¼‰å…¥
        if features_df is None or ohlcv_df is None:
            print("ğŸ”§ è‡ªå‹•è¼‰å…¥æ•¸æ“š...")
            features_df, ohlcv_df = self.load_data()
            if features_df is None or ohlcv_df is None:
                print("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
                return {}
        
        try:
            # ä½¿ç”¨LabelOptimizeré€²è¡Œå„ªåŒ–
            price_data = ohlcv_df['close']
            results = self.label_optimizer.optimize(features_df, price_data, n_trials=n_trials)
            
            if results and 'best_params' in results:
                # ä¿å­˜æœ€ä½³åƒæ•¸
                self.save_optuna_params('labels', results['best_params'], results.get('best_score', 0.0))
                
                print(f"âœ… Labelså„ªåŒ–å®Œæˆ")
                print(f"æœ€ä½³åƒæ•¸: {results['best_params']}")
                print(f"æœ€ä½³åˆ†æ•¸: {results.get('best_score', 0.0):.4f}")
                
                return results
            else:
                print("âŒ Labelså„ªåŒ–å¤±æ•—")
                return {}
                
        except Exception as e:
            print(f"âŒ Labelså„ªåŒ–å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def run_models_optuna_optimization(self, n_trials: int = 100,
                                     features_df: pd.DataFrame = None, labels: pd.Series = None,
                                     selected_features: List[str] = None) -> Dict[str, Any]:
        """åŸ·è¡ŒModelsåƒæ•¸çš„Optunaå„ªåŒ–"""
        print(f"ğŸ¤– é–‹å§‹Models Optunaå„ªåŒ– - {self.symbol} {self.timeframe}")
        print(f"Trialæ•¸é‡: {n_trials}")
        
        # ğŸ”§ å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œè‡ªå‹•è¼‰å…¥
        if features_df is None or labels is None:
            print("ğŸ”§ è‡ªå‹•è¼‰å…¥æ•¸æ“š...")
            features_df, ohlcv_df = self.load_data()
            if features_df is None:
                print("âŒ ç‰¹å¾µæ•¸æ“šè¼‰å…¥å¤±æ•—")
                return {}
            
            # ç”Ÿæˆæ¨™ç±¤
            try:
                labels = self.label_optimizer.generate_labels(
                    price_data=ohlcv_df['close'],
                    lag=5,
                    pos_threshold=0.75,
                    neg_threshold=0.25, 
                    label_type="technical_based",
                    threshold_method="quantile"
                )
            except Exception as e:
                print(f"âŒ æ¨™ç±¤ç”Ÿæˆå¤±æ•—: {e}")
                return {}
        
        # å¦‚æœæ²’æœ‰æä¾›é¸å®šç‰¹å¾µï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾µ
        if selected_features is None:
            selected_features = features_df.columns.tolist()
            
        print(f"ç‰¹å¾µæ•¸é‡: {len(selected_features)}")
        
        try:
            # ä½¿ç”¨ModelOptimizeré€²è¡Œå„ªåŒ–ï¼ˆå®Œæ•´é©—è­‰æ¨¡å¼ï¼‰
            X = features_df[selected_features]
            results = self.model_optimizer.optimize(X, labels, n_trials=n_trials, fast_mode=False)
            
            if results and 'best_params' in results:
                # ä¿å­˜æœ€ä½³åƒæ•¸
                self.save_optuna_params('model', results['best_params'], results.get('best_score', 0.0))
                
                print(f"âœ… Modelså„ªåŒ–å®Œæˆ")
                print(f"æœ€ä½³åƒæ•¸: {results['best_params']}")
                print(f"æœ€ä½³åˆ†æ•¸: {results.get('best_score', 0.0):.4f}")
                
                return results
            else:
                print("âŒ Modelså„ªåŒ–å¤±æ•—")
                return {}
                
        except Exception as e:
            print(f"âŒ Modelså„ªåŒ–å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def run_features_optuna_optimization(self, n_trials: int = 100, 
                                        features_df: pd.DataFrame = None, labels: pd.Series = None) -> Dict[str, Any]:
        """ç‚ºFeaturesçµ„ä»¶é‹è¡ŒOptunaå„ªåŒ–ï¼Œæ‰¾å‡ºæœ€ä½³ç‰¹å¾µé¸æ“‡åƒæ•¸"""
        print(f"\nğŸ” é–‹å§‹Featuresçµ„ä»¶Optunaå„ªåŒ– - {self.symbol} {self.timeframe}")
        
        # ğŸ”§ å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œè‡ªå‹•è¼‰å…¥
        if features_df is None or labels is None:
            print("ğŸ”§ è‡ªå‹•è¼‰å…¥æ•¸æ“š...")
            features_df, ohlcv_df = self.load_data()
            if features_df is None:
                print("âŒ ç‰¹å¾µæ•¸æ“šè¼‰å…¥å¤±æ•—")
                return {}
            
            # ç”Ÿæˆæ¨™ç±¤
            try:
                labels = self.label_optimizer.generate_labels(
                    price_data=ohlcv_df['close'],
                    lag=5,
                    pos_threshold=0.75,
                    neg_threshold=0.25, 
                    label_type="technical_based",
                    threshold_method="quantile"
                )
            except Exception as e:
                print(f"âŒ æ¨™ç±¤ç”Ÿæˆå¤±æ•—: {e}")
                return {}
        
        print(f"ğŸ¯ ç›®æ¨™: å¾{len(features_df.columns)}å€‹ç‰¹å¾µä¸­æ‰¾å‡ºæœ€ä½³é¸æ“‡ç­–ç•¥")
        print(f"ğŸ”„ è©¦é©—æ•¸é‡: {n_trials}")
        
        import optuna
        
        # ğŸ”§ é è™•ç†æ•¸æ“šï¼Œé¿å…æ¯å€‹trialé‡è¤‡è™•ç†
        print("ğŸ”§ é è™•ç†æ•¸æ“šä»¥åŠ é€ŸOptunaæœç´¢...")
        min_length = min(len(features_df), len(labels))
        X_base = features_df.iloc[:min_length]
        y_base = labels.iloc[:min_length]
        
        # ä½¿ç”¨æ¡æ¨£ä¾†åŠ é€Ÿæœç´¢ (æœ€æ–°20%æ•¸æ“š)
        sample_size = min(20000, len(X_base))  # æœ€å¤š2è¬æ¨£æœ¬
        X_sample = X_base.tail(sample_size)
        y_sample = y_base.tail(sample_size)
        print(f"ğŸ¯ ä½¿ç”¨æ¡æ¨£æ•¸æ“š: {X_sample.shape} (åŸå§‹: {X_base.shape})")
        
        def features_objective(trial):
            """Featureså„ªåŒ–ç›®æ¨™å‡½æ•¸ (å„ªåŒ–ç‰ˆ)"""
            try:
                # 1. ç‰¹å¾µé¸æ“‡æ–¹æ³•
                selection_method = trial.suggest_categorical(
                    "feature_selection_method", 
                    ["lightgbm", "mutual_info", "combined"]  # ç§»é™¤RFEï¼Œå¤ªæ…¢
                )
                
                # 2. ç‰¹å¾µæ•¸é‡æ§åˆ¶
                target_feature_count = trial.suggest_int("target_feature_count", 15, 35)
                
                # 3. ç›¸é—œæ€§æ§åˆ¶
                correlation_threshold = trial.suggest_float("correlation_threshold", 0.80, 0.95)
                
                # 4. ç‰¹å¾µé¡å‹æ¬Šé‡
                technical_weight = trial.suggest_float("technical_weight", 0.8, 1.5)
                volume_weight = trial.suggest_float("volume_weight", 0.8, 1.5)
                time_weight = trial.suggest_float("time_weight", 0.5, 1.2)
                
                # ä½¿ç”¨é è™•ç†çš„æ¡æ¨£æ•¸æ“š
                X = X_sample
                y = y_sample
                
                # æ‡‰ç”¨ç‰¹å¾µé¡å‹æ¬Šé‡
                feature_weights = self._calculate_feature_weights(
                    X.columns, technical_weight, volume_weight, time_weight
                )
                
                # ç¬¬ä¸€éšæ®µï¼šç²—é¸ (ä½¿ç”¨ç›®æ¨™æ•¸é‡çš„2å€ä½œç‚ºç²—é¸)
                coarse_k = min(target_feature_count * 2, len(X.columns))
                
                if selection_method == "lightgbm":
                    selected_features = self._select_features_lightgbm(X, y, coarse_k, feature_weights)
                elif selection_method == "mutual_info":
                    selected_features = self._select_features_mutual_info(X, y, coarse_k)
                elif selection_method == "combined":
                    # çµ„åˆæ–¹æ³•
                    lgb_features = self._select_features_lightgbm(X, y, coarse_k//2, feature_weights)
                    mi_features = self._select_features_mutual_info(X, y, coarse_k//2)
                    selected_features = list(set(lgb_features + mi_features))[:coarse_k]
                
                # å»ç›¸é—œè™•ç†
                if len(selected_features) > 1:
                    selected_features = self._remove_correlated_features(
                        X[selected_features], correlation_threshold
                    )
                
                # ç¬¬äºŒéšæ®µï¼šç²¾é¸åˆ°ç›®æ¨™æ•¸é‡
                if len(selected_features) > target_feature_count:
                    final_features = self._select_features_lightgbm(
                        X[selected_features], y, target_feature_count, feature_weights
                    )
                else:
                    final_features = selected_features
                
                # è©•ä¼°ç‰¹å¾µè³ªé‡
                if len(final_features) < 5:
                    return 0.0  # ç‰¹å¾µå¤ªå°‘
                
                # è¨ˆç®—ç¶œåˆè©•åˆ†
                quality_score = self._evaluate_feature_quality(
                    X[final_features], y, final_features, 
                    target_feature_count, correlation_threshold
                )
                
                return quality_score
                
            except Exception as e:
                print(f"Trialå¤±æ•—: {e}")
                return 0.0
        
        # åŸ·è¡ŒOptunaæœç´¢
        try:
            study = optuna.create_study(direction="maximize")
            study.optimize(features_objective, n_trials=n_trials, show_progress_bar=True)
            
            best_params = study.best_params
            best_score = study.best_value
            
            print(f"âœ… Features Optunaå„ªåŒ–å®Œæˆ")
            print(f"ğŸ¯ æœ€ä½³åƒæ•¸: {best_params}")
            print(f"ğŸ“Š æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
            
            # ä¿å­˜Featuresæœ€ä½³åƒæ•¸
            self.save_optuna_params("features", best_params, best_score)
            
            return {
                "best_params": best_params,
                "best_score": best_score,
                "study": study
            }
            
        except Exception as e:
            print(f"âŒ Features Optunaå„ªåŒ–å¤±æ•—: {e}")
            return {}
    
    def _calculate_feature_weights(self, feature_names: List[str], 
                                 technical_weight: float, volume_weight: float, 
                                 time_weight: float) -> Dict[str, float]:
        """è¨ˆç®—ç‰¹å¾µæ¬Šé‡"""
        weights = {}
        
        for feature in feature_names:
            feature_lower = feature.lower()
            if any(word in feature_lower for word in ['rsi', 'macd', 'atr', 'adx', 'bb', 'cci']):
                weights[feature] = technical_weight
            elif any(word in feature_lower for word in ['volume', 'vol', 'obv', 'mfi']):
                weights[feature] = volume_weight
            elif any(word in feature_lower for word in ['hour', 'day', 'week', 'month', 'sin', 'cos']):
                weights[feature] = time_weight
            else:
                weights[feature] = 1.0  # é»˜èªæ¬Šé‡
        
        return weights
    
    def _select_features_lightgbm(self, X: pd.DataFrame, y: pd.Series, 
                                k: int, weights: Dict[str, float] = None) -> List[str]:
        """ä½¿ç”¨LightGBMé¸æ“‡ç‰¹å¾µ"""
        try:
            model = lgb.LGBMClassifier(
                n_estimators=50, max_depth=3, random_state=42, verbose=-1
            )
            model.fit(X, y)
            
            # ç²å–ç‰¹å¾µé‡è¦æ€§
            importance = model.feature_importances_
            
            # æ‡‰ç”¨æ¬Šé‡
            if weights:
                weighted_importance = []
                for i, feature in enumerate(X.columns):
                    weight = weights.get(feature, 1.0)
                    weighted_importance.append(importance[i] * weight)
                importance = weighted_importance
            
            # é¸æ“‡top-kç‰¹å¾µ
            feature_importance = list(zip(X.columns, importance))
            feature_importance.sort(key=lambda x: x[1], reverse=True)
            
            return [feat[0] for feat in feature_importance[:k]]
            
        except Exception as e:
            print(f"LightGBMç‰¹å¾µé¸æ“‡å¤±æ•—: {e}")
            return list(X.columns[:k])
    
    def _select_features_mutual_info(self, X: pd.DataFrame, y: pd.Series, k: int) -> List[str]:
        """ä½¿ç”¨äº’ä¿¡æ¯é¸æ“‡ç‰¹å¾µ"""
        try:
            from sklearn.feature_selection import SelectKBest, mutual_info_classif
            
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
            selector.fit(X, y)
            
            selected_mask = selector.get_support()
            return X.columns[selected_mask].tolist()
            
        except Exception as e:
            print(f"äº’ä¿¡æ¯ç‰¹å¾µé¸æ“‡å¤±æ•—: {e}")
            return list(X.columns[:k])
    
    def _select_features_rfe(self, X: pd.DataFrame, y: pd.Series, k: int) -> List[str]:
        """ä½¿ç”¨RFEé¸æ“‡ç‰¹å¾µ"""
        try:
            from sklearn.feature_selection import RFE
            from sklearn.ensemble import RandomForestClassifier
            
            estimator = RandomForestClassifier(n_estimators=50, random_state=42)
            selector = RFE(estimator, n_features_to_select=k)
            selector.fit(X, y)
            
            selected_mask = selector.get_support()
            return X.columns[selected_mask].tolist()
            
        except Exception as e:
            print(f"RFEç‰¹å¾µé¸æ“‡å¤±æ•—: {e}")
            return list(X.columns[:k])
    
    def _remove_correlated_features(self, X: pd.DataFrame, threshold: float) -> List[str]:
        """ç§»é™¤é«˜ç›¸é—œæ€§ç‰¹å¾µ"""
        try:
            corr_matrix = X.corr().abs()
            upper_tri = corr_matrix.where(
                np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
            )
            
            to_drop = [column for column in upper_tri.columns 
                      if any(upper_tri[column] > threshold)]
            
            return [col for col in X.columns if col not in to_drop]
            
        except Exception as e:
            print(f"å»ç›¸é—œè™•ç†å¤±æ•—: {e}")
            return list(X.columns)
    
    def _safe_time_series_split(self, X: pd.DataFrame, y: pd.Series, n_splits: int = 3, 
                               lag_periods: int = 5) -> List[Tuple[np.ndarray, np.ndarray]]:
        """å®‰å…¨çš„æ™‚é–“åºåˆ—åˆ†å‰²ï¼Œé˜²æ­¢æ¨™ç±¤æ•¸æ“šæ´©æ¼"""
        total_samples = len(X)
        
        # æ’é™¤æœ€å¾Œlag_periodså€‹æ¨£æœ¬ï¼Œå› ç‚ºå®ƒå€‘çš„æ¨™ç±¤ä½¿ç”¨äº†æœªä¾†æ•¸æ“š
        safe_samples = total_samples - lag_periods
        
        if safe_samples <= 100:  # æ•¸æ“šå¤ªå°‘
            return []
        
        splits = []
        fold_size = safe_samples // (n_splits + 1)
        
        for i in range(n_splits):
            # è¨“ç·´é›†ï¼šå¾é–‹å§‹åˆ°ç•¶å‰foldçµæŸ
            train_end = (i + 1) * fold_size
            train_idx = np.arange(0, train_end)
            
            # æ¸¬è©¦é›†ï¼šä¸‹ä¸€å€‹foldï¼Œä½†è¦ç¢ºä¿ä¸è¶…ésafe_samples
            test_start = train_end
            test_end = min(train_end + fold_size, safe_samples)
            
            if test_end - test_start < 50:  # æ¸¬è©¦é›†å¤ªå°
                continue
                
            test_idx = np.arange(test_start, test_end)
            
            # é©—è­‰æ™‚é–“é †åº
            if len(train_idx) > 0 and len(test_idx) > 0 and max(train_idx) < min(test_idx):
                splits.append((train_idx, test_idx))
        
        return splits

    def _evaluate_feature_quality(self, X: pd.DataFrame, y: pd.Series, 
                                features: List[str], target_count: int, 
                                corr_threshold: float) -> float:
        """è©•ä¼°ç‰¹å¾µè³ªé‡ç¶œåˆåˆ†æ•¸"""
        try:
            # 1. æ•¸é‡æ•ˆç‡åˆ†æ•¸ (æ¥è¿‘ç›®æ¨™æ•¸é‡å¾—åˆ†æ›´é«˜)
            count_efficiency = 1.0 - abs(len(features) - target_count) / target_count
            
            # 2. å¤šæ¨£æ€§åˆ†æ•¸ (ç‰¹å¾µé¡å‹è¶Šå¤šæ¨£è¶Šå¥½)
            feature_types = self._categorize_features_simple(features)
            diversity_score = len(feature_types) / 6  # æœ€å¤š6ç¨®é¡å‹
            
            # 3. ç›¸é—œæ€§åˆ†æ•¸ (ä½ç›¸é—œæ€§æ›´å¥½)
            if len(features) > 1:
                corr_matrix = X.corr().abs()
                avg_correlation = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean()
                correlation_score = 1.0 - min(avg_correlation, 0.9) / 0.9
            else:
                correlation_score = 1.0
            
            # 4. é æ¸¬èƒ½åŠ›åˆ†æ•¸ (åš´æ ¼æ™‚é–“åºåˆ—é©—è­‰ï¼Œç¢ºä¿ç„¡æ•¸æ“šæ´©æ¼)
            try:
                # ç¢ºä¿æ•¸æ“šæœ‰æ™‚é–“ç´¢å¼•ï¼Œå¦‚æœæ²’æœ‰å‰‡æŒ‰é †åºè™•ç†
                if not isinstance(X.index, pd.DatetimeIndex):
                    # å‡è¨­æ•¸æ“šå·²æŒ‰æ™‚é–“æ’åºï¼Œé‡ç½®ç´¢å¼•ç¢ºä¿é †åº
                    X_sample = X.reset_index(drop=True)
                    y_sample = y.reset_index(drop=True)
                else:
                    X_sample = X.sort_index()
                    y_sample = y.sort_index()
                
                # ä½¿ç”¨åˆç†æ¨£æœ¬å¤§å°é€²è¡Œè©•ä¼°
                sample_size = min(5000, len(X_sample))
                if len(X_sample) > sample_size:
                    # ä½¿ç”¨æœ€è¿‘çš„æ•¸æ“šé€²è¡Œè©•ä¼°ï¼Œä½†ä¿æŒæ™‚é–“é †åº
                    X_sample = X_sample.tail(sample_size)
                    y_sample = y_sample.tail(sample_size)
                
                # ä½¿ç”¨å®‰å…¨çš„æ™‚é–“åºåˆ—åˆ†å‰²ï¼Œé˜²æ­¢æ¨™ç±¤æ•¸æ“šæ´©æ¼
                safe_splits = self._safe_time_series_split(X_sample, y_sample, n_splits=3, lag_periods=5)
                scores = []
                
                if len(safe_splits) == 0:
                    print("âš ï¸ è­¦å‘Š: æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œå®‰å…¨çš„æ™‚é–“åºåˆ—åˆ†å‰²")
                    prediction_score = 0.0
                else:
                    for fold, (train_idx, test_idx) in enumerate(safe_splits):
                        X_train = X_sample.iloc[train_idx]
                        X_test = X_sample.iloc[test_idx]
                        y_train = y_sample.iloc[train_idx]
                        y_test = y_sample.iloc[test_idx]
                        
                        # æª¢æŸ¥æ¨™ç±¤åˆ†ä½ˆ
                        if len(y_train.unique()) > 1 and len(y_test.unique()) > 1:
                            # ä½¿ç”¨è¼•é‡ç´šæ¨¡å‹å¿«é€Ÿè©•ä¼°
                            model = lgb.LGBMClassifier(
                                n_estimators=15,  
                                max_depth=4,      
                                learning_rate=0.1,
                                num_leaves=15,
                                subsample=0.8,
                                feature_fraction=0.8,
                                random_state=42,
                                verbose=-1,
                                force_col_wise=True
                            )
                            
                            model.fit(X_train, y_train)
                            y_pred = model.predict(X_test)
                            score = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                            scores.append(score)
                            
                            # èª¿è©¦ä¿¡æ¯ï¼ˆå¯é¸ï¼‰
                            if fold == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                                print(f"   å®‰å…¨æ™‚é–“åºåˆ—é©—è­‰: è¨“ç·´æœŸ[{train_idx[0]}:{train_idx[-1]}] -> æ¸¬è©¦æœŸ[{test_idx[0]}:{test_idx[-1]}], F1: {score:.4f}")
                    
                    prediction_score = np.mean(scores) if scores else 0.0
            except:
                prediction_score = 0.0
            
            # ç¶œåˆè©•åˆ†
            total_score = (
                count_efficiency * 0.2 +    # æ•¸é‡æ•ˆç‡
                diversity_score * 0.3 +     # å¤šæ¨£æ€§
                correlation_score * 0.2 +   # ä½ç›¸é—œæ€§
                prediction_score * 0.3      # é æ¸¬èƒ½åŠ›
            )
            
            return max(0.0, min(1.0, total_score))
            
        except Exception as e:
            print(f"ç‰¹å¾µè³ªé‡è©•ä¼°å¤±æ•—: {e}")
            return 0.0
    
    def _categorize_features_simple(self, features: List[str]) -> Dict[str, int]:
        """ç°¡å–®ç‰¹å¾µåˆ†é¡"""
        categories = {"technical": 0, "volume": 0, "time": 0, "price": 0, "statistical": 0, "other": 0}
        
        for feature in features:
            feature_lower = feature.lower()
            if any(kw in feature_lower for kw in ['rsi', 'macd', 'atr', 'adx', 'bb', 'cci']):
                categories["technical"] += 1
            elif any(kw in feature_lower for kw in ['volume', 'vol', 'obv', 'mfi']):
                categories["volume"] += 1
            elif any(kw in feature_lower for kw in ['hour', 'day', 'week', 'month', 'sin', 'cos']):
                categories["time"] += 1
            elif any(kw in feature_lower for kw in ['open', 'high', 'low', 'close', 'price']):
                categories["price"] += 1
            elif any(kw in feature_lower for kw in ['return', 'std', 'mean', 'skew', 'kurt']):
                categories["statistical"] += 1
            else:
                categories["other"] += 1
        
        return {k: v for k, v in categories.items() if v > 0}
    
    def get_next_version(self) -> str:
        """è‡ªå‹•ç²å–ä¸‹ä¸€å€‹ç‰ˆæœ¬è™Ÿ"""
        base_dirs = [
            f"data/processed/features/{self.symbol}_{self.timeframe}",
            f"data/processed/labels/{self.symbol}_{self.timeframe}",
            f"results/modular_optimization/{self.symbol}_{self.timeframe}"
        ]
        
        max_version = 0
        for base_dir in base_dirs:
            if os.path.exists(base_dir):
                for item in os.listdir(base_dir):
                    if item.startswith('v') and item[1:].isdigit():
                        version_num = int(item[1:])
                        max_version = max(max_version, version_num)
        
        return f"v{max_version + 1}"
    
    def list_versions(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç‰ˆæœ¬"""
        base_dir = f"results/modular_optimization/{self.symbol}_{self.timeframe}"
        versions = []
        
        if os.path.exists(base_dir):
            for item in os.listdir(base_dir):
                if item.startswith('v') and item[1:].isdigit():
                    versions.append(item)
        
        # æŒ‰ç‰ˆæœ¬è™Ÿæ’åº
        versions.sort(key=lambda x: int(x[1:]))
        return versions
    
    def get_latest_version(self) -> str:
        """ç²å–æœ€æ–°ç‰ˆæœ¬è™Ÿ"""
        versions = self.list_versions()
        return versions[-1] if versions else "v1"

    def setup_data_paths(self):
        """è¨­ç½®ç‰ˆæœ¬åŒ–æ•¸æ“šæ–‡ä»¶è·¯å¾‘ - ğŸ”§ ä¿®å¾©ç‰ˆæœ¬ä¸€è‡´æ€§å•é¡Œ"""
        # ğŸ”¢ ç‰ˆæœ¬åŒ–è·¯å¾‘çµæ§‹ - ç¢ºä¿ç‰¹å¾µå’Œæ¨™ç±¤ä½¿ç”¨ç›¸åŒç‰ˆæœ¬
        self.data_paths = {
            # âœ… ä¿®å¾©ï¼šç‰¹å¾µä¹Ÿä½¿ç”¨ç‰ˆæœ¬åŒ–è·¯å¾‘ï¼Œç¢ºä¿ç‰ˆæœ¬ä¸€è‡´æ€§
            "features": f"data/processed/features/{self.symbol}_{self.timeframe}/{self.version}/{self.symbol}_{self.timeframe}_features.parquet",
            
            # ğŸ”„ ä¿ç•™ç„¡ç‰ˆæœ¬ç‰¹å¾µä½œç‚ºå›é€€é¸é …
            "features_fallback": f"data/processed/features/{self.symbol}_{self.timeframe}_features.parquet",
            
            # ç‰ˆæœ¬åŒ–è·¯å¾‘
            "version_base": {
                "features": f"data/processed/features/{self.symbol}_{self.timeframe}/{self.version}",
                "labels": f"data/processed/labels/{self.symbol}_{self.timeframe}/{self.version}",
                "results": f"results/modular_optimization/{self.symbol}_{self.timeframe}/{self.version}",
                "models": f"results/models/{self.symbol}_{self.timeframe}/{self.version}",
                "logs": f"logs/optimization/{self.symbol}_{self.timeframe}/{self.version}"
            },
            
            # ç‰ˆæœ¬åŒ–æ–‡ä»¶è·¯å¾‘ - ç¢ºä¿ä¸‰éšæ®µä½¿ç”¨ä¸€è‡´ç‰ˆæœ¬
            "labels": f"data/processed/labels/{self.symbol}_{self.timeframe}/{self.version}/{self.symbol}_{self.timeframe}_labels.parquet",
            "selected_features": f"data/processed/features/{self.symbol}_{self.timeframe}/{self.version}/{self.symbol}_{self.timeframe}_selected_features.parquet",
            
            # åŸå§‹æ•¸æ“šè·¯å¾‘ï¼ˆä¸ç‰ˆæœ¬åŒ–ï¼‰
            "raw_ohlcv": f"data/raw/{self.symbol}/{self.symbol}_{self.timeframe}.parquet",
            
            # ç‰ˆæœ¬åŒ–çµæœç›®éŒ„
            "results_dir": f"results/modular_optimization/{self.symbol}_{self.timeframe}/{self.version}",
            "models_dir": f"results/models/{self.symbol}_{self.timeframe}/{self.version}",
            "logs_dir": f"logs/optimization/{self.symbol}_{self.timeframe}/{self.version}"
        }
        
        # å‰µå»ºç‰ˆæœ¬åŒ–ç›®éŒ„
        for key, dir_path in self.data_paths["version_base"].items():
            os.makedirs(dir_path, exist_ok=True)
        
        # å‰µå»ºåŸºç¤ç›®éŒ„
        base_dirs = [
            "data/processed/labels",
            "data/processed/features"
        ]
        for dir_path in base_dirs:
            os.makedirs(dir_path, exist_ok=True)

    def log_execution(self, stage: str, status: str, details: str = "", 
                     duration: float = 0.0):
        """è¨˜éŒ„åŸ·è¡Œæ—¥èªŒ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "stage": stage,
            "status": status,
            "details": details,
            "duration_seconds": duration,
            "symbol": self.symbol,
            "timeframe": self.timeframe
        }
        
        self.execution_log.append(log_entry)
        
        # å¯¦æ™‚ä¿å­˜æ—¥èªŒ
        log_file = os.path.join(self.data_paths["logs_dir"], "execution.json")
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(self.execution_log, f, indent=2, ensure_ascii=False, default=str)
    
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """è¼‰å…¥æˆ–ç”Ÿæˆç‰¹å¾µæ•¸æ“š - ğŸ”§ ç‰ˆæœ¬åŒ–ç‰¹å¾µæ”¯æŒ"""
        print("ğŸ“Š è¼‰å…¥æ•¸æ“š...")
        
        try:
            # ğŸ”§ å„ªå…ˆè¼‰å…¥ç‰ˆæœ¬åŒ–ç‰¹å¾µï¼Œä¿è­‰ç‰ˆæœ¬ä¸€è‡´æ€§
            features_df = None
            
            # æ–¹æ¡ˆ1ï¼šè¼‰å…¥ç‰ˆæœ¬åŒ–ç‰¹å¾µï¼ˆå„ªå…ˆï¼‰
            if os.path.exists(self.data_paths["features"]):
                features_df = pd.read_parquet(self.data_paths["features"])
                print(f" âœ… è¼‰å…¥ç‰ˆæœ¬åŒ–ç‰¹å¾µæ•¸æ“š {self.version}: {features_df.shape}")
            
            # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°ç„¡ç‰ˆæœ¬ç‰¹å¾µï¼ˆå¦‚æœç‰ˆæœ¬åŒ–ä¸å­˜åœ¨ï¼‰
            elif os.path.exists(self.data_paths["features_fallback"]):
                features_df = pd.read_parquet(self.data_paths["features_fallback"])
                print(f" ğŸ”„ è¼‰å…¥å›é€€ç‰¹å¾µæ•¸æ“š: {features_df.shape}")
                print(f" âš ï¸  è­¦å‘Šï¼šä½¿ç”¨ç„¡ç‰ˆæœ¬ç‰¹å¾µï¼Œå¯èƒ½å°è‡´ç‰ˆæœ¬ä¸ä¸€è‡´å•é¡Œ")
                
                # è‡ªå‹•ç”Ÿæˆç‰ˆæœ¬åŒ–ç‰¹å¾µå‰¯æœ¬
                print(f" ğŸ”§ æ­£åœ¨ç‚º{self.version}å‰µå»ºç‰ˆæœ¬åŒ–ç‰¹å¾µå‰¯æœ¬...")
                versioned_features_path = self.data_paths["features"]
                os.makedirs(os.path.dirname(versioned_features_path), exist_ok=True)
                features_df.to_parquet(versioned_features_path)
                print(f" ğŸ’¾ ç‰ˆæœ¬åŒ–ç‰¹å¾µå·²ä¿å­˜: {versioned_features_path}")
            
            # æ–¹æ¡ˆ3ï¼šé‡æ–°ç”Ÿæˆç‰¹å¾µ
            else:
                print(" ğŸ”§ ç‰¹å¾µæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆç”Ÿæˆç‰¹å¾µ")
                features_df = self.generate_features_from_raw()
                if features_df is None:
                    return None, None
            
            # è¼‰å…¥åŸå§‹OHLCVæ•¸æ“š 
            if os.path.exists(self.data_paths["raw_ohlcv"]):
                ohlcv_df = pd.read_parquet(self.data_paths["raw_ohlcv"])
                print(f" âœ… è¼‰å…¥OHLCVæ•¸æ“š: {ohlcv_df.shape}")
            else:
                raise FileNotFoundError(f"åŸå§‹OHLCVæ•¸æ“šä¸å­˜åœ¨: {self.data_paths['raw_ohlcv']}")
            
            return features_df, ohlcv_df
            
        except Exception as e:
            error_msg = f"æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}"
            print(f" âŒ {error_msg}")
            self.log_execution("data_loading", "failed", error_msg)
            return None, None
    
    def generate_features_from_raw(self) -> pd.DataFrame:
        """ğŸ”§ ç”Ÿæˆç‰ˆæœ¬åŒ–ç‰¹å¾µç³»çµ± - ç¢ºä¿ç‰ˆæœ¬ä¸€è‡´æ€§"""
        try:
            print(" ğŸ”§ æª¢æŸ¥ç‰¹å¾µæ–‡ä»¶...")
            
            # ğŸ†• ç­–ç•¥ï¼šç”Ÿæˆç‰ˆæœ¬åŒ–ç‰¹å¾µï¼Œç¢ºä¿èˆ‡æ¨™ç±¤ç‰ˆæœ¬ä¸€è‡´
            print(f" ğŸš€ ç‚º{self.version}ç”Ÿæˆç‰ˆæœ¬åŒ–ç‰¹å¾µç³»çµ±...")
            
            # è¼‰å…¥åŸå§‹OHLCVæ•¸æ“š
            raw_ohlcv_path = self.data_paths["raw_ohlcv"]
            if not os.path.exists(raw_ohlcv_path):
                print(f" âŒ åŸå§‹OHLCVæ•¸æ“šä¸å­˜åœ¨: {raw_ohlcv_path}")
                return None
                
            ohlcv_df = pd.read_parquet(raw_ohlcv_path)
            print(f" âœ… è¼‰å…¥OHLCVæ•¸æ“š: {ohlcv_df.shape}")
            
            # ä½¿ç”¨æ–°çš„ç‰¹å¾µå·¥ç¨‹ç³»çµ±
            from src.features.feature_generator import FeatureEngineering
            fe = FeatureEngineering(self.timeframe)
            features_df = fe.generate_all_features(ohlcv_df)
            
            print(f" ğŸ†• ç”Ÿæˆ{self.version}ç‰¹å¾µ: {features_df.shape} ({len(features_df.columns)}å€‹ç‰¹å¾µ)")
            
            # ä¿å­˜æ–°ç‰¹å¾µåˆ°ç‰ˆæœ¬åŒ–è·¯å¾‘
            features_save_path = self.data_paths["features"]
            os.makedirs(os.path.dirname(features_save_path), exist_ok=True)
            features_df.to_parquet(features_save_path)
            print(f" ğŸ’¾ ç‰ˆæœ¬åŒ–ç‰¹å¾µå·²ä¿å­˜: {features_save_path}")
            
            # ğŸ”§ åŒæ™‚ä¿å­˜åˆ°ç„¡ç‰ˆæœ¬è·¯å¾‘ä½œç‚ºå›é€€
            fallback_path = self.data_paths["features_fallback"]
            if not os.path.exists(fallback_path):
                os.makedirs(os.path.dirname(fallback_path), exist_ok=True)
                features_df.to_parquet(fallback_path)
                print(f" ğŸ”„ å›é€€ç‰¹å¾µå·²ä¿å­˜: {fallback_path}")
            
            return features_df
            
        except Exception as e:
            print(f" âŒ ç‰ˆæœ¬åŒ–ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")
            return None

    def run_complete_optimization(self) -> Dict[str, Any]:
        """é‹è¡Œå®Œæ•´çš„ä¸‰éšæ®µå„ªåŒ–æµç¨‹"""
        print("ğŸš€ é–‹å§‹æ¨¡çµ„åŒ–Optunaä¸‰éšæ®µå„ªåŒ–")
        print(f"äº¤æ˜“å°: {self.symbol}")
        print(f"æ™‚é–“æ¡†æ¶: {self.timeframe}")
        print("="*80)
        
        total_start_time = datetime.now()
        
        try:
            # è¼‰å…¥æ•¸æ“š
            features_df, ohlcv_df = self.load_data()
            if features_df is None or ohlcv_df is None:
                raise ValueError("æ•¸æ“šè¼‰å…¥å¤±æ•—")
            
            # ç¬¬ä¸€éšæ®µï¼šæ¨™ç±¤å„ªåŒ–
            label_results = self.run_stage1_label_optimization(features_df, ohlcv_df)
            if not label_results:
                raise ValueError("æ¨™ç±¤å„ªåŒ–å¤±æ•—")

            # ğŸ“Š å®Œæ•´ä¿å­˜ç¬¬ä¸€éšæ®µçµæœ
            self.optimization_results["label_optimization"] = self._extract_label_results(label_results)
            labels = label_results["labels"]
            
            # ç¬¬äºŒéšæ®µï¼šç‰¹å¾µé¸æ“‡
            feature_results = self.run_stage2_feature_selection(features_df, labels)
            if not feature_results:
                raise ValueError("ç‰¹å¾µé¸æ“‡å¤±æ•—")

            # ğŸ“Š å®Œæ•´ä¿å­˜ç¬¬äºŒéšæ®µçµæœ
            self.optimization_results["feature_selection"] = self._extract_feature_results(feature_results)
            selected_features = feature_results["best_features"]
            
            # ç¬¬ä¸‰éšæ®µï¼šæ¨¡å‹å„ªåŒ–
            model_results = self.run_stage3_model_optimization(features_df, labels, selected_features)
            if not model_results:
                raise ValueError("æ¨¡å‹å„ªåŒ–å¤±æ•—")

            # ğŸ“Š å®Œæ•´ä¿å­˜ç¬¬ä¸‰éšæ®µçµæœ
            self.optimization_results["model_optimization"] = self._extract_model_results(model_results)
            
            # ğŸ” ç”Ÿæˆéåº¦æ“¬åˆåˆ†æ
            overfitting_analysis = self._analyze_overfitting()
            self.optimization_results["overfitting_analysis"] = overfitting_analysis
            
            # ğŸ†• è‡ªå‹•åŒ–ä¸€è‡´æ€§å¾©æ ¸ï¼ˆæœ€å¾Œéšæ®µï¼‰
            consistency_report = self._run_automated_consistency_check()
            self.optimization_results["consistency_check"] = consistency_report
            
            # ä¿å­˜çµæœ
            results_file = self.save_optimization_results()

            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            final_report = self.generate_final_report()
            print(final_report)

            total_duration = (datetime.now() - total_start_time).total_seconds()
            self.log_execution("complete_optimization", "success", 
                             f"å®Œæ•´å„ªåŒ–æˆåŠŸï¼Œè€—æ™‚ {total_duration:.1f} ç§’", total_duration)
            
            return self.optimization_results
            
        except Exception as e:
            total_duration = (datetime.now() - total_start_time).total_seconds()
            error_msg = f"å®Œæ•´å„ªåŒ–å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            self.log_execution("complete_optimization", "failed", error_msg, total_duration)
            return {}

    def run_complete_training_with_optimized_params(self, use_full_data: bool = True) -> Dict[str, Any]:
        """ğŸ¯ ä½¿ç”¨å„ªåŒ–å¾Œçš„åƒæ•¸é€²è¡Œå®Œæ•´æ¨¡å‹è¨“ç·´ - æ­£ç¢ºçš„MLæµç¨‹"""
        print("ğŸš€ é–‹å§‹å®Œæ•´æ¨¡å‹è¨“ç·´ï¼ˆä½¿ç”¨å„ªåŒ–åƒæ•¸ï¼‰")
        print(f"äº¤æ˜“å°: {self.symbol}")
        print(f"æ™‚é–“æ¡†æ¶: {self.timeframe}")
        print(f"ç‰ˆæœ¬: {self.version}")
        print("="*80)
        
        start_time = datetime.now()
        
        try:
            # ğŸ” ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥å·²å„ªåŒ–çš„åƒæ•¸
            print("\nğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥å·²å„ªåŒ–çš„åƒæ•¸...")
            
            # è¼‰å…¥å„ªåŒ–çµæœ
            results_file = os.path.join(self.data_paths["results_dir"], "modular_optuna_results.json")
            if not os.path.exists(results_file):
                raise FileNotFoundError(f"å„ªåŒ–çµæœä¸å­˜åœ¨: {results_file}")
            
            with open(results_file, 'r', encoding='utf-8') as f:
                optimization_results = json.load(f)
            
            # æå–å·²å„ªåŒ–çš„åƒæ•¸
            label_params = optimization_results["results"]["label_optimization"]["best_params"]
            feature_selection = optimization_results["results"]["feature_selection"]["best_features"]
            model_params = optimization_results["results"]["model_optimization"]["best_params"]
            
            print(f" âœ… è¼‰å…¥æ¨™ç±¤åƒæ•¸: {label_params}")
            print(f" âœ… è¼‰å…¥ç‰¹å¾µé¸æ“‡: {len(feature_selection)}å€‹ç‰¹å¾µ")
            print(f" âœ… è¼‰å…¥æ¨¡å‹åƒæ•¸: {len(model_params)}å€‹åƒæ•¸")
            
            # ğŸ” ç¬¬äºŒæ­¥ï¼šè¼‰å…¥ç‰ˆæœ¬åŒ–çš„ç‰¹å¾µå’Œæ¨™ç±¤
            print("\nğŸ“Š ç¬¬äºŒæ­¥ï¼šè¼‰å…¥ç‰ˆæœ¬åŒ–æ•¸æ“š...")
            
            # è¼‰å…¥ç‰ˆæœ¬åŒ–ç‰¹å¾µï¼ˆv55ç‰¹å¾µï¼‰
            if os.path.exists(self.data_paths["features"]):
                features_df = pd.read_parquet(self.data_paths["features"])
                print(f" âœ… è¼‰å…¥{self.version}ç‰¹å¾µ: {features_df.shape}")
            else:
                raise FileNotFoundError(f"ç‰ˆæœ¬åŒ–ç‰¹å¾µä¸å­˜åœ¨: {self.data_paths['features']}")
            
            # è¼‰å…¥ç‰ˆæœ¬åŒ–æ¨™ç±¤ï¼ˆv55æ¨™ç±¤ï¼‰
            if os.path.exists(self.data_paths["labels"]):
                labels_df = pd.read_parquet(self.data_paths["labels"])
                labels = labels_df['label']  # å‡è¨­æ¨™ç±¤åˆ—åç‚º'label'
                print(f" âœ… è¼‰å…¥{self.version}æ¨™ç±¤: {labels.shape}")
            else:
                raise FileNotFoundError(f"ç‰ˆæœ¬åŒ–æ¨™ç±¤ä¸å­˜åœ¨: {self.data_paths['labels']}")
            
            # ğŸ” ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨å„ªåŒ–å¾Œçš„ç‰¹å¾µå­é›†
            print("\nğŸ¯ ç¬¬ä¸‰æ­¥ï¼šæ‡‰ç”¨å„ªåŒ–ç‰¹å¾µé¸æ“‡...")
            
            # ç¢ºä¿ç‰¹å¾µå­˜åœ¨
            missing_features = [f for f in feature_selection if f not in features_df.columns]
            if missing_features:
                print(f" âš ï¸ è­¦å‘Šï¼šç¼ºå°‘{len(missing_features)}å€‹ç‰¹å¾µ: {missing_features[:5]}...")
                feature_selection = [f for f in feature_selection if f in features_df.columns]
            
            selected_features_df = features_df[feature_selection]
            print(f" âœ… æ‡‰ç”¨ç‰¹å¾µé¸æ“‡: {selected_features_df.shape}")
            
            # ğŸ” ç¬¬å››æ­¥ï¼šæ•¸æ“šå°é½Šå’Œæ¸…ç†
            print("\nğŸ”§ ç¬¬å››æ­¥ï¼šæ•¸æ“šå°é½Šå’Œæ¸…ç†...")
            
            min_length = min(len(selected_features_df), len(labels))
            X_final = selected_features_df.iloc[:min_length].copy()
            y_final = labels.iloc[:min_length].copy()
            
            # ç§»é™¤NaNå€¼
            valid_idx = ~(X_final.isnull().any(axis=1) | y_final.isnull())
            X_final = X_final[valid_idx]
            y_final = y_final[valid_idx]
            
            print(f" âœ… æœ€çµ‚è¨“ç·´æ•¸æ“š: {X_final.shape}, æ¨™ç±¤: {y_final.shape}")
            print(f" ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {y_final.value_counts().to_dict()}")
            
            # ğŸ” ç¬¬äº”æ­¥ï¼šæ™‚åºäº¤å‰é©—è­‰ï¼ˆç¢ºä¿æ¨¡å‹ç©©å¥æ€§ï¼‰
            print("\nâ° ç¬¬äº”æ­¥ï¼šæ™‚åºäº¤å‰é©—è­‰...")
            
            # é…ç½®LightGBMåƒæ•¸
            lgb_params = model_params.copy()
            
            # æ ¹æ“šæ¨™ç±¤é¡åˆ¥æ•¸èª¿æ•´ç›®æ¨™å‡½æ•¸
            n_classes = len(y_final.unique())
            if n_classes <= 2:
                lgb_params["objective"] = "binary"
                lgb_params["metric"] = "binary_logloss"
                lgb_params["num_class"] = None
            else:
                lgb_params["objective"] = "multiclass" 
                lgb_params["metric"] = "multi_logloss"
                lgb_params["num_class"] = n_classes
            
            # æ·»åŠ å›ºå®šåƒæ•¸
            lgb_params.update({
                "verbose": -1,
                "random_state": 42,
                "num_threads": 1,
                "force_col_wise": True
            })
            
            # æ™‚åºäº¤å‰é©—è­‰ - ç¢ºä¿æ¨¡å‹åœ¨ä¸åŒæ™‚æœŸè¡¨ç¾ç©©å®š
            cv_scores = self._run_time_series_cv(X_final, y_final, lgb_params, n_splits=5)
            mean_cv_score = np.mean(cv_scores)
            std_cv_score = np.std(cv_scores)
            
            print(f" ğŸ“Š æ™‚åºCV F1åˆ†æ•¸: {cv_scores}")
            print(f" ğŸ“Š å¹³å‡CV F1: {mean_cv_score:.4f} (Â±{std_cv_score:.4f})")
            
            # ğŸ” ç¬¬å…­æ­¥ï¼šåŸºæ–¼CVçµæœæ±ºå®šæ˜¯å¦è¨“ç·´æœ€çµ‚æ¨¡å‹
            print("\nğŸ¤– ç¬¬å…­æ­¥ï¼šè¨“ç·´å®Œæ•´ç”Ÿç”¢æ¨¡å‹...")
            
            # è¨­å®šCVè¡¨ç¾é–¾å€¼ï¼ˆå¯æ ¹æ“šå…·é«”æƒ…æ³èª¿æ•´ï¼‰
            cv_threshold = 0.35  # F1åˆ†æ•¸é–¾å€¼
            if mean_cv_score >= cv_threshold:
                print(f" âœ… CVè¡¨ç¾é”æ¨™ ({mean_cv_score:.4f} >= {cv_threshold})")
                
                # å‰µå»ºä¸¦è¨“ç·´æœ€çµ‚æ¨¡å‹
                final_model = lgb.LGBMClassifier(**lgb_params)
                
                if use_full_data:
                    # ä½¿ç”¨å…¨éƒ¨æ•¸æ“šè¨“ç·´ï¼ˆç”Ÿç”¢æ¨¡å¼ï¼‰
                    print(" ğŸš€ ä½¿ç”¨å…¨éƒ¨æ•¸æ“šè¨“ç·´ç”Ÿç”¢æ¨¡å‹...")
                    final_model.fit(X_final, y_final)
                    training_mode = "å…¨æ•¸æ“šè¨“ç·´"
                else:
                    # ä¿ç•™æ¸¬è©¦é›†é€²è¡Œé©—è­‰
                    from sklearn.model_selection import train_test_split
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_final, y_final, test_size=0.2, random_state=42, stratify=y_final
                    )
                    
                    final_model.fit(
                        X_train, y_train,
                        eval_set=[(X_test, y_test)],
                        callbacks=[lgb.early_stopping(30)]
                    )
                    training_mode = "ç•™å‡ºé©—è­‰é›†è¨“ç·´"
                    
                    # è¨ˆç®—æ¸¬è©¦é›†æ€§èƒ½
                    y_pred = final_model.predict(X_test)
                    if n_classes > 2:
                        test_f1 = f1_score(y_test, y_pred, average='weighted')
                    else:
                        test_f1 = f1_score(y_test, y_pred, average='binary')
                    print(f" ğŸ“Š æ¸¬è©¦é›†F1åˆ†æ•¸: {test_f1:.4f}")
            
            else:
                print(f" âŒ CVè¡¨ç¾æœªé”æ¨™ ({mean_cv_score:.4f} < {cv_threshold})")
                print(" âš ï¸  å»ºè­°é‡æ–°èª¿æ•´åƒæ•¸æˆ–å¢åŠ æ•¸æ“šè³ªé‡")
                return {
                    "status": "cv_failed",
                    "cv_scores": cv_scores,
                    "mean_cv_score": mean_cv_score,
                    "threshold": cv_threshold,
                    "message": "æ™‚åºäº¤å‰é©—è­‰è¡¨ç¾æœªé”æ¨™ï¼Œæœªç”Ÿæˆæœ€çµ‚æ¨¡å‹"
                }
            
            # ğŸ” ç¬¬å…­æ­¥ï¼šä¿å­˜å®Œæ•´æ¨¡å‹
            print("\nğŸ’¾ ç¬¬å…­æ­¥ï¼šä¿å­˜å®Œæ•´ç”Ÿç”¢æ¨¡å‹...")
            
            model_save_dir = os.path.join(self.data_paths["models_dir"], "production")
            os.makedirs(model_save_dir, exist_ok=True)
            
            # ä¿å­˜LightGBMæ¨¡å‹
            model_file = os.path.join(model_save_dir, f"{self.symbol}_{self.timeframe}_{self.version}_production.txt")
            final_model.booster_.save_model(model_file)
            
            # ä¿å­˜æ¨¡å‹é…ç½®
            model_config = {
                "version": self.version,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "training_mode": training_mode,
                "training_data_shape": X_final.shape,
                "feature_count": len(feature_selection),
                "feature_names": feature_selection,
                "label_params": label_params,
                "model_params": lgb_params,
                "trained_at": datetime.now().isoformat(),
                "training_duration": (datetime.now() - start_time).total_seconds()
            }
            
            config_file = os.path.join(model_save_dir, f"{self.symbol}_{self.timeframe}_{self.version}_config.json")
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False, default=str)
            
            # ä¿å­˜ç‰¹å¾µé‡è¦æ€§
            feature_importance = dict(zip(feature_selection, final_model.feature_importance()))
            importance_file = os.path.join(model_save_dir, f"{self.symbol}_{self.timeframe}_{self.version}_importance.json")
            with open(importance_file, 'w', encoding='utf-8') as f:
                json.dump(feature_importance, f, indent=2, ensure_ascii=False)
            
            training_duration = (datetime.now() - start_time).total_seconds()
            
            # ğŸ¯ ç”Ÿæˆè¨“ç·´å ±å‘Š
            training_results = {
                "status": "success",
                "version": self.version,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "training_mode": training_mode,
                "training_duration": training_duration,
                "cross_validation": {
                    "cv_scores": cv_scores,
                    "mean_cv_score": mean_cv_score,
                    "std_cv_score": std_cv_score,
                    "cv_threshold": cv_threshold,
                    "cv_passed": mean_cv_score >= cv_threshold
                },
                "data_summary": {
                    "feature_count": len(feature_selection),
                    "sample_count": len(X_final),
                    "label_distribution": y_final.value_counts().to_dict()
                },
                "model_files": {
                    "model": model_file,
                    "config": config_file,
                    "importance": importance_file
                },
                "parameters_used": {
                    "label_params": label_params,
                    "selected_features": feature_selection,
                    "model_params": lgb_params
                }
            }
            
            if not use_full_data:
                training_results["test_performance"] = {
                    "f1_score": test_f1,
                    "test_samples": len(X_test)
                }
            
            print(f"\nâœ… å®Œæ•´è¨“ç·´å®Œæˆï¼")
            print(f"â±ï¸  è¨“ç·´æ™‚é–“: {training_duration:.2f}ç§’")
            print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: {model_file}")
            print(f"ğŸ“‹ é…ç½®å·²ä¿å­˜: {config_file}")
            print("="*80)
            
            return training_results
            
        except Exception as e:
            error_msg = f"å®Œæ•´è¨“ç·´å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return {}

    def _run_time_series_cv(self, X: pd.DataFrame, y: pd.Series, lgb_params: dict, n_splits: int = 5) -> List[float]:
        """åŸ·è¡Œæ™‚åºäº¤å‰é©—è­‰ - åŸºæ–¼æ–‡æª”è¨­è¨ˆ"""
        from sklearn.model_selection import TimeSeriesSplit
        
        print(f" â° åŸ·è¡Œ{n_splits}æŠ˜æ™‚åºäº¤å‰é©—è­‰...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            print(f"   ğŸ”„ ç¬¬{fold}æŠ˜é©—è­‰...")
            
            # åˆ†å‰²æ•¸æ“š
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # æª¢æŸ¥æ¨™ç±¤åˆ†ä½ˆ
            if len(y_train.unique()) < 2 or len(y_val.unique()) < 2:
                print(f"     âš ï¸ ç¬¬{fold}æŠ˜æ¨™ç±¤é¡åˆ¥ä¸è¶³ï¼Œè·³é...")
                continue
            
            try:
                # è¨“ç·´æ¨¡å‹
                model = lgb.LGBMClassifier(**lgb_params)
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]
                )
                
                # é æ¸¬å’Œè©•ä¼°
                y_val_pred = model.predict(X_val)
                
                # è¨ˆç®—F1åˆ†æ•¸
                n_classes = len(y.unique())
                if n_classes > 2:
                    fold_score = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)
                else:
                    fold_score = f1_score(y_val, y_val_pred, average='binary', zero_division=0)
                
                cv_scores.append(fold_score)
                print(f"     âœ… ç¬¬{fold}æŠ˜ F1åˆ†æ•¸: {fold_score:.4f}")
                
            except Exception as e:
                print(f"     âŒ ç¬¬{fold}æŠ˜è¨“ç·´å¤±æ•—: {e}")
                continue
        
        if not cv_scores:
            print("   âŒ æ‰€æœ‰æŠ˜æ¬¡éƒ½å¤±æ•—ï¼Œä½¿ç”¨é»˜èªåˆ†æ•¸0.0")
            cv_scores = [0.0]
        
        return cv_scores

    def _evaluate_sharpe_ratio(self, y_true: np.ndarray, y_pred_proba: np.ndarray, 
                              returns: np.ndarray) -> float:
        """æ ¹æ“šé æ¸¬æ¦‚ç‡å’ŒçœŸå¯¦æ”¶ç›Šè¨ˆç®—å¹´åŒ–Sharpe Ratio - åŸºæ–¼æ–‡æª”è¨­è¨ˆ"""
        try:
            # åŸºæ–¼é æ¸¬æ¦‚ç‡ç”Ÿæˆäº¤æ˜“ä¿¡è™Ÿ
            signals = (y_pred_proba >= 0.5).astype(int) * 2 - 1  # è½‰æ›ç‚º -1, 1 ä¿¡è™Ÿ
            
            # è¨ˆç®—ç­–ç•¥æ”¶ç›Šï¼ˆé¿å…å‰ç»åå·®ï¼‰
            strategy_returns = signals[:-1] * returns[1:]
            
            if len(strategy_returns) == 0 or np.std(strategy_returns) == 0:
                return 0.0
            
            # è¨ˆç®—å¹´åŒ–Sharpeæ¯”ç‡ï¼ˆå‡è¨­15åˆ†é˜æ•¸æ“šï¼‰
            mean_return = np.mean(strategy_returns)
            std_return = np.std(strategy_returns)
            
            # å¹´åŒ–ï¼š252å¤© * 24å°æ™‚ * 4å€‹15åˆ†é˜
            annualization_factor = np.sqrt(252 * 24 * 4)
            sharpe_ratio = (mean_return / std_return) * annualization_factor
            
            return sharpe_ratio
            
        except Exception as e:
            print(f"   âš ï¸ Sharpeæ¯”ç‡è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def run_model_optimization_only(self) -> Dict[str, Any]:
        """ğŸ¯ åƒ…ä½¿ç”¨å·²æœ‰çš„v55ç‰¹å¾µå’Œæ¨™ç±¤é€²è¡Œæ¨¡å‹åƒæ•¸å„ªåŒ–"""
        print("ğŸš€ é–‹å§‹æ¨¡å‹åƒæ•¸å„ªåŒ–ï¼ˆä½¿ç”¨å·²æœ‰v55æ•¸æ“šï¼‰")
        print(f"äº¤æ˜“å°: {self.symbol}")
        print(f"æ™‚é–“æ¡†æ¶: {self.timeframe}")
        print(f"ç‰ˆæœ¬: {self.version}")
        print("="*80)
        
        start_time = datetime.now()
        
        try:
            # ğŸ” ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥å·²æœ‰çš„v55ç‰¹å¾µå’Œæ¨™ç±¤
            print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šè¼‰å…¥å·²æœ‰çš„v55æ•¸æ“š...")
            
            # è¼‰å…¥v55é¸ä¸­ç‰¹å¾µ
            if os.path.exists(self.data_paths["selected_features"]):
                features_df = pd.read_parquet(self.data_paths["selected_features"])
                print(f" âœ… è¼‰å…¥v55é¸ä¸­ç‰¹å¾µ: {features_df.shape}")
            else:
                raise FileNotFoundError(f"v55é¸ä¸­ç‰¹å¾µä¸å­˜åœ¨: {self.data_paths['selected_features']}")
            
            # è¼‰å…¥v55æ¨™ç±¤
            if os.path.exists(self.data_paths["labels"]):
                labels_df = pd.read_parquet(self.data_paths["labels"])
                labels = labels_df['label']
                print(f" âœ… è¼‰å…¥v55æ¨™ç±¤: {labels.shape}")
            else:
                raise FileNotFoundError(f"v55æ¨™ç±¤ä¸å­˜åœ¨: {self.data_paths['labels']}")
            
            # ğŸ” ç¬¬äºŒæ­¥ï¼šæ•¸æ“šå°é½Šå’Œæ¸…ç†
            print("\nğŸ”§ ç¬¬äºŒæ­¥ï¼šæ•¸æ“šå°é½Šå’Œæ¸…ç†...")
            
            min_length = min(len(features_df), len(labels))
            features_aligned = features_df.iloc[:min_length].copy()
            labels_aligned = labels.iloc[:min_length].copy()
            
            # ç§»é™¤NaNå€¼
            valid_idx = ~(features_aligned.isnull().any(axis=1) | labels_aligned.isnull())
            X_final = features_aligned[valid_idx]
            y_final = labels_aligned[valid_idx]
            
            print(f" âœ… æœ€çµ‚æ•¸æ“š: {X_final.shape}, æ¨™ç±¤: {y_final.shape}")
            print(f" ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {y_final.value_counts().to_dict()}")
            
            # ğŸ” ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹åƒæ•¸å„ªåŒ–
            print("\nğŸ¤– ç¬¬ä¸‰æ­¥ï¼šåŸ·è¡Œæ¨¡å‹åƒæ•¸å„ªåŒ–...")
            
            model_results = self.model_optimizer.optimize(X_final, y_final, n_trials=50, fast_mode=False)
            
            if not model_results:
                raise ValueError("æ¨¡å‹å„ªåŒ–å¤±æ•—")
            
            # ğŸ” ç¬¬å››æ­¥ï¼šä¿å­˜å„ªåŒ–çµæœ
            print("\nğŸ’¾ ç¬¬å››æ­¥ï¼šä¿å­˜æ¨¡å‹åƒæ•¸...")
            
            # ä¿å­˜æœ€ä½³åƒæ•¸
            self.save_optuna_params('model', model_results['best_params'], model_results.get('best_score', 0.0))
            
            # ä¿å­˜æ¨¡å‹åƒæ•¸åˆ°ç‰ˆæœ¬åŒ–ç›®éŒ„
            model_params_path = os.path.join(self.data_paths["models_dir"], f"{self.symbol}_{self.timeframe}_model_params.json")
            os.makedirs(os.path.dirname(model_params_path), exist_ok=True)
            
            model_info = {
                "best_params": model_results.get('best_params', {}),
                "best_score": model_results.get('best_score', 0.0),
                "cv_results": model_results.get('cv_results', {}),
                "version": self.version,
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "optimized_at": datetime.now().isoformat(),
                "optimization_duration": (datetime.now() - start_time).total_seconds()
            }
            
            with open(model_params_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False, default=str)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            print(f"\nâœ… æ¨¡å‹åƒæ•¸å„ªåŒ–å®Œæˆï¼")
            print(f"â±ï¸  å„ªåŒ–æ™‚é–“: {duration:.2f}ç§’")
            print(f"ğŸ“Š æœ€ä½³F1åˆ†æ•¸: {model_results.get('best_score', 0.0):.4f}")
            print(f"ğŸ“ åƒæ•¸å·²ä¿å­˜: {model_params_path}")
            print(f"ğŸ¯ æœ€ä½³åƒæ•¸: {model_results.get('best_params', {})}")
            print("="*80)
            
            return {
                "status": "success",
                "version": self.version,
                "best_params": model_results.get('best_params', {}),
                "best_score": model_results.get('best_score', 0.0),
                "optimization_duration": duration,
                "data_info": {
                    "features_shape": X_final.shape,
                    "labels_shape": y_final.shape,
                    "label_distribution": y_final.value_counts().to_dict()
                },
                "files": {
                    "model_params": model_params_path,
                    "used_features": self.data_paths["selected_features"],
                    "used_labels": self.data_paths["labels"]
                }
            }
            
        except Exception as e:
            error_msg = f"æ¨¡å‹åƒæ•¸å„ªåŒ–å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return {"status": "failed", "error": error_msg}
    
    def run_stage1_label_optimization(self, features_df: pd.DataFrame, 
                                    ohlcv_df: pd.DataFrame) -> Dict[str, Any]:
        """åŸ·è¡Œç¬¬ä¸€éšæ®µï¼šæ¨™ç±¤å„ªåŒ–"""
        print("\n" + "="*60)
        print("ğŸ¯ ç¬¬ä¸€éšæ®µï¼šæ¨™ç±¤å„ªåŒ–")
        print("="*60)
        
        start_time = datetime.now()
        
        try:
            # ç²å–åƒ¹æ ¼æ•¸æ“š
            price_data = ohlcv_df['close']
            
            # åŸ·è¡Œæ¨™ç±¤å„ªåŒ–
            label_results = self.label_optimizer.optimize(features_df, price_data, n_trials=200)
            
            if label_results:
                duration = (datetime.now() - start_time).total_seconds()
                success_msg = f"æ¨™ç±¤å„ªåŒ–æˆåŠŸï¼Œæœ€ä½³F1: {label_results.get('best_score', 0):.6f}"
                print(f"âœ… {success_msg}")
                self.log_execution("label_optimization", "success", success_msg, duration)
                
                # ğŸ’¾ ä¿å­˜æ¨™ç±¤åˆ°ç‰ˆæœ¬åŒ– Parquet æ–‡ä»¶
                try:
                    labels = label_results["labels"]  # pandas Series
                    labels_path = self.data_paths["labels"]
                    
                    # ğŸ”¢ ç¢ºä¿ç‰ˆæœ¬åŒ–ç›®éŒ„å­˜åœ¨
                    os.makedirs(os.path.dirname(labels_path), exist_ok=True)
                    
                    # å°‡ Series è½‰æ›ç‚º DataFrame ä¸¦ä¿å­˜
                    labels_df = labels.to_frame(name="label")
                    labels_df.to_parquet(labels_path)
                    print(f"ğŸ’¾ æ¨™ç±¤å·²ä¿å­˜åˆ°: {labels_path}")
                    
                    # åŒæ™‚ä¿å­˜æ¨™ç±¤åƒæ•¸åˆ°åŒç›®éŒ„
                    params_path = labels_path.replace('_labels.parquet', '_label_params.json')
                    with open(params_path, 'w', encoding='utf-8') as f:
                        json.dump(label_results.get('best_params', {}), f, indent=2, ensure_ascii=False)
                    print(f"ğŸ’¾ æ¨™ç±¤åƒæ•¸å·²ä¿å­˜åˆ°: {params_path}")
                    
                except Exception as e:
                    print(f"âš ï¸ æ¨™ç±¤ä¿å­˜å¤±æ•—: {e}")
                
                # ç”Ÿæˆä¸¦æ‰“å°å ±å‘Š
                report = self.label_optimizer.generate_report()
                print(report)
                
                return label_results
            else:
                raise ValueError("æ¨™ç±¤å„ªåŒ–è¿”å›ç©ºçµæœ")
                
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            error_msg = f"æ¨™ç±¤å„ªåŒ–å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            self.log_execution("label_optimization", "failed", error_msg, duration)
            return {}
    
    def run_stage2_feature_selection(self, features_df: pd.DataFrame, 
                                   labels: pd.Series) -> Dict[str, Any]:
        """åŸ·è¡Œç¬¬äºŒéšæ®µï¼šå…©éšæ®µç‰¹å¾µé¸æ“‡ (åŸºæ–¼å­¸è¡“æ–‡ç»çš„4éšæ®µæ¶æ§‹)"""
        print("\n" + "="*80)  
        print("ğŸ¯ ç¬¬äºŒéšæ®µï¼šå…©éšæ®µç‰¹å¾µé¸æ“‡ (4éšæ®µæ¶æ§‹)")
        print("="*80)
        
        # ğŸ”§ é—œéµä¿®å¾©ï¼šç«‹å³ä¿å­˜åŸå§‹ç‰¹å¾µä¿¡æ¯ï¼Œé˜²æ­¢å¾ŒçºŒä¿®æ”¹
        original_feature_count = len(features_df.columns)
        original_feature_names = list(features_df.columns)
        
        print(f"ğŸ“Š æ™‚é–“æ¡†æ¶: {self.timeframe} | åŸå§‹ç‰¹å¾µæ•¸: {original_feature_count}")
        print(f"ğŸ”§ åŸå§‹ç‰¹å¾µæ•¸å·²å®‰å…¨ä¿å­˜: {original_feature_count}")
        
        total_start_time = datetime.now()
        
        try:
            # æ•¸æ“šå°é½Šå’Œé è™•ç†
            min_length = min(len(features_df), len(labels))
            features_aligned = features_df.iloc[:min_length]
            labels_aligned = labels.iloc[:min_length]
            
            print(f"ğŸ“ˆ å°é½Šå¾Œæ•¸æ“š: ç‰¹å¾µ{len(features_aligned.columns)}å€‹ï¼Œæ¨£æœ¬{len(features_aligned)}å€‹")
            print(f"ğŸ“‹ æ¨™ç±¤åˆ†ä½ˆ: {labels_aligned.value_counts().to_dict()}")
            
            # ğŸ” ç¬¬2Aéšæ®µï¼šç²—é¸ç‰¹å¾µå€™é¸æ± 
            print(f"\n{'-'*60}")
            print("ğŸ” ç¬¬2Aéšæ®µï¼šç²—é¸ç‰¹å¾µå€™é¸æ±  (å¿«é€Ÿç¯©é¸)")
            print(f"{'-'*60}")
            
            coarse_results = self._run_stage2a_coarse_selection(features_aligned, labels_aligned)
            if not coarse_results or not coarse_results.get("candidate_features"):
                raise ValueError("ç²—é¸éšæ®µå¤±æ•—æˆ–æœªæ‰¾åˆ°å€™é¸ç‰¹å¾µ")
                
            candidate_features = coarse_results["candidate_features"]
            print(f"âœ… ç¬¬2Aéšæ®µå®Œæˆ: å¾{len(features_aligned.columns)}å€‹ç‰¹å¾µä¸­ç²—é¸å‡º{len(candidate_features)}å€‹å€™é¸ç‰¹å¾µ")
            print(f"ğŸ“Š å€™é¸ç‰¹å¾µ: {candidate_features[:10]}..." if len(candidate_features) > 10 else f"ğŸ“Š å€™é¸ç‰¹å¾µ: {candidate_features}")
            
            # ğŸ¯ ç¬¬2Béšæ®µï¼šç²¾é¸æœ€çµ‚ç‰¹å¾µ  
            print(f"\n{'-'*60}")
            print("ğŸ¯ ç¬¬2Béšæ®µï¼šç²¾é¸æœ€çµ‚ç‰¹å¾µ (ç²¾ç´°å„ªåŒ–)")
            print(f"{'-'*60}")
            
            candidate_features_df = features_aligned[candidate_features]
            fine_results = self._run_stage2b_fine_selection(candidate_features_df, labels_aligned)
            if not fine_results or not fine_results.get("final_features"):
                raise ValueError("ç²¾é¸éšæ®µå¤±æ•—æˆ–æœªæ‰¾åˆ°æœ€çµ‚ç‰¹å¾µ")
                
            final_features = fine_results["final_features"]
            print(f"âœ… ç¬¬2Béšæ®µå®Œæˆ: å¾{len(candidate_features)}å€‹å€™é¸ä¸­ç²¾é¸å‡º{len(final_features)}å€‹æœ€çµ‚ç‰¹å¾µ")
            print(f"ğŸ† æœ€çµ‚ç‰¹å¾µ: {final_features}")
            
            # ğŸ“Š åˆä½µå…©éšæ®µçµæœ
            feature_results = {
                "stage2a_results": coarse_results,
                "stage2b_results": fine_results,
                "best_features": final_features,
                "best_score": fine_results.get("best_score", 0.0),
                "selection_pipeline": {
                    "original_count": original_feature_count,  # ğŸ”§ é—œéµä¿®å¾©ï¼šä½¿ç”¨ä¿å­˜çš„åŸå§‹æ•¸æ“š
                    "cleaned_count": len(features_aligned.columns),  # æ¸…æ´—å¾Œæ•¸é‡
                    "candidate_count": len(candidate_features), 
                    "final_count": len(final_features),
                    "coarse_ratio": len(candidate_features) / original_feature_count if original_feature_count > 0 else 0,
                    "fine_ratio": len(final_features) / len(candidate_features) if len(candidate_features) > 0 else 0,
                    "overall_ratio": len(final_features) / original_feature_count if original_feature_count > 0 else 0,
                    "pipeline": f"{original_feature_count} â†’ {len(candidate_features)} â†’ {len(final_features)}"
                }
            }
            
            if feature_results:
                total_duration = (datetime.now() - total_start_time).total_seconds()
                best_features = feature_results.get('best_features', [])
                pipeline_info = feature_results.get('selection_pipeline', {})
                
                success_msg = f"å…©éšæ®µç‰¹å¾µé¸æ“‡æˆåŠŸ: {pipeline_info.get('pipeline', 'N/A')}"
                print(f"\nğŸ‰ {success_msg}")
                print(f"â° ç¸½è€—æ™‚: {total_duration:.1f}ç§’")
                print(f"ğŸ“Š æœ€çµ‚æ•ˆæœ: {len(best_features)}å€‹é«˜è³ªé‡ç‰¹å¾µ")
                
                self.log_execution("feature_selection", "success", success_msg, total_duration)
                
                # ğŸ’¾ ä¿å­˜é¸ä¸­çš„ç‰¹å¾µåˆ°ç‰ˆæœ¬åŒ– Parquet æ–‡ä»¶
                try:
                    if best_features:
                        # ç²å–é¸ä¸­çš„ç‰¹å¾µæ•¸æ“š
                        selected_features_df = features_aligned[best_features]
                        selected_features_path = self.data_paths["selected_features"]
                        
                        # ğŸ”¢ ç¢ºä¿ç‰ˆæœ¬åŒ–ç›®éŒ„å­˜åœ¨
                        os.makedirs(os.path.dirname(selected_features_path), exist_ok=True)
                        
                        # ä¿å­˜é¸ä¸­çš„ç‰¹å¾µæ•¸æ“š
                        selected_features_df.to_parquet(selected_features_path)
                        print(f"ğŸ’¾ é¸ä¸­ç‰¹å¾µæ•¸æ“šå·²ä¿å­˜åˆ°: {selected_features_path}")
                        
                        # ä¿å­˜ç‰¹å¾µé¸æ“‡åƒæ•¸å’Œçµæœ - ğŸ”§ ä¿®å¾©ç‰¹å¾µè¨ˆæ•¸é‚è¼¯bug  
                        feature_params_path = selected_features_path.replace('_selected_features.parquet', '_feature_selection_params.json')
                        feature_selection_info = {
                            # å…©éšæ®µé¸æ“‡çµæœ
                            "stage2a_results": feature_results.get('stage2a_results', {}),
                            "stage2b_results": feature_results.get('stage2b_results', {}),
                            "best_features": best_features,
                            "best_score": feature_results.get('best_score', 0.0),
                            
                            # ğŸ”§ å®Œå…¨ä¿®å¾©ï¼šä½¿ç”¨ä¿å­˜çš„åŸå§‹ç‰¹å¾µæ•¸
                            "original_feature_count": original_feature_count,  # âœ… ä½¿ç”¨å®‰å…¨ä¿å­˜çš„åŸå§‹æ•¸æ“š
                            "original_feature_names": original_feature_names,  # âœ… åŸå§‹ç‰¹å¾µåç¨±åˆ—è¡¨
                            "cleaned_feature_count": len(features_aligned.columns),  # æ¸…æ´—å¾Œæ•¸é‡  
                            "selected_feature_count": len(best_features),
                            
                            # å®Œæ•´çš„é¸æ“‡ç®¡é“ä¿¡æ¯
                            "selection_pipeline": feature_results.get('selection_pipeline', {}),
                            "selection_ratio": len(best_features) / original_feature_count if original_feature_count > 0 else 0,  # âœ… ä½¿ç”¨å®‰å…¨è¨ˆç®—
                            "method": "two_stage_selection_4stage_architecture"
                        }
                        
                        with open(feature_params_path, 'w', encoding='utf-8') as f:
                            json.dump(feature_selection_info, f, indent=2, ensure_ascii=False)
                        print(f"ğŸ’¾ ç‰¹å¾µé¸æ“‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {feature_params_path}")
                        
                except Exception as e:
                    print(f"âš ï¸ ç‰¹å¾µä¿å­˜å¤±æ•—: {e}")
                
                # ç”Ÿæˆä¸¦æ‰“å°å ±å‘Š
                report = self.feature_selector.generate_report(features_df)
                print(report)
                
                return feature_results
            else:
                raise ValueError("ç‰¹å¾µé¸æ“‡è¿”å›ç©ºçµæœ")
                
        except Exception as e:
            total_duration = (datetime.now() - total_start_time).total_seconds()
            error_msg = f"å…©éšæ®µç‰¹å¾µé¸æ“‡å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            self.log_execution("feature_selection", "failed", error_msg, total_duration)
            return {}
    
    def _run_stage2a_coarse_selection(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ç¬¬2Aéšæ®µï¼šç²—é¸ç‰¹å¾µå€™é¸æ±  (å¿«é€Ÿç¯©é¸) - åŸºæ–¼HFTæ–‡ç»"""
        try:
            print(f"ğŸ” é–‹å§‹ç²—é¸éšæ®µ...")
            start_time = datetime.now()
            
            # ç²å–æ™‚é–“æ¡†æ¶é…ç½®
            feature_config = self.config.get_feature_config(self.timeframe)
            coarse_config = feature_config.get("two_stage_selection", {}).get("coarse_selection", {})
            
            # ç²—é¸åƒæ•¸
            coarse_k_range = feature_config.get("coarse_k_range", (30, 50))
            target_k = min(coarse_k_range[1], len(X.columns))  # ä¸è¶…éç¾æœ‰ç‰¹å¾µæ•¸
            
            print(f"ğŸ“Š ç²—é¸ç›®æ¨™: é¸å‡ºç´„{target_k}å€‹å€™é¸ç‰¹å¾µ")
            
            # ğŸš€ ä½¿ç”¨LightGBMå¿«é€Ÿé‡è¦æ€§é¸æ“‡ (ä¸»è¦æ–¹æ³•)
            candidate_features_lgb = self.feature_selector.select_features_lightgbm(
                X, y, k=target_k
            )
            
            if len(candidate_features_lgb) == 0:
                print("âš ï¸ LightGBMæœªé¸å‡ºä»»ä½•ç‰¹å¾µï¼Œä½¿ç”¨å‰Nå€‹ç‰¹å¾µä½œç‚ºå¾Œå‚™")
                candidate_features_lgb = list(X.columns[:target_k])
            
            print(f"âœ… LightGBMç²—é¸: {len(candidate_features_lgb)}å€‹ç‰¹å¾µ")
            
            # ğŸ”„ äº’ä¿¡æ¯è¼”åŠ©é©—è­‰ (è¼”åŠ©æ–¹æ³•ï¼ŒåŸºæ–¼æ–‡ç»æ¨è–¦)
            if len(X.columns) > 50:  # åªåœ¨ç‰¹å¾µè¼ƒå¤šæ™‚ä½¿ç”¨è¼”åŠ©é©—è­‰
                try:
                    candidate_features_mi = self.feature_selector.select_features_mutual_info(
                        X, y, k=target_k
                    )
                    
                    # å–äº¤é›†å’Œä¸¦é›†çš„å¹³è¡¡
                    intersection = set(candidate_features_lgb) & set(candidate_features_mi)
                    union = set(candidate_features_lgb) | set(candidate_features_mi)
                    
                    # å„ªå…ˆä¿ç•™äº¤é›†ï¼Œç„¶å¾ŒæŒ‰é‡è¦æ€§è£œå……
                    final_candidates = list(intersection)
                    remaining_slots = target_k - len(final_candidates)
                    
                    if remaining_slots > 0:
                        additional = [f for f in candidate_features_lgb if f not in final_candidates]
                        final_candidates.extend(additional[:remaining_slots])
                    
                    candidate_features = final_candidates[:target_k]
                    print(f"âœ… äº’ä¿¡æ¯é©—è­‰: äº¤é›†{len(intersection)}å€‹, æœ€çµ‚{len(candidate_features)}å€‹")
                    
                except Exception as e:
                    print(f"âš ï¸ äº’ä¿¡æ¯è¼”åŠ©é©—è­‰å¤±æ•—: {e}, ä½¿ç”¨LightGBMçµæœ")
                    candidate_features = candidate_features_lgb[:target_k]
            else:
                candidate_features = candidate_features_lgb[:target_k]
            
            # ğŸ“Š ç²—é¸çµæœçµ±è¨ˆ
            duration = (datetime.now() - start_time).total_seconds()
            selection_ratio = len(candidate_features) / len(X.columns)
            
            print(f"ğŸ¯ ç²—é¸å®Œæˆ: {len(X.columns)} â†’ {len(candidate_features)} ç‰¹å¾µ")
            print(f"ğŸ“ˆ é¸æ“‡æ¯”ä¾‹: {selection_ratio:.2%}, è€—æ™‚: {duration:.1f}ç§’")
            
            return {
                "candidate_features": candidate_features,
                "method": "lightgbm_primary_mutual_info_auxiliary",
                "selection_stats": {
                    "original_count": len(X.columns),
                    "selected_count": len(candidate_features),
                    "selection_ratio": selection_ratio,
                    "duration_seconds": duration
                },
                "stage": "coarse_selection"
            }
            
        except Exception as e:
            print(f"âŒ ç²—é¸éšæ®µå¤±æ•—: {e}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return {}
    
    def _run_stage2b_fine_selection(self, candidate_X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ç¬¬2Béšæ®µï¼šç²¾é¸æœ€çµ‚ç‰¹å¾µ (ç²¾ç´°å„ªåŒ–) - åŸºæ–¼é‡åŒ–äº¤æ˜“æœ€ä½³å¯¦è¸"""
        try:
            print(f"ğŸ¯ é–‹å§‹ç²¾é¸éšæ®µ...")
            start_time = datetime.now()
            
            # ç²å–æ™‚é–“æ¡†æ¶é…ç½®
            feature_config = self.config.get_feature_config(self.timeframe)
            fine_config = feature_config.get("two_stage_selection", {}).get("fine_selection", {})
            
            # ç²¾é¸åƒæ•¸
            fine_k_range = feature_config.get("fine_k_range", (15, 25))
            fine_trials = feature_config.get("fine_n_trials", 150)
            
            print(f"ğŸ“Š ç²¾é¸ç›®æ¨™: {fine_k_range[0]}-{fine_k_range[1]}å€‹æœ€çµ‚ç‰¹å¾µ")
            print(f"ğŸ”„ å„ªåŒ–è©¦é©—: {fine_trials}æ¬¡")
            
            # ğŸ¯ ä½¿ç”¨å®Œæ•´çš„Optunaå„ªåŒ–é€²è¡Œç²¾é¸
            # è‡¨æ™‚ä¿®æ”¹ç‰¹å¾µé¸æ“‡å™¨é…ç½®ç‚ºç²¾é¸æ¨¡å¼
            original_config = self.feature_selector.config.copy()
            
            # è¨­ç½®ç²¾é¸æ¨¡å¼é…ç½®
            self.feature_selector.config.update({
                "k_range": fine_k_range,
                "n_trials": fine_trials,
                "correlation_threshold": fine_config.get("correlation_threshold", 0.85),
                "feature_importance_threshold": 0.001  # æ›´åš´æ ¼çš„é–¾å€¼
            })
            
            # åŸ·è¡Œç²¾ç´°å„ªåŒ–
            print("ğŸ”„ åŸ·è¡ŒOptunaç²¾ç´°å„ªåŒ–...")
            fine_results = self.feature_selector.optimize(candidate_X, y)
            
            # æ¢å¾©åŸå§‹é…ç½®
            self.feature_selector.config = original_config
            
            if not fine_results or not fine_results.get("best_features"):
                # å¾Œå‚™æ–¹æ¡ˆï¼šå¦‚æœå„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„é‡è¦æ€§æ’åº
                print("âš ï¸ Optunaå„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™æ–¹æ¡ˆ")
                backup_features = self.feature_selector.select_features_lightgbm(
                    candidate_X, y, k=fine_k_range[1]
                )
                fine_results = {
                    "best_features": backup_features,
                    "best_score": 0.5,  # é»˜èªåˆ†æ•¸
                    "method": "backup_lightgbm"
                }
            
            final_features = fine_results["best_features"]
            best_score = fine_results.get("best_score", 0.0)
            
            # ğŸ“Š æœ€çµ‚å»ç›¸é—œè™•ç†ï¼ˆåŸºæ–¼æ–‡ç»å»ºè­°ï¼‰
            if len(final_features) > 1:
                final_features = self.feature_selector.remove_correlated_features(
                    candidate_X, final_features
                )
            
            # ğŸ“Š ç²¾é¸çµæœçµ±è¨ˆ
            duration = (datetime.now() - start_time).total_seconds()
            selection_ratio = len(final_features) / len(candidate_X.columns)
            
            print(f"ğŸ¯ ç²¾é¸å®Œæˆ: {len(candidate_X.columns)} â†’ {len(final_features)} ç‰¹å¾µ")
            print(f"ğŸ“ˆ æœ€ä½³åˆ†æ•¸: {best_score:.6f}, é¸æ“‡æ¯”ä¾‹: {selection_ratio:.2%}")
            print(f"â° è€—æ™‚: {duration:.1f}ç§’")
            
            # ğŸ” ç‰¹å¾µè³ªé‡åˆ†æ
            feature_quality = self._analyze_feature_quality(candidate_X, final_features, y)
            
            return {
                "final_features": final_features,
                "best_score": best_score,
                "method": "optuna_combined_optimization",
                "selection_stats": {
                    "candidate_count": len(candidate_X.columns),
                    "selected_count": len(final_features),
                    "selection_ratio": selection_ratio,
                    "duration_seconds": duration
                },
                "feature_quality": feature_quality,
                "stage": "fine_selection"
            }
            
        except Exception as e:
            print(f"âŒ ç²¾é¸éšæ®µå¤±æ•—: {e}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return {}
    
    def _analyze_feature_quality(self, X: pd.DataFrame, features: List[str], y: pd.Series) -> Dict[str, Any]:
        """åˆ†ææœ€çµ‚ç‰¹å¾µè³ªé‡ - åŸºæ–¼é‡‘èMLæ–‡ç»"""
        try:
            if not features:
                return {}
            
            X_selected = X[features]
            
            # ç‰¹å¾µé–“ç›¸é—œæ€§åˆ†æ
            feature_corr = X_selected.corr().abs()
            avg_correlation = feature_corr.values[np.triu_indices_from(feature_corr.values, k=1)].mean()
            max_correlation = feature_corr.values[np.triu_indices_from(feature_corr.values, k=1)].max()
            
            # ç‰¹å¾µç©©å®šæ€§åˆ†æï¼ˆæ–¹å·®ï¼‰
            feature_stability = X_selected.var().mean()
            
            # ç‰¹å¾µå¤šæ¨£æ€§åˆ†æ
            feature_categories = self._categorize_features(features)
            
            quality_metrics = {
                "avg_inter_correlation": avg_correlation,
                "max_inter_correlation": max_correlation,
                "feature_stability": feature_stability,
                "feature_diversity": len(feature_categories),
                "feature_categories": feature_categories,
                "quality_score": self._calculate_quality_score(avg_correlation, max_correlation, len(feature_categories))
            }
            
            print(f"ğŸ“Š ç‰¹å¾µè³ªé‡åˆ†æ:")
            print(f"   å¹³å‡ç›¸é—œæ€§: {avg_correlation:.3f}")
            print(f"   æœ€å¤§ç›¸é—œæ€§: {max_correlation:.3f}")
            print(f"   ç‰¹å¾µå¤šæ¨£æ€§: {len(feature_categories)}é¡")
            print(f"   è³ªé‡è©•åˆ†: {quality_metrics['quality_score']:.3f}")
            
            return quality_metrics
            
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾µè³ªé‡åˆ†æå¤±æ•—: {e}")
            return {}
    
    def _categorize_features(self, features: List[str]) -> Dict[str, int]:
        """ç‰¹å¾µåˆ†é¡çµ±è¨ˆ - åŸºæ–¼é‡åŒ–äº¤æ˜“ç‰¹å¾µåˆ†é¡"""
        categories = {
            "price": 0, "volume": 0, "technical": 0, "momentum": 0,
            "volatility": 0, "time": 0, "statistical": 0, "other": 0
        }
        
        for feature in features:
            feature_lower = feature.lower()
            if any(kw in feature_lower for kw in ["price", "open", "high", "low", "close", "ma", "ema"]):
                categories["price"] += 1
            elif any(kw in feature_lower for kw in ["volume", "vol"]):
                categories["volume"] += 1
            elif any(kw in feature_lower for kw in ["rsi", "macd", "adx", "atr", "bb", "kdj"]):
                categories["technical"] += 1
            elif any(kw in feature_lower for kw in ["roc", "momentum", "mom"]):
                categories["momentum"] += 1
            elif any(kw in feature_lower for kw in ["std", "var", "volatility", "vol"]):
                categories["volatility"] += 1
            elif any(kw in feature_lower for kw in ["hour", "day", "week", "month", "time"]):
                categories["time"] += 1
            elif any(kw in feature_lower for kw in ["mean", "median", "skew", "kurt"]):
                categories["statistical"] += 1
            else:
                categories["other"] += 1
        
        return {k: v for k, v in categories.items() if v > 0}
    
    def _calculate_quality_score(self, avg_corr: float, max_corr: float, diversity: int) -> float:
        """è¨ˆç®—ç‰¹å¾µè³ªé‡è©•åˆ† (0-1ä¹‹é–“ï¼Œè¶Šé«˜è¶Šå¥½)"""
        # ç›¸é—œæ€§æ‡²ç½° (ç›¸é—œæ€§è¶Šä½è¶Šå¥½)
        corr_penalty = 1.0 - min(avg_corr, 0.8) / 0.8
        
        # å¤šæ¨£æ€§çå‹µ (åˆ†é¡è¶Šå¤šè¶Šå¥½ï¼Œæœ€å¤š8é¡)
        diversity_reward = min(diversity, 8) / 8
        
        # æœ€å¤§ç›¸é—œæ€§æ‡²ç½° (é¿å…æ¥µåº¦ç›¸é—œçš„ç‰¹å¾µå°)
        max_corr_penalty = 1.0 - min(max_corr, 0.95) / 0.95
        
        # ç¶œåˆè©•åˆ†
        quality_score = (corr_penalty * 0.4 + diversity_reward * 0.3 + max_corr_penalty * 0.3)
        return max(0.0, min(1.0, quality_score))
    
    def run_stage3_model_optimization(self, features_df: pd.DataFrame, 
                                    labels: pd.Series,
                                    selected_features: List[str]) -> Dict[str, Any]:
        """åŸ·è¡Œç¬¬ä¸‰éšæ®µï¼šæ¨¡å‹è¶…åƒæ•¸å„ªåŒ–"""
        print("\n" + "="*60)
        print("ğŸ¯ ç¬¬ä¸‰éšæ®µï¼šæ¨¡å‹è¶…åƒæ•¸å„ªåŒ–")
        print("="*60)
        
        start_time = datetime.now()
        
        try:
            # æº–å‚™æ•¸æ“š
            X_selected = features_df[selected_features]
            min_length = min(len(X_selected), len(labels))
            X_aligned = X_selected.iloc[:min_length]
            y_aligned = labels.iloc[:min_length]
            
            # åŸ·è¡Œæ¨¡å‹å„ªåŒ–
            model_results = self.model_optimizer.optimize(X_aligned, y_aligned)
            
            if model_results:
                duration = (datetime.now() - start_time).total_seconds()
                success_msg = f"æ¨¡å‹å„ªåŒ–æˆåŠŸï¼Œæœ€ä½³F1: {model_results.get('best_score', 0):.6f}"
                print(f"âœ… {success_msg}")
                self.log_execution("model_optimization", "success", success_msg, duration)
                
                # ğŸ’¾ ä¿å­˜æ¨¡å‹åƒæ•¸åˆ°ç‰ˆæœ¬åŒ–ç›®éŒ„
                try:
                    model_params_path = os.path.join(self.data_paths["models_dir"], f"{self.symbol}_{self.timeframe}_model_params.json")
                    
                    # ğŸ”¢ ç¢ºä¿ç‰ˆæœ¬åŒ–æ¨¡å‹ç›®éŒ„å­˜åœ¨
                    os.makedirs(os.path.dirname(model_params_path), exist_ok=True)
                    
                    model_info = {
                        "best_params": model_results.get('best_params', {}),
                        "best_score": model_results.get('best_score', 0.0),
                        "cv_results": model_results.get('cv_results', {}),
                        "version": self.version,
                        "symbol": self.symbol,
                        "timeframe": self.timeframe
                    }
                    
                    with open(model_params_path, 'w', encoding='utf-8') as f:
                        json.dump(model_info, f, indent=2, ensure_ascii=False, default=str)
                    print(f"ğŸ’¾ æ¨¡å‹åƒæ•¸å·²ä¿å­˜åˆ°: {model_params_path}")
                    
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹åƒæ•¸ä¿å­˜å¤±æ•—: {e}")
                
                # ç”Ÿæˆä¸¦æ‰“å°å ±å‘Š
                report = self.model_optimizer.generate_report()
                print(report)
                
                return model_results
            else:
                raise ValueError("æ¨¡å‹å„ªåŒ–è¿”å›ç©ºçµæœ")

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            error_msg = f"æ¨¡å‹å„ªåŒ–å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            self.log_execution("model_optimization", "failed", error_msg, duration)
            return {}

    def save_optimization_results(self) -> str:
        """ä¿å­˜å„ªåŒ–çµæœåˆ°ç‰ˆæœ¬åŒ–ç›®éŒ„"""
        try:
            # æº–å‚™å®Œæ•´çµæœ
            complete_results = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "symbol": self.symbol,
                    "timeframe": self.timeframe,
                    "version": self.version,  # ğŸ”¢ æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
                    "optimization_strategy": "Modular Optuna Optimization",
                    "system_version": "2.1"
                },
                "results": self.optimization_results.copy(),
                "execution_log": self.execution_log,
                "summary": self.generate_optimization_summary()
            }
            
            # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å°è±¡
            self.clean_results_for_json(complete_results["results"])
            
            # ğŸ”¢ ç¢ºä¿ç‰ˆæœ¬åŒ–çµæœç›®éŒ„å­˜åœ¨
            os.makedirs(self.data_paths["results_dir"], exist_ok=True)
            
            # ä¿å­˜JSONçµæœåˆ°ç‰ˆæœ¬åŒ–ç›®éŒ„
            results_file = os.path.join(self.data_paths["results_dir"], "modular_optuna_results.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(complete_results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"ğŸ’¾ å„ªåŒ–çµæœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ğŸ“‹ ç”Ÿæˆç‰ˆæœ¬æ‘˜è¦æ–‡ä»¶
            version_summary_file = os.path.join(self.data_paths["results_dir"], "version_summary.json")
            version_summary = {
                "version": self.version,
                "created_at": datetime.now().isoformat(),
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "performance": {
                    "final_f1_score": complete_results["results"].get("model_optimization", {}).get("best_score", 0),
                    "selected_features_count": len(complete_results["results"].get("feature_selection", {}).get("best_features", [])),
                    "overfitting_risk": complete_results["results"].get("overfitting_analysis", {}).get("overall_risk_level", "UNKNOWN")
                },
                "file_paths": {
                    "labels": self.data_paths["labels"],
                    "selected_features": self.data_paths["selected_features"], 
                    "results": results_file,
                    "models": self.data_paths["models_dir"]
                }
            }
            
            with open(version_summary_file, 'w', encoding='utf-8') as f:
                json.dump(version_summary, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“‹ ç‰ˆæœ¬æ‘˜è¦å·²ä¿å­˜åˆ°: {version_summary_file}")
            
            return results_file
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _extract_label_results(self, label_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ä¸¦æ ¼å¼åŒ–æ¨™ç±¤å„ªåŒ–çµæœ"""
        try:
            extracted = {
                "best_params": label_results.get("best_params", {}),
                "best_score": label_results.get("best_score", 0.0),
                "labels": self._safe_serialize_series(label_results.get("labels")),
                "n_pareto_solutions": label_results.get("n_pareto_solutions", 0),
                "cv_details": {
                    "pareto_solutions_count": label_results.get("n_pareto_solutions", 0),
                    "optimization_direction": ["maximize", "maximize"],  # F1 + Stability
                    "sampler_type": "NSGA-II"
                }
            }
            
            # å¦‚æœæœ‰Studyå°è±¡ï¼Œæå–è©¦é©—æ­·å²
            if "study" in label_results:
                study = label_results["study"]
                if hasattr(study, 'trials'):
                    extracted["trial_history"] = [
                        {
                            "number": trial.number,
                            "values": trial.values if trial.values else [0.0, 0.0],
                            "params": trial.params,
                            "state": trial.state.name if hasattr(trial.state, 'name') else 'COMPLETE'
                        }
                        for trial in study.trials[-20:]  # ä¿å­˜æœ€å¾Œ20æ¬¡trial
                    ]
            
            return extracted
            
        except Exception as e:
            print(f"âš ï¸ æ¨™ç±¤çµæœæå–å¤±æ•—: {e}")
            return label_results

    def _extract_feature_results(self, feature_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ä¸¦æ ¼å¼åŒ–ç‰¹å¾µé¸æ“‡çµæœ"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿å…¼å®¹æ€§ï¼Œä½¿ç”¨æ­£ç¡®çš„é”®å
            original_count = feature_results.get("original_feature_count", 0)
            selected_count = len(feature_results.get("best_features", []))
            
            extracted = {
                "best_params": feature_results.get("best_params", {}),
                "best_features": feature_results.get("best_features", []),
                "best_score": feature_results.get("best_score", 0.0),
                "feature_importance": self._safe_serialize_dataframe(
                    feature_results.get("feature_importance")
                ),
                # ğŸ”§ ä½¿ç”¨ä¼˜å…ˆè¯»å–çš„é”®å selection_pipeline
                "selection_pipeline": {
                    "original_count": original_count,
                    "final_count": selected_count,
                    "overall_ratio": selected_count / max(original_count, 1),
                    "pipeline": feature_results.get("selection_pipeline", {}).get("pipeline", "two_stage_selection"),
                    "stage2a_results": feature_results.get("stage2a_results", {}),
                    "stage2b_results": feature_results.get("stage2b_results", {})
                },
                # ğŸ”§ ä¿æŒå‘åå…¼å®¹
                "selection_details": {
                    "original_feature_count": original_count,
                    "selected_feature_count": selected_count,
                    "selection_ratio": selected_count / max(original_count, 1),
                    "selection_method": feature_results.get("best_params", {}).get("selector_method", "unknown")
                }
            }
            
            # æå–è©¦é©—æ­·å²
            if "study" in feature_results:
                study = feature_results["study"]
                if hasattr(study, 'trials'):
                    extracted["trial_history"] = [
                        {
                            "number": trial.number,
                            "value": trial.value if trial.value else 0.0,
                            "params": trial.params,
                            "state": trial.state.name if hasattr(trial.state, 'name') else 'COMPLETE'
                        }
                        for trial in study.trials[-20:]  # ä¿å­˜æœ€å¾Œ20æ¬¡trial
                    ]
            
            return extracted
            
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾µçµæœæå–å¤±æ•—: {e}")
            return feature_results

    def _extract_model_results(self, model_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ä¸¦æ ¼å¼åŒ–æ¨¡å‹å„ªåŒ–çµæœ"""
        try:
            extracted = {
                "best_params": model_results.get("best_params", {}),
                "best_score": model_results.get("best_score", 0.0),
                "cv_results": model_results.get("cv_results", {}),
                "model_details": {
                    "model_type": "LightGBM",
                    "optimization_metric": "F1-Score",
                    "cv_folds": 5,
                    "cv_method": "TimeSeriesSplit"
                }
            }
            
            # ğŸ” æå–CVè©³ç´°ä¿¡æ¯ç”¨æ–¼éåº¦æ“¬åˆæª¢æ¸¬
            cv_results = model_results.get("cv_results", {})
            if cv_results:
                fold_scores = cv_results.get("fold_scores", [])
                extracted["cv_detailed"] = {
                    "fold_scores": fold_scores,
                    "cv_mean": np.mean(fold_scores) if fold_scores else 0.0,
                    "cv_std": np.std(fold_scores) if fold_scores else 0.0,
                    "cv_min": np.min(fold_scores) if fold_scores else 0.0,
                    "cv_max": np.max(fold_scores) if fold_scores else 0.0,
                    "fold_score_range": np.max(fold_scores) - np.min(fold_scores) if fold_scores else 0.0
                }
            
            # æå–è©¦é©—æ­·å²å’Œæ”¶æ–‚ä¿¡æ¯
            if "study" in model_results:
                study = model_results["study"]
                if hasattr(study, 'trials'):
                    trial_values = [t.value for t in study.trials if t.value is not None]
                    extracted["convergence_analysis"] = {
                        "total_trials": len(study.trials),
                        "successful_trials": len(trial_values),
                        "best_trial_number": study.best_trial.number if hasattr(study, 'best_trial') else -1,
                        "score_progression": trial_values[-10:] if len(trial_values) >= 10 else trial_values,
                        "early_convergence": len(trial_values) < len(study.trials) * 0.5  # å¦‚æœè¶…éä¸€åŠè¢«å‰ªæ
                    }
                    
                    extracted["trial_history"] = [
                        {
                            "number": trial.number,
                            "value": trial.value if trial.value else 0.0,
                            "params": trial.params,
                            "state": trial.state.name if hasattr(trial.state, 'name') else 'COMPLETE'
                        }
                        for trial in study.trials[-20:]  # ä¿å­˜æœ€å¾Œ20æ¬¡trial
                    ]
            
            return extracted
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹çµæœæå–å¤±æ•—: {e}")
            return model_results

    def _analyze_overfitting(self) -> Dict[str, Any]:
        """å¢å¼·éåº¦æ“¬åˆé¢¨éšªåˆ†æ - ğŸ”§ ä¿®å¾©æª¢æ¸¬å¤±æ•ˆå•é¡Œï¼ŒåŸºæ–¼é‡åŒ–äº¤æ˜“é¢¨æ§æ¨™æº–"""
        overfitting_analysis = {
            "overall_risk_level": "LOW",
            "risk_factors": [],
            "recommendations": [],
            "detailed_metrics": {}
        }
        
        try:
            print("ğŸ” åŸ·è¡Œå¢å¼·éåº¦æ“¬åˆåˆ†æ...")
            
            # 1. æª¢æŸ¥æ¨™ç±¤å„ªåŒ–éšæ®µ
            if "label_optimization" in self.optimization_results:
                label_data = self.optimization_results["label_optimization"]
                
                # ğŸ“Š æ¨™ç±¤åˆ†æ•¸ç•°å¸¸æª¢æŸ¥ (åŸºæ–¼æ¥­ç•Œæ¨™æº–ä¿®æ­£)
                label_score = label_data.get("best_score", 0.0)
                if label_score > 0.95:  # ğŸ”§ ä¿®æ­£ï¼šF1åˆ†æ•¸>95%æ‰è¦–ç‚ºé«˜åº¦æ‡·ç–‘éåº¦æ“¬åˆ
                    overfitting_analysis["risk_factors"].append(
                        f"æ¨™ç±¤å„ªåŒ–F1åˆ†æ•¸æ¥µé«˜({label_score:.3f} > 0.95)ï¼Œé«˜åº¦æ‡·ç–‘éåº¦æ“¬åˆ"
                    )
                    overfitting_analysis["overall_risk_level"] = "HIGH"
                elif label_score > 0.85:  # F1 85-95%ç‚ºæ¥µå„ªç§€è¡¨ç¾ï¼Œéœ€è¦é©—è­‰ä½†ä¸ç›´æ¥åˆ¤å®šéæ“¬åˆ
                    overfitting_analysis["risk_factors"].append(
                        f"æ¨™ç±¤å„ªåŒ–F1åˆ†æ•¸æ¥µå„ªç§€({label_score:.3f} > 0.85)ï¼Œå»ºè­°é€²ä¸€æ­¥é©—è­‰"
                    )
                    overfitting_analysis["overall_risk_level"] = max(
                        overfitting_analysis["overall_risk_level"], "MEDIUM"
                    )
                
                # Paretoè§£æ•¸é‡æª¢æŸ¥
                n_pareto = label_data.get("n_pareto_solutions", 0)
                if n_pareto <= 1:
                    overfitting_analysis["risk_factors"].append(
                        "æ¨™ç±¤å„ªåŒ–åƒ…ç”¢ç”Ÿ1å€‹Paretoè§£ï¼Œå¯èƒ½éåº¦æ“¬åˆç‰¹å®šæ™‚é–“æ®µ"
                    )
                    overfitting_analysis["overall_risk_level"] = max(
                        overfitting_analysis["overall_risk_level"], "MEDIUM"
                    )
            
                overfitting_analysis["detailed_metrics"]["label_score"] = label_score
                overfitting_analysis["detailed_metrics"]["n_pareto_solutions"] = n_pareto
            
            # 2. æª¢æŸ¥ç‰¹å¾µé¸æ“‡éšæ®µ - ğŸ”§ å¢å¼·æª¢æ¸¬é‚è¼¯
            if "feature_selection" in self.optimization_results:
                feature_data = self.optimization_results["feature_selection"]
                
                # è™•ç†å…©ç¨®æ•¸æ“šçµæ§‹ (å…¼å®¹èˆŠç‰ˆå’Œæ–°ç‰ˆ)
                if "selection_pipeline" in feature_data:  # æ–°ç‰ˆ4éšæ®µæ¶æ§‹
                    pipeline = feature_data["selection_pipeline"]
                    original_count = pipeline.get("original_count", 0)
                    selected_count = pipeline.get("final_count", 0)
                    selection_ratio = pipeline.get("overall_ratio", 0.0)
                else:  # èˆŠç‰ˆçµæ§‹
                    selection_details = feature_data.get("selection_details", {})
                    original_count = selection_details.get("original_feature_count", 0)
                    selected_count = selection_details.get("selected_feature_count", 0)
                    selection_ratio = selection_details.get("selection_ratio", 0.0)
                
                # ğŸ”§ ä¿®å¾©ï¼šæª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
                if original_count == 0:
                    overfitting_analysis["risk_factors"].append(
                        "ç‰¹å¾µè¨ˆæ•¸ç•°å¸¸(åŸå§‹ç‰¹å¾µæ•¸=0)ï¼Œå¯èƒ½å­˜åœ¨æ•¸æ“šæµå•é¡Œ"
                    )
                    overfitting_analysis["overall_risk_level"] = "HIGH"
                elif selected_count == 0:
                    overfitting_analysis["risk_factors"].append(
                        "ç‰¹å¾µé¸æ“‡å¤±æ•—(é¸ä¸­ç‰¹å¾µæ•¸=0)ï¼Œç³»çµ±å­˜åœ¨åš´é‡å•é¡Œ"
                    )
                    overfitting_analysis["overall_risk_level"] = "HIGH"
                else:
                    # ç‰¹å¾µæ•¸é‡æª¢æŸ¥ (åŸºæ–¼é‡åŒ–äº¤æ˜“é¢¨æ§æ¨™æº–)
                    if selected_count == 1:
                        overfitting_analysis["risk_factors"].append(
                            f"æ¥µåº¦ç‰¹å¾µå£“ç¸®(åƒ…{selected_count}å€‹ç‰¹å¾µ)ï¼Œåš´é‡éåº¦æ“¬åˆé¢¨éšª"
                        )
                        overfitting_analysis["overall_risk_level"] = "CRITICAL"  # æ–°å¢å±æ€¥ç­‰ç´š
                    elif selected_count < 5:
                        overfitting_analysis["risk_factors"].append(
                            f"ç‰¹å¾µæ•¸éå°‘({selected_count}å€‹)ï¼Œç¼ºä¹è¶³å¤ ä¿¡æ¯å¤šæ¨£æ€§"
                        )
                        overfitting_analysis["overall_risk_level"] = "HIGH"
                    elif selection_ratio < 0.03:  # é¸ä¸­æ¯”ä¾‹<3%
                        overfitting_analysis["risk_factors"].append(
                            f"ç‰¹å¾µé¸æ“‡éåº¦å£“ç¸®(åƒ…ä¿ç•™{selection_ratio:.1%})ï¼Œè³‡è¨Šæå¤±åš´é‡"
                        )
                        overfitting_analysis["overall_risk_level"] = "HIGH"
                    elif selected_count < 10:
                        overfitting_analysis["risk_factors"].append(
                            f"ç‰¹å¾µæ•¸åå°‘({selected_count}å€‹)ï¼Œå»ºè­°å¢åŠ åˆ°15-25å€‹"
                        )
                        overfitting_analysis["overall_risk_level"] = "MEDIUM"
                
                # è¨˜éŒ„è©³ç´°æŒ‡æ¨™ - ğŸ”§ å¢å¼·èª¿è©¦ä¿¡æ¯
                overfitting_analysis["detailed_metrics"].update({
                    "original_feature_count": original_count,
                    "selected_feature_count": selected_count,
                    "selection_ratio": selection_ratio,
                    "feature_count_debug": {
                        "has_pipeline": "selection_pipeline" in feature_data,
                        "has_legacy": "selection_details" in feature_data,
                        "data_source": "pipeline" if "selection_pipeline" in feature_data else "legacy"
                    }
                })
                
                # ğŸ†• ç‰¹å¾µé‡è¦æ€§ç©©å®šæ€§åˆ†ææª¢æŸ¥
                feature_stability = feature_data.get("feature_importance_stability", {})
                if feature_stability:
                    stability_report = feature_stability.get("stability_report", {})
                    if stability_report:
                        stability_summary = stability_report.get("summary", {})
                        stability_score = stability_summary.get("overall_stability_score", 0.0)
                        stability_grade = stability_summary.get("overall_stability_grade", "N/A")
                        meets_threshold = stability_summary.get("meets_minimum_threshold", False)
                        
                        # ç©©å®šæ€§é¢¨éšªæª¢æŸ¥
                        if stability_score < 0.3:
                            overfitting_analysis["risk_factors"].append(
                                f"ç‰¹å¾µé‡è¦æ€§æ¥µä¸ç©©å®š(ç©©å®šæ€§:{stability_score:.2f} < 0.3)ï¼Œåš´é‡éæ“¬åˆé¢¨éšª"
                            )
                            overfitting_analysis["overall_risk_level"] = "CRITICAL"
                        elif stability_score < 0.5:
                            overfitting_analysis["risk_factors"].append(
                                f"ç‰¹å¾µé‡è¦æ€§ä¸ç©©å®š(ç©©å®šæ€§:{stability_score:.2f} < 0.5)ï¼Œæ¨¡å‹å¯èƒ½éæ“¬åˆ"
                            )
                            overfitting_analysis["overall_risk_level"] = max(
                                overfitting_analysis["overall_risk_level"], "HIGH"
                            )
                        elif stability_score < 0.6 and selected_count > 20:
                            overfitting_analysis["risk_factors"].append(
                                f"ç‰¹å¾µè¼ƒå¤šä¸”ç©©å®šæ€§ä¸€èˆ¬(ç©©å®šæ€§:{stability_score:.2f}, ç‰¹å¾µæ•¸:{selected_count})ï¼Œå»ºè­°æ¸›å°‘ç‰¹å¾µ"
                            )
                            overfitting_analysis["overall_risk_level"] = max(
                                overfitting_analysis["overall_risk_level"], "MEDIUM"
                            )
                        
                        # ç©©å®šæ€§å»ºè­°
                        recommendations = stability_report.get("recommendations", [])
                        if recommendations:
                            overfitting_analysis["recommendations"].extend([
                                f"ç‰¹å¾µç©©å®šæ€§å»ºè­°: {rec}" for rec in recommendations
                            ])
                        
                        # è¨˜éŒ„ç©©å®šæ€§æŒ‡æ¨™
                        overfitting_analysis["detailed_metrics"]["feature_stability"] = {
                            "overall_score": stability_score,
                            "grade": stability_grade,
                            "meets_threshold": meets_threshold,
                            "num_windows_analyzed": stability_summary.get("num_windows_analyzed", 0)
                        }
                    else:
                        overfitting_analysis["risk_factors"].append(
                            "ç‰¹å¾µé‡è¦æ€§ç©©å®šæ€§åˆ†ææœªå®Œæˆï¼Œç„¡æ³•è©•ä¼°æ™‚é–“ä¸€è‡´æ€§"
                        )
                        overfitting_analysis["overall_risk_level"] = max(
                            overfitting_analysis["overall_risk_level"], "MEDIUM"
                        )
            
            # 3. æª¢æŸ¥æ¨¡å‹å„ªåŒ–éšæ®µ - ğŸ”§ å¢å¼·æ€§èƒ½æª¢æ¸¬
            if "model_optimization" in self.optimization_results:
                model_data = self.optimization_results["model_optimization"]
                model_score = model_data.get("best_score", 0.0)
                
                # ğŸ“Š æ¨¡å‹åˆ†æ•¸ç•°å¸¸æª¢æŸ¥ (åŸºæ–¼æ¥­ç•Œæ¨™æº–ä¿®æ­£)
                if model_score > 0.95:  # F1>95%é«˜åº¦æ‡·ç–‘éåº¦æ“¬åˆ
                    overfitting_analysis["risk_factors"].append(
                        f"æ¨¡å‹F1åˆ†æ•¸æ¥µé«˜({model_score:.3f} > 0.95)ï¼Œé«˜åº¦æ‡·ç–‘éåº¦æ“¬åˆ"
                    )
                    overfitting_analysis["overall_risk_level"] = "CRITICAL"  # æ–°å¢å±æ€¥ç­‰ç´š
                elif model_score > 0.85:  # F1 85-95%ç‚ºæ¥µå„ªç§€è¡¨ç¾ï¼Œéœ€è¦å¤šç¶­åº¦é©—è­‰
                    overfitting_analysis["risk_factors"].append(
                        f"æ¨¡å‹F1åˆ†æ•¸æ¥µå„ªç§€({model_score:.3f} > 0.85)ï¼Œå»ºè­°å¤šç¶­åº¦é©—è­‰"
                    )
                    overfitting_analysis["overall_risk_level"] = max(
                        overfitting_analysis["overall_risk_level"], "MEDIUM"
                    )
                elif model_score > 0.75 and self.timeframe in ["5m", "15m"]:  # çŸ­æ™‚æ¡†æ›´åš´æ ¼ä½†åˆç†èª¿æ•´
                    overfitting_analysis["risk_factors"].append(
                        f"çŸ­æ™‚æ¡†F1åˆ†æ•¸å„ªç§€({model_score:.3f} > 0.75)ï¼Œå»ºè­°é¡å¤–é©—è­‰"
                    )
                    overfitting_analysis["overall_risk_level"] = max(
                        overfitting_analysis["overall_risk_level"], "LOW"
                    )
                
                # CVè©³ç´°åˆ†æ
                cv_detailed = model_data.get("cv_detailed", {})
                if cv_detailed:
                    cv_std = cv_detailed.get("cv_std", 0.0)
                    cv_max = cv_detailed.get("cv_max", 0.0)
                    cv_min = cv_detailed.get("cv_min", 0.0)
                    fold_score_range = cv_detailed.get("fold_score_range", 0.0)
                    
                    # CVæ¨™æº–å·®æª¢æŸ¥ (åŸºæ–¼æ¥­ç•Œæ¨™æº–ä¿®æ­£)
                    if cv_std > 0.05:  # ğŸ”§ ä¿®æ­£ï¼šæ¨™æº–å·®>5%ç‚ºé«˜é¢¨éšª (æ¥­ç•Œæ¨è–¦)
                        overfitting_analysis["risk_factors"].append(
                            f"CVæŠ˜é–“å·®ç•°éå¤§(std={cv_std:.4f} > 0.05)ï¼Œæ¨¡å‹ä¸ç©©å®šï¼Œå¯èƒ½éåº¦æ“¬åˆ"
                        )
                        overfitting_analysis["overall_risk_level"] = "HIGH"
                    elif cv_std > 0.03:  # æ¨™æº–å·®>3%ç‚ºä¸­ç­‰é¢¨éšª
                        overfitting_analysis["risk_factors"].append(
                            f"CVæŠ˜é–“å·®ç•°è¼ƒå¤§(std={cv_std:.4f} > 0.03)ï¼Œå»ºè­°å¢åŠ æ¨£æœ¬æˆ–èª¿æ•´æ¨¡å‹"
                        )
                        overfitting_analysis["overall_risk_level"] = max(
                            overfitting_analysis["overall_risk_level"], "MEDIUM"
                        )
                    
                    # ğŸ”§ æ–°å¢ï¼šè¨“ç·´/é©—è­‰å·®è·æª¢æŸ¥ (æ¥­ç•Œæ¨è–¦å¤šç¶­åº¦æª¢æ¸¬)
                    if cv_max > 0 and cv_min > 0:
                        performance_gap = (cv_max - cv_min) / cv_min
                        if performance_gap > 0.15:  # å·®è·>15%ç‚ºé¢¨éšª
                            overfitting_analysis["risk_factors"].append(
                                f"ä¸åŒæ™‚æœŸè¡¨ç¾å·®è·éå¤§({performance_gap:.1%})ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³"
                            )
                            overfitting_analysis["overall_risk_level"] = max(
                                overfitting_analysis["overall_risk_level"], "MEDIUM"
                            )
                    
                    # CVç¯„åœæª¢æŸ¥
                    if fold_score_range > 0.15:  # æœ€é«˜æœ€ä½å·®>15%
                        overfitting_analysis["risk_factors"].append(
                            f"CVæŠ˜é–“åˆ†æ•¸ç¯„åœéå¤§({fold_score_range:.3f})ï¼Œä¸åŒæ•¸æ“šæ®µè¡¨ç¾å·®ç•°æ¥µå¤§"
                        )
                        overfitting_analysis["overall_risk_level"] = "HIGH"
                    
                    overfitting_analysis["detailed_metrics"].update({
                        "cv_std": cv_std,
                        "cv_range": fold_score_range,
                        "cv_max": cv_max,
                        "cv_min": cv_min
                    })
                
                # æ¨¡å‹è¤‡é›œåº¦æª¢æŸ¥
                best_params = model_data.get("best_params", {})
                n_estimators = best_params.get("n_estimators", 0)
                max_depth = best_params.get("max_depth", 0)
                num_leaves = best_params.get("num_leaves", 0)
                
                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦éæ–¼è¤‡é›œ
                if n_estimators > 500 and num_leaves > 60:
                    overfitting_analysis["risk_factors"].append(
                        f"æ¨¡å‹éåº¦è¤‡é›œ(trees={n_estimators}, leaves={num_leaves})ï¼Œå®¹æ˜“éæ“¬åˆ"
                    )
                    overfitting_analysis["overall_risk_level"] = max(
                        overfitting_analysis["overall_risk_level"], "MEDIUM"
                    )
                
                overfitting_analysis["detailed_metrics"].update({
                    "model_score": model_score,
                    "model_complexity": {"n_estimators": n_estimators, "max_depth": max_depth, "num_leaves": num_leaves}
                })
                
                # æ”¶æ–‚æª¢æŸ¥
                convergence = model_data.get("convergence_analysis", {})
                early_convergence = convergence.get("early_convergence", False)
                if early_convergence:
                    overfitting_analysis["risk_factors"].append(
                        "æ¨¡å‹å„ªåŒ–æå‰æ”¶æ–‚ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª"
                    )
            
                # ğŸ†• CV vs WFA ä¸€è‡´æ€§æª¢æŸ¥
                trial_results = model_data.get("trial_results", {})
                if trial_results:
                    best_trial_num = model_data.get("best_trial_number", None)
                    if best_trial_num and str(best_trial_num) in trial_results:
                        best_trial_data = trial_results[str(best_trial_num)]
                        cv_vs_oos_analysis = best_trial_data.get("cv_vs_oos_analysis", {})
                        
                        if cv_vs_oos_analysis:
                            oos_delta_pct = cv_vs_oos_analysis.get("delta_pct", 0.0)
                            consistency_status = cv_vs_oos_analysis.get("consistency_status", "æœªçŸ¥")
                            cv_mean = cv_vs_oos_analysis.get("cv_mean", 0.0)
                            oos_mean = cv_vs_oos_analysis.get("oos_mean", 0.0)
                            
                            # CV vs WFA ä¸€è‡´æ€§é¢¨éšªæª¢æŸ¥
                            if oos_delta_pct > 0.20:  # å·®ç•°>20%ç‚ºåš´é‡ä¸ä¸€è‡´
                                overfitting_analysis["risk_factors"].append(
                                    f"CVèˆ‡æ¨£æœ¬å¤–é©—è­‰åš´é‡ä¸ä¸€è‡´(å·®ç•°:{oos_delta_pct:.1%} > 20%)ï¼Œé«˜åº¦æ‡·ç–‘éæ“¬åˆ"
                                )
                                overfitting_analysis["overall_risk_level"] = "CRITICAL"
                            elif oos_delta_pct > 0.15:  # å·®ç•°>15%ç‚ºä¸­ç­‰ä¸ä¸€è‡´
                                overfitting_analysis["risk_factors"].append(
                                    f"CVèˆ‡æ¨£æœ¬å¤–é©—è­‰ä¸ä¸€è‡´(å·®ç•°:{oos_delta_pct:.1%} > 15%)ï¼Œå¯èƒ½éæ“¬åˆ"
                                )
                                overfitting_analysis["overall_risk_level"] = max(
                                    overfitting_analysis["overall_risk_level"], "HIGH"
                                )
                            elif oos_delta_pct > 0.10:  # å·®ç•°>10%ç‚ºè¼•å¾®ä¸ä¸€è‡´
                                overfitting_analysis["risk_factors"].append(
                                    f"CVèˆ‡æ¨£æœ¬å¤–é©—è­‰è¼•å¾®ä¸ä¸€è‡´(å·®ç•°:{oos_delta_pct:.1%} > 10%)ï¼Œéœ€è¦é—œæ³¨"
                                )
                                overfitting_analysis["overall_risk_level"] = max(
                                    overfitting_analysis["overall_risk_level"], "MEDIUM"
                                )
                            
                            # è¨˜éŒ„CV vs WFAæŒ‡æ¨™
                            overfitting_analysis["detailed_metrics"]["cv_vs_oos_consistency"] = {
                                "delta_pct": oos_delta_pct,
                                "consistency_status": consistency_status,
                                "cv_mean": cv_mean,
                                "oos_mean": oos_mean,
                                "is_consistent": oos_delta_pct <= 0.10
                            }
                            
                            print(f"ğŸ“Š CV vs WFAä¸€è‡´æ€§: CV={cv_mean:.4f}, WFA={oos_mean:.4f}, å·®ç•°={oos_delta_pct:.1%}")
                        else:
                            overfitting_analysis["risk_factors"].append(
                                "ç¼ºå°‘CVèˆ‡æ¨£æœ¬å¤–é©—è­‰å°æ¯”ï¼Œç„¡æ³•è©•ä¼°æ³›åŒ–ä¸€è‡´æ€§"
                            )
                            overfitting_analysis["overall_risk_level"] = max(
                                overfitting_analysis["overall_risk_level"], "MEDIUM"
                            )
            
            # 4. æª¢æŸ¥æ¨™ç±¤å¹³è¡¡æ€§ - ğŸ”§ æ–°å¢é—œéµæŒ‡æ¨™
            try:
                # å˜—è©¦è¼‰å…¥æœ€æ–°çš„æ¨™ç±¤æ•¸æ“šé€²è¡Œå¹³è¡¡æ€§æª¢æŸ¥
                labels_path = f"data/processed/labels/{self.symbol}_{self.timeframe}/{self.version}/{self.symbol}_{self.timeframe}_labels.parquet"
                if os.path.exists(labels_path):
                    labels_df = pd.read_parquet(labels_path)
                    label_counts = labels_df["label"].value_counts()
                    total_samples = len(labels_df)
                    
                    # è¨ˆç®—æ¨™ç±¤åˆ†ä½ˆ
                    label_distribution = {}
                    for label, count in label_counts.items():
                        label_distribution[f"label_{int(label)}"] = {
                            "count": int(count),
                            "percentage": float(count / total_samples * 100)
                        }
                    
                    # æª¢æŸ¥åš´é‡ä¸å¹³è¡¡ - ğŸš¨ é—œéµé¢¨éšªæª¢æ¸¬
                    max_percentage = max([dist["percentage"] for dist in label_distribution.values()])
                    if max_percentage > 90:  # è¶…é90%ç‚ºä¸€å€‹é¡åˆ¥
                        overfitting_analysis["risk_factors"].append(
                            f"æ¨™ç±¤æ¥µåº¦ä¸å¹³è¡¡ï¼šæœ€å¤§é¡åˆ¥ä½”{max_percentage:.1f}%ï¼Œåš´é‡å½±éŸ¿æ¨¡å‹å­¸ç¿’"
                        )
                        overfitting_analysis["overall_risk_level"] = "CRITICAL"
                    elif max_percentage > 80:  # è¶…é80%ç‚ºä¸€å€‹é¡åˆ¥
                        overfitting_analysis["risk_factors"].append(
                            f"æ¨™ç±¤åš´é‡ä¸å¹³è¡¡ï¼šæœ€å¤§é¡åˆ¥ä½”{max_percentage:.1f}%ï¼Œæ¨¡å‹åå‘æ€§å¼·"
                        )
                        overfitting_analysis["overall_risk_level"] = "HIGH"
                    elif max_percentage > 70:  # è¶…é70%ç‚ºä¸€å€‹é¡åˆ¥
                        overfitting_analysis["risk_factors"].append(
                            f"æ¨™ç±¤ä¸å¹³è¡¡ï¼šæœ€å¤§é¡åˆ¥ä½”{max_percentage:.1f}%ï¼Œå»ºè­°é‡æ–°èª¿æ•´é–¾å€¼"
                        )
                        overfitting_analysis["overall_risk_level"] = max(
                            overfitting_analysis["overall_risk_level"], "MEDIUM"
                        )
                    
                    # æª¢æŸ¥ä¸‰åˆ†é¡æ¨™ç±¤åˆ†ä½ˆçš„åˆç†æ€§
                    if len(label_distribution) == 3:
                        percentages = [dist["percentage"] for dist in label_distribution.values()]
                        min_percentage = min(percentages)
                        if min_percentage < 5:  # æŸé¡åˆ¥å°‘æ–¼5%
                            overfitting_analysis["risk_factors"].append(
                                f"æ¨™ç±¤é¡åˆ¥éå°‘ï¼šæœ€å°é¡åˆ¥åƒ…{min_percentage:.1f}%ï¼Œå­¸ç¿’æ¨£æœ¬ä¸è¶³"
                            )
                            overfitting_analysis["overall_risk_level"] = max(
                                overfitting_analysis["overall_risk_level"], "MEDIUM"
                            )
                    
                    # è¨˜éŒ„è©³ç´°æ¨™ç±¤åˆ†ä½ˆä¿¡æ¯
                    overfitting_analysis["detailed_metrics"]["label_distribution"] = label_distribution
                    overfitting_analysis["detailed_metrics"]["label_balance_ratio"] = max_percentage / min([dist["percentage"] for dist in label_distribution.values()])
                    
                    print(f"ğŸ“Š æ¨™ç±¤åˆ†ä½ˆæª¢æŸ¥å®Œæˆï¼š{len(label_distribution)}é¡ï¼Œæœ€å¤§ä½”æ¯”{max_percentage:.1f}%")
                else:
                    print("âš ï¸ æ¨™ç±¤æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³éå¹³è¡¡æ€§æª¢æŸ¥")
            except Exception as e:
                print(f"âš ï¸ æ¨™ç±¤å¹³è¡¡æ€§æª¢æŸ¥å¤±æ•—: {e}")
            
            # 5. ç”Ÿæˆåˆ†ç´šå»ºè­° - ğŸ”§ åŸºæ–¼é¢¨éšªç­‰ç´š
            risk_level = overfitting_analysis["overall_risk_level"]
            
            if risk_level == "CRITICAL":
                overfitting_analysis["recommendations"].extend([
                    "ğŸš¨ ç·Šæ€¥ï¼šç«‹å³åœæ­¢ä½¿ç”¨ç•¶å‰æ¨¡å‹ï¼Œå­˜åœ¨åš´é‡éåº¦æ“¬åˆ",
                    "ğŸ“Š é‡æ–°æ¡æ¨£æ•¸æ“šï¼šè€ƒæ…®å¾1mæ•¸æ“šé‡æ–°ç”Ÿæˆæ›´é•·æ™‚é–“çª—å£çš„æ•¸æ“š",
                    "ğŸ”§ å¤§å¹…é™ä½æ¨¡å‹è¤‡é›œåº¦ï¼šæ¸›å°‘ç‰¹å¾µæ•¸å’Œæ¨¹çš„æ•¸é‡",
                    "ğŸ“ˆ ä½¿ç”¨ç¨ç«‹çš„é•·æœŸç•™å­˜é›†(6å€‹æœˆä»¥ä¸Š)é€²è¡Œé©—è­‰",
                    "âš™ï¸ é‡æ–°è¨­è¨ˆç‰¹å¾µå·¥ç¨‹ï¼Œé¿å…æœªä¾†ä¿¡æ¯æ´©éœ²",
                    "ğŸ¯ èª¿æ•´æ¨™ç±¤é–¾å€¼ï¼šé‡æ–°è¨­è¨ˆthreshold_rangeä»¥å¹³è¡¡æ¨™ç±¤åˆ†ä½ˆ",
                    "âš–ï¸ å¯¦æ–½é‡æ¡æ¨£ç­–ç•¥ï¼šè€ƒæ…®SMOTEæˆ–undersamplingå¹³è¡¡é¡åˆ¥"
                ])
            elif risk_level == "HIGH":
                overfitting_analysis["recommendations"].extend([
                    "âš ï¸ é«˜é¢¨éšªï¼šéœ€è¦å¤šç¶­åº¦é©—è­‰ç¢ºèªæ¨¡å‹å¯é æ€§",
                    "ğŸ“Š CVæ¨™æº–å·®æª¢æ¸¬ï¼šæª¢æŸ¥æ¨¡å‹åœ¨ä¸åŒæ™‚æœŸçš„ç©©å®šæ€§",
                    "ğŸ”„ æ¨£æœ¬å¤–å›æ¸¬ï¼šä½¿ç”¨æœªä¾†6å€‹æœˆæ•¸æ“šé€²è¡Œç¨ç«‹é©—è­‰",
                    "ğŸ”§ å¢åŠ æ­£å‰‡åŒ–ç´„æŸ(L1/L2)å’Œæ—©åœæ©Ÿåˆ¶",
                    "ğŸ“ˆ ç‰¹å¾µç©©å®šæ€§åˆ†æï¼šæª¢æŸ¥ç‰¹å¾µé‡è¦æ€§æ˜¯å¦ä¸€è‡´",
                    "âš–ï¸ æª¢æŸ¥æ¨™ç±¤åˆ†ä½ˆï¼šèª¿æ•´thresholdåƒæ•¸ä»¥æ”¹å–„é¡åˆ¥å¹³è¡¡"
                ])
            elif risk_level == "MEDIUM":
                overfitting_analysis["recommendations"].extend([
                    "ğŸ“Š å»ºè­°åœ¨ä¸åŒå¸‚å ´ç’°å¢ƒä¸‹æ¸¬è©¦æ¨¡å‹ç©©å¥æ€§",
                    "ğŸ”„ è€ƒæ…®å¤šæ™‚æ¡†è¯åˆé©—è­‰å’Œäº¤å‰é©—è­‰",
                    "âš™ï¸ ç›£æ§æ¨¡å‹åœ¨å¯¦éš›äº¤æ˜“ä¸­çš„é€€åŒ–æƒ…æ³",
                    "ğŸ“ˆ å®šæœŸé‡æ–°è¨“ç·´å’Œåƒæ•¸èª¿æ•´",
                    "âš–ï¸ ç›£æ§æ¨™ç±¤åˆ†ä½ˆè®ŠåŒ–ï¼šç¢ºä¿è¨“ç·´å’Œæ¸¬è©¦æœŸé–“çš„ä¸€è‡´æ€§"
                ])
            else:
                overfitting_analysis["recommendations"].append(
                    "âœ… ç•¶å‰å„ªåŒ–çµæœé¡¯ç¤ºè¼ƒä½çš„éåº¦æ“¬åˆé¢¨éšªï¼Œå¯ä»¥è¬¹æ…é€²å…¥å›æ¸¬éšæ®µ"
                )
            
            # 5. è¨˜éŒ„åˆ†ææ‘˜è¦
            risk_count = len(overfitting_analysis["risk_factors"])
            overfitting_analysis["analysis_summary"] = {
                "total_risk_factors": risk_count,
                "analysis_date": datetime.now().isoformat(),
                "timeframe_analyzed": self.timeframe,
                "recommendation_priority": "HIGH" if risk_level in ["CRITICAL", "HIGH"] else "MEDIUM"
            }
            
            print(f"ğŸ” éåº¦æ“¬åˆåˆ†æå®Œæˆ: {risk_level}é¢¨éšª, {risk_count}å€‹é¢¨éšªå› å­")
            
        except Exception as e:
            overfitting_analysis["error"] = f"éåº¦æ“¬åˆåˆ†æå¤±æ•—: {e}"
            import traceback
            print(f"âš ï¸ éåº¦æ“¬åˆåˆ†æéŒ¯èª¤: {traceback.format_exc()}")
        
        return overfitting_analysis

    def _safe_serialize_series(self, series):
        """å®‰å…¨åºåˆ—åŒ–pandas Series"""
        if series is None:
            return None
        try:
            if hasattr(series, 'to_dict'):
                return {str(k): v for k, v in series.to_dict().items()}
            return series
        except:
            return None

    def _safe_serialize_dataframe(self, df):
        """å®‰å…¨åºåˆ—åŒ–pandas DataFrame"""
        if df is None:
            return None
        try:
            if hasattr(df, 'to_dict'):
                return df.to_dict('records')
            return df
        except:
            return None

    def clean_results_for_json(self, results: Dict[str, Any]):
        """æ¸…ç†çµæœä¸­ä¸å¯JSONåºåˆ—åŒ–çš„å°è±¡"""
        for stage_key, stage_results in results.items():
            if isinstance(stage_results, dict):
                # ç§»é™¤Optuna Studyå°è±¡
                if 'study' in stage_results:
                    del stage_results['study']
                
                # ç§»é™¤æ¨¡å‹å°è±¡
                if 'best_model' in stage_results:
                    del stage_results['best_model']
                
                # è™•ç†pandaså°è±¡å’ŒTimestamp
                for key, value in list(stage_results.items()):
                    if hasattr(value, 'to_dict'):
                        if hasattr(value, 'index') and len(value.index) > 0:
                            # æ˜¯Seriesæˆ–DataFrameï¼Œä¸”æœ‰æ™‚é–“ç´¢å¼•
                            try:
                                stage_results[key] = {
                                    str(k): v for k, v in value.to_dict().items()
                                }
                            except:
                                stage_results[key] = None
                    elif isinstance(value, (pd.Timestamp, datetime)):
                        stage_results[key] = value.isoformat()
                    elif isinstance(value, dict):
                        # éæ­¸è™•ç†åµŒå¥—å­—å…¸
                        self._clean_nested_dict(value)

    def _clean_nested_dict(self, d: dict):
        """éæ­¸æ¸…ç†åµŒå¥—å­—å…¸ä¸­çš„ä¸å¯åºåˆ—åŒ–å°è±¡"""
        for key, value in list(d.items()):
            if isinstance(value, (pd.Timestamp, datetime)):
                d[key] = value.isoformat()
            elif isinstance(value, dict):
                self._clean_nested_dict(value)
            elif hasattr(value, 'to_dict'):
                try:
                    d[key] = {str(k): v for k, v in value.to_dict().items()}
                except:
                    d[key] = None
    
    def generate_optimization_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆå„ªåŒ–æ‘˜è¦"""
        summary = {
            "total_stages": 3,
            "completed_stages": 0,
            "failed_stages": 0,
            "total_duration": 0.0,
            "final_performance": {},
            "resource_usage": {}
        }
        
        # çµ±è¨ˆåŸ·è¡Œæƒ…æ³
        for log_entry in self.execution_log:
            if log_entry["status"] == "success":
                summary["completed_stages"] += 1
            elif log_entry["status"] == "failed":
                summary["failed_stages"] += 1
            
            summary["total_duration"] += log_entry.get("duration_seconds", 0)
        
        # æå–æœ€çµ‚æ€§èƒ½
        if "model_optimization" in self.optimization_results:
            model_results = self.optimization_results["model_optimization"]
            summary["final_performance"] = {
                "best_f1_score": model_results.get("best_score", 0),
                "selected_features_count": len(model_results.get("cv_results", {}).get("fold_scores", [])),
                "model_complexity": {
                    "n_estimators": model_results.get("best_params", {}).get("n_estimators", 0),
                    "max_depth": model_results.get("best_params", {}).get("max_depth", 0),
                    "num_leaves": model_results.get("best_params", {}).get("num_leaves", 0)
                }
            }
        
        return summary
    
    def _run_automated_consistency_check(self) -> Dict[str, Any]:
        """
        ğŸ†• è‡ªå‹•åŒ–CV vs WFAä¸€è‡´æ€§å¾©æ ¸ - åµŒå…¥main_optimizeræœ€å¾Œéšæ®µ
        
        è‡ªå‹•è®€å–CVçµæœèˆ‡WFAçµæœï¼Œä¸¦è¼¸å‡ºçµ±ä¸€JSONå ±å‘Š
        """
        print("\n" + "="*60)
        print("ğŸ” è‡ªå‹•åŒ–ä¸€è‡´æ€§å¾©æ ¸")
        print("="*60)
        
        try:
            consistency_report = {
                "timestamp": datetime.now().isoformat(),
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "version": self.version,
                "status": "pending",
                "cv_results": {},
                "wfa_results": {},
                "consistency_analysis": {},
                "recommendations": []
            }
            
            # 1. æå–CVçµæœ
            model_data = self.optimization_results.get("model_optimization", {})
            cv_results = model_data.get("cv_results", {})
            
            if cv_results:
                cv_fold_scores = cv_results.get("fold_scores", [])
                if cv_fold_scores:
                    consistency_report["cv_results"] = {
                        "mean": float(np.mean(cv_fold_scores)),
                        "std": float(np.std(cv_fold_scores)),
                        "folds": len(cv_fold_scores),
                        "scores": [float(s) for s in cv_fold_scores],
                        "min": float(np.min(cv_fold_scores)),
                        "max": float(np.max(cv_fold_scores))
                    }
                    print(f"ğŸ“Š CVçµæœ: {consistency_report['cv_results']['mean']:.4f} Â± {consistency_report['cv_results']['std']:.4f}")
            
            # 2. æå–WFAçµæœï¼ˆå¾trial_resultsä¸­æŸ¥æ‰¾ï¼‰
            trial_results = model_data.get("trial_results", {})
            best_trial_num = model_data.get("best_trial_number", model_data.get("cv_results", {}).get("best_trial_number"))
            
            wfa_data = None
            if best_trial_num and str(best_trial_num) in trial_results:
                best_trial_data = trial_results[str(best_trial_num)]
                wfa_summary = best_trial_data.get("oos_summary", {})
                
                if wfa_summary:
                    consistency_report["wfa_results"] = {
                        "mean": float(wfa_summary.get("mean", 0.0)),
                        "std": float(wfa_summary.get("std", 0.0)),
                        "folds": int(wfa_summary.get("folds", 0)),
                        "scores": [float(s) for s in wfa_summary.get("scores", [])],
                        "min": float(np.min(wfa_summary.get("scores", [0]))),
                        "max": float(np.max(wfa_summary.get("scores", [0])))
                    }
                    wfa_data = consistency_report["wfa_results"]
                    print(f"ğŸ“Š WFAçµæœ: {wfa_data['mean']:.4f} Â± {wfa_data['std']:.4f}")
            
            # 3. è¨ˆç®—ä¸€è‡´æ€§æŒ‡æ¨™
            cv_data = consistency_report["cv_results"]
            if cv_data and wfa_data and cv_data.get("mean", 0) > 0:
                cv_mean = cv_data["mean"]
                wfa_mean = wfa_data["mean"]
                delta_pct = abs(wfa_mean - cv_mean) / cv_mean * 100
                
                consistency_report["consistency_analysis"] = {
                    "delta_pct": float(delta_pct),
                    "delta_abs": float(abs(wfa_mean - cv_mean)),
                    "is_consistent": delta_pct <= 10.0,
                    "consistency_level": self._get_consistency_level(delta_pct),
                    "cv_vs_wfa_ratio": float(wfa_mean / cv_mean) if cv_mean > 0 else 0.0
                }
                
                print(f"ğŸ“Š ä¸€è‡´æ€§åˆ†æ: å·®ç•° {delta_pct:.2f}% ({consistency_report['consistency_analysis']['consistency_level']})")
                
                # é¢¨éšªè©•ä¼°èˆ‡å»ºè­°
                consistency_report["recommendations"] = self._generate_consistency_recommendations(
                    delta_pct, cv_data, wfa_data
                )
                
                consistency_report["status"] = "completed"
            else:
                print("âš ï¸ ç¼ºå°‘CVæˆ–WFAçµæœï¼Œç„¡æ³•å®Œæˆä¸€è‡´æ€§åˆ†æ")
                consistency_report["status"] = "incomplete"
                consistency_report["recommendations"] = [
                    "ç¼ºå°‘WFAçµæœï¼Œå»ºè­°é‡æ–°é‹è¡Œæ¨¡å‹å„ªåŒ–ä¸¦å•Ÿç”¨WFA",
                    "ç¢ºä¿enhanced_regularization.compare_cv_vs_oos=True"
                ]
            
            # 4. è¨ˆç®—æ¨™ç±¤åˆ†ä½ˆå’ŒMacro F1ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                labels_path = f"data/processed/labels/{self.symbol}_{self.timeframe}/{self.version}/{self.symbol}_{self.timeframe}_labels.parquet"
                if os.path.exists(labels_path):
                    labels_df = pd.read_parquet(labels_path)
                    label_counts = labels_df["label"].value_counts().sort_index()
                    total_samples = len(labels_df)
                    
                    label_distribution = {}
                    for label, count in label_counts.items():
                        label_distribution[f"label_{int(label)}"] = {
                            "count": int(count),
                            "percentage": float(count / total_samples * 100)
                        }
                    
                    consistency_report["label_distribution"] = label_distribution
                    
                    # è¨ˆç®—å¹³è¡¡åˆ†æ•¸
                    percentages = [dist["percentage"] for dist in label_distribution.values()]
                    balance_score = min(percentages) / max(percentages) if max(percentages) > 0 else 0
                    consistency_report["label_balance_score"] = float(balance_score)
                    
                    print(f"ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {len(label_distribution)} é¡ï¼Œå¹³è¡¡åˆ†æ•¸: {balance_score:.3f}")
                    
                    # æ¨™ç±¤å¹³è¡¡å»ºè­°
                    if balance_score < 0.4:
                        consistency_report["recommendations"].append(
                            f"æ¨™ç±¤åš´é‡ä¸å¹³è¡¡ï¼ˆå¹³è¡¡åˆ†æ•¸={balance_score:.3f} < 0.4ï¼‰ï¼Œå»ºè­°é‡æ–°å„ªåŒ–æ¨™ç±¤åƒæ•¸"
                        )
                        
            except Exception as e:
                print(f"âš ï¸ æ¨™ç±¤åˆ†ä½ˆåˆ†æå¤±æ•—: {e}")
            
            # 5. ä¿å­˜ä¸€è‡´æ€§å ±å‘Šåˆ°ç‰ˆæœ¬ç›®éŒ„
            try:
                consistency_file = f"results/models/{self.symbol}_{self.timeframe}/{self.version}/consistency_check_report.json"
                os.makedirs(os.path.dirname(consistency_file), exist_ok=True)
                
                with open(consistency_file, 'w', encoding='utf-8') as f:
                    json.dump(consistency_report, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ ä¸€è‡´æ€§å ±å‘Šå·²ä¿å­˜åˆ°: {consistency_file}")
                
            except Exception as e:
                print(f"âš ï¸ ä¸€è‡´æ€§å ±å‘Šä¿å­˜å¤±æ•—: {e}")
            
            return consistency_report
            
        except Exception as e:
            print(f"âŒ è‡ªå‹•åŒ–ä¸€è‡´æ€§å¾©æ ¸å¤±æ•—: {e}")
            import traceback
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            
            return {
                "timestamp": datetime.now().isoformat(),
                "symbol": self.symbol,
                "timeframe": self.timeframe,
                "version": self.version,
                "status": "failed",
                "error": str(e),
                "recommendations": ["ä¸€è‡´æ€§å¾©æ ¸å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¨¡å‹å„ªåŒ–çµæœçš„å®Œæ•´æ€§"]
            }
    
    def _get_consistency_level(self, delta_pct: float) -> str:
        """ç²å–ä¸€è‡´æ€§æ°´å¹³æè¿°"""
        if delta_pct <= 5.0:
            return "å„ªç§€"
        elif delta_pct <= 10.0:
            return "è‰¯å¥½"
        elif delta_pct <= 15.0:
            return "ä¸€èˆ¬"
        elif delta_pct <= 20.0:
            return "è¼ƒå·®"
        else:
            return "ä¸ä¸€è‡´"
    
    def _generate_consistency_recommendations(self, delta_pct: float, 
                                            cv_data: Dict, wfa_data: Dict) -> List[str]:
        """ç”Ÿæˆä¸€è‡´æ€§å»ºè­°"""
        recommendations = []
        
        if delta_pct <= 5.0:
            recommendations.append("CVèˆ‡WFAé«˜åº¦ä¸€è‡´ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›å„ªç§€")
            recommendations.append("å¯ä»¥é€²è¡Œå¯¦ç›¤éƒ¨ç½²å‰çš„æœ€çµ‚é©—è­‰")
        elif delta_pct <= 10.0:
            recommendations.append("CVèˆ‡WFAåŸºæœ¬ä¸€è‡´ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
            recommendations.append("å»ºè­°é€²è¡Œæ›´å¤šæ¨£æœ¬å¤–æ¸¬è©¦ä»¥ç¢ºèªç©©å®šæ€§")
        elif delta_pct <= 15.0:
            recommendations.append("CVèˆ‡WFAå­˜åœ¨è¼•å¾®ä¸ä¸€è‡´ï¼Œéœ€è¦è¬¹æ…")
            recommendations.append("å»ºè­°é™ä½æ¨¡å‹è¤‡é›œåº¦æˆ–å¢åŠ æ­£å‰‡åŒ–")
            recommendations.append("è€ƒæ…®é‡æ–°é€²è¡Œç‰¹å¾µé¸æ“‡ä»¥æå‡ç©©å¥æ€§")
        elif delta_pct <= 20.0:
            recommendations.append("CVèˆ‡WFAä¸ä¸€è‡´ç¨‹åº¦è¼ƒé«˜ï¼Œå­˜åœ¨éæ“¬åˆé¢¨éšª")
            recommendations.append("å¼·çƒˆå»ºè­°é™ä½æ¨¡å‹è¤‡é›œåº¦ï¼ˆæ¸›å°‘æ¨¹æ·±åº¦ã€è‘‰å­æ•¸ï¼‰")
            recommendations.append("å¢åŠ æ­£å‰‡åŒ–åƒæ•¸ï¼ˆreg_alpha, reg_lambdaï¼‰")
            recommendations.append("æ¸›å°‘ç‰¹å¾µæ•¸é‡æˆ–é‡æ–°é€²è¡Œç‰¹å¾µé¸æ“‡")
        else:
            recommendations.append("CVèˆ‡WFAåš´é‡ä¸ä¸€è‡´ï¼Œæ¨¡å‹åš´é‡éæ“¬åˆ")
            recommendations.append("ä¸å»ºè­°éƒ¨ç½²è©²æ¨¡å‹")
            recommendations.append("éœ€è¦é‡æ–°è¨­è¨ˆç‰¹å¾µå·¥ç¨‹å’Œæ¨¡å‹æ¶æ§‹")
            recommendations.append("è€ƒæ…®ä½¿ç”¨æ›´ç°¡å–®çš„æ¨¡å‹æˆ–æ›´å¼·çš„æ­£å‰‡åŒ–")
        
        # åŸºæ–¼æ¨™æº–å·®çš„é¡å¤–å»ºè­°
        cv_std = cv_data.get("std", 0.0)
        wfa_std = wfa_data.get("std", 0.0)
        
        if cv_std > 0.05:
            recommendations.append(f"CVæ¨™æº–å·®è¼ƒå¤§({cv_std:.3f})ï¼Œæ¨¡å‹åœ¨ä¸åŒæ™‚æœŸè¡¨ç¾ä¸ç©©å®š")
        
        if wfa_std > 0.05:
            recommendations.append(f"WFAæ¨™æº–å·®è¼ƒå¤§({wfa_std:.3f})ï¼Œæ¨¡å‹åœ¨æ™‚é–“åºåˆ—ä¸Šä¸ç©©å®š")
        
        return recommendations
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€çµ‚ç¶œåˆå ±å‘Š"""
        summary = self.generate_optimization_summary()
        overfitting = self.optimization_results.get("overfitting_analysis", {})
        
        report = f"""
ğŸ‰ {'='*78} ğŸ‰
ğŸ† {self.symbol} {self.timeframe} æ¨¡çµ„åŒ–å„ªåŒ–ç³»çµ±ç¸½çµå ±å‘Š
ğŸ‰ {'='*78} ğŸ‰

ğŸ“Š **åŸ·è¡Œæ‘˜è¦**
â”œâ”€ å®Œæˆéšæ®µ: {summary['completed_stages']}/3
â”œâ”€ å¤±æ•—éšæ®µ: {summary['failed_stages']}
â”œâ”€ ç¸½åŸ·è¡Œæ™‚é–“: {summary['total_duration']:.1f} ç§’
â””â”€ å„ªåŒ–ç­–ç•¥: ä¸‰éšæ®µæ¨¡çµ„åŒ–å„ªåŒ–

ğŸ“ˆ **æ€§èƒ½æŒ‡æ¨™**"""
        
        if "final_performance" in summary and summary["final_performance"]:
            perf = summary["final_performance"]
            report += f"""
â”œâ”€ æœ€çµ‚F1åˆ†æ•¸: {perf.get('best_f1_score', 0):.6f}
â”œâ”€ é¸ä¸­ç‰¹å¾µæ•¸: {perf.get('selected_features_count', 0)}
â””â”€ æ¨¡å‹è¤‡é›œåº¦: {perf.get('model_complexity', {}).get('n_estimators', 0)} æ¨¹"""
        else:
            report += "\nâ””â”€ å„ªåŒ–æœªå®Œæˆæˆ–å¤±æ•—"
        
        # ğŸ” éåº¦æ“¬åˆé¢¨éšªè©•ä¼°
        if overfitting:
            risk_level = overfitting.get("overall_risk_level", "UNKNOWN")
            risk_factors = overfitting.get("risk_factors", [])
            
            report += f"""
            
ğŸ” **éåº¦æ“¬åˆé¢¨éšªè©•ä¼°**
â”œâ”€ é¢¨éšªç­‰ç´š: {risk_level}
â”œâ”€ è­˜åˆ¥é¢¨éšªå› å­: {len(risk_factors)} é …"""
            
            for i, factor in enumerate(risk_factors[:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                report += f"\nâ”‚  {i}. {factor}"
            
            if len(risk_factors) > 3:
                report += f"\nâ”‚  ... åŠå…¶ä»– {len(risk_factors) - 3} é …é¢¨éšª"

        report += f"""

ğŸ’¡ **ä¸‹ä¸€æ­¥å»ºè­°**
â”œâ”€ å¦‚éœ€é€²ä¸€æ­¥æå‡: è€ƒæ…®æ–¹æ¡ˆCæ“´å±•(é«˜éšç‰¹å¾µ+å¤šæ¨¡å‹é›†æˆ)
â”œâ”€ å¦‚éœ€éƒ¨ç½²: ä½¿ç”¨æœ€ä½³åƒæ•¸é‡æ–°è¨“ç·´å®Œæ•´æ¨¡å‹
â””â”€ å¦‚éœ€å›æ¸¬: çµåˆBacktraderé€²è¡Œç­–ç•¥é©—è­‰

ğŸ‰ {'='*78} ğŸ‰
"""

        return report

    def load_processed_data_for_training(self, version: str = None) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """
        è¼‰å…¥å·²è™•ç†çš„ç‰¹å¾µå’Œæ¨™ç±¤æ•¸æ“šï¼Œç”¨æ–¼æ¨¡å‹è¨“ç·´
        
        Args:
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿï¼Œå¦‚ä¸æŒ‡å®šå‰‡ä½¿ç”¨ç•¶å‰ç‰ˆæœ¬
            
        Returns:
            Tuple[pd.DataFrame, pd.Series, Dict]: é¸ä¸­çš„ç‰¹å¾µæ•¸æ“šã€æ¨™ç±¤ã€åƒæ•¸ä¿¡æ¯
        """
        # ğŸ”¢ ç¢ºå®šä½¿ç”¨çš„ç‰ˆæœ¬
        use_version = version or self.version
        
        # æ§‹å»ºç‰ˆæœ¬åŒ–è·¯å¾‘
        version_paths = self._get_version_paths(use_version)
        
        try:
            print(f"ğŸ“Š è¼‰å…¥å·²è™•ç†çš„è¨“ç·´æ•¸æ“š (ç‰ˆæœ¬: {use_version})...")
            
            # è¼‰å…¥é¸ä¸­çš„ç‰¹å¾µ
            if os.path.exists(version_paths["selected_features"]):
                features_df = pd.read_parquet(version_paths["selected_features"])
                print(f" âœ… è¼‰å…¥é¸ä¸­ç‰¹å¾µ: {features_df.shape}")
            else:
                print(f" âš ï¸ é¸ä¸­ç‰¹å¾µæ–‡ä»¶ä¸å­˜åœ¨: {version_paths['selected_features']}")
                return None, None, {}
            
            # è¼‰å…¥æ¨™ç±¤
            if os.path.exists(version_paths["labels"]):
                labels_df = pd.read_parquet(version_paths["labels"])
                labels = labels_df["label"]
                print(f" âœ… è¼‰å…¥æ¨™ç±¤: {len(labels)} å€‹æ¨£æœ¬")
            else:
                print(f" âš ï¸ æ¨™ç±¤æ–‡ä»¶ä¸å­˜åœ¨: {version_paths['labels']}")
                return None, None, {}
            
            # è¼‰å…¥åƒæ•¸ä¿¡æ¯
            optimization_info = {"version": use_version}
            
            # è¼‰å…¥æ¨™ç±¤åƒæ•¸
            label_params_path = version_paths["labels"].replace('_labels.parquet', '_label_params.json')
            if os.path.exists(label_params_path):
                with open(label_params_path, 'r', encoding='utf-8') as f:
                    optimization_info["label_params"] = json.load(f)
                print(f" âœ… è¼‰å…¥æ¨™ç±¤åƒæ•¸")
            
            # è¼‰å…¥ç‰¹å¾µé¸æ“‡åƒæ•¸
            feature_params_path = version_paths["selected_features"].replace('_selected_features.parquet', '_feature_selection_params.json')
            if os.path.exists(feature_params_path):
                with open(feature_params_path, 'r', encoding='utf-8') as f:
                    optimization_info["feature_selection_params"] = json.load(f)
                print(f" âœ… è¼‰å…¥ç‰¹å¾µé¸æ“‡åƒæ•¸")
            
            # æ•¸æ“šå°é½Š
            common_idx = features_df.index.intersection(labels.index)
            features_aligned = features_df.loc[common_idx]
            labels_aligned = labels.loc[common_idx]
            
            print(f" ğŸ”— å°é½Šå¾Œæ•¸æ“š: ç‰¹å¾µ{features_aligned.shape}, æ¨™ç±¤{len(labels_aligned)}å€‹")
            print(f" ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {np.bincount(labels_aligned)}")
            
            return features_aligned, labels_aligned, optimization_info
            
        except Exception as e:
            print(f"âŒ è¨“ç·´æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return None, None, {}
    
    def _get_version_paths(self, version: str) -> Dict[str, str]:
        """ç²å–æŒ‡å®šç‰ˆæœ¬çš„æ–‡ä»¶è·¯å¾‘"""
        return {
            "labels": f"data/processed/labels/{self.symbol}_{self.timeframe}/{version}/{self.symbol}_{self.timeframe}_labels.parquet",
            "selected_features": f"data/processed/features/{self.symbol}_{self.timeframe}/{version}/{self.symbol}_{self.timeframe}_selected_features.parquet",
            "results_dir": f"results/modular_optimization/{self.symbol}_{self.timeframe}/{version}",
            "models_dir": f"results/models/{self.symbol}_{self.timeframe}/{version}",
            "logs_dir": f"logs/optimization/{self.symbol}_{self.timeframe}/{version}"
        }
    
    def get_training_data_info(self, version: str = None) -> Dict[str, Any]:
        """
        ç²å–è¨“ç·´æ•¸æ“šçš„è©³ç´°ä¿¡æ¯
        
        Args:
            version: æŒ‡å®šç‰ˆæœ¬è™Ÿï¼Œå¦‚ä¸æŒ‡å®šå‰‡ä½¿ç”¨ç•¶å‰ç‰ˆæœ¬
            
        Returns:
            Dict: åŒ…å«æ•¸æ“šè·¯å¾‘ã€æ–‡ä»¶ç‹€æ…‹ç­‰ä¿¡æ¯
        """
        use_version = version or self.version
        version_paths = self._get_version_paths(use_version)
        
        info = {
            "symbol": self.symbol,
            "timeframe": self.timeframe,
            "current_version": self.version,
            "requested_version": use_version,
            "available_versions": self.list_versions(),
            "latest_version": self.get_latest_version() if self.list_versions() else "v1",
            "data_paths": version_paths,
            "file_status": {}
        }
        
        # æª¢æŸ¥ç‰ˆæœ¬åŒ–æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        key_files = ["labels", "selected_features"]
        for key in key_files:
            file_path = version_paths[key]
            info["file_status"][key] = {
                "path": file_path,
                "exists": os.path.exists(file_path),
                "size_mb": round(os.path.getsize(file_path) / 1024 / 1024, 2) if os.path.exists(file_path) else 0
            }
        
        return info
    
    def compare_versions(self, version1: str, version2: str) -> Dict[str, Any]:
        """æ¯”è¼ƒå…©å€‹ç‰ˆæœ¬çš„çµæœ"""
        comparison = {
            "version1": version1,
            "version2": version2,
            "comparison_results": {}
        }
        
        try:
            # è¼‰å…¥å…©å€‹ç‰ˆæœ¬çš„çµæœ
            results1_path = f"results/modular_optimization/{self.symbol}_{self.timeframe}/{version1}/modular_optuna_results.json"
            results2_path = f"results/modular_optimization/{self.symbol}_{self.timeframe}/{version2}/modular_optuna_results.json"
            
            if os.path.exists(results1_path) and os.path.exists(results2_path):
                with open(results1_path, 'r', encoding='utf-8') as f:
                    results1 = json.load(f)
                with open(results2_path, 'r', encoding='utf-8') as f:
                    results2 = json.load(f)
                
                # æ¯”è¼ƒé—œéµæŒ‡æ¨™
                comparison["comparison_results"] = {
                    "model_performance": {
                        "v1_score": results1.get("results", {}).get("model_optimization", {}).get("best_score", 0),
                        "v2_score": results2.get("results", {}).get("model_optimization", {}).get("best_score", 0)
                    },
                    "feature_selection": {
                        "v1_features": len(results1.get("results", {}).get("feature_selection", {}).get("best_features", [])),
                        "v2_features": len(results2.get("results", {}).get("feature_selection", {}).get("best_features", []))
                    },
                    "overfitting_risk": {
                        "v1_risk": results1.get("results", {}).get("overfitting_analysis", {}).get("overall_risk_level", "UNKNOWN"),
                        "v2_risk": results2.get("results", {}).get("overfitting_analysis", {}).get("overall_risk_level", "UNKNOWN")
                    }
                }
                
                # æ¨è–¦æœ€ä½³ç‰ˆæœ¬
                v1_score = comparison["comparison_results"]["model_performance"]["v1_score"]
                v2_score = comparison["comparison_results"]["model_performance"]["v2_score"]
                comparison["recommendation"] = version1 if v1_score > v2_score else version2
                
            else:
                comparison["error"] = "å…¶ä¸­ä¸€å€‹æˆ–å…©å€‹ç‰ˆæœ¬çš„çµæœæ–‡ä»¶ä¸å­˜åœ¨"
                
        except Exception as e:
            comparison["error"] = f"ç‰ˆæœ¬æ¯”è¼ƒå¤±æ•—: {e}"
        
        return comparison
    
    def delete_version(self, version: str) -> bool:
        """åˆªé™¤æŒ‡å®šç‰ˆæœ¬çš„æ‰€æœ‰æ•¸æ“š"""
        try:
            import shutil
            
            # è¦åˆªé™¤çš„ç›®éŒ„åˆ—è¡¨
            dirs_to_delete = [
                f"data/processed/features/{self.symbol}_{self.timeframe}/{version}",
                f"data/processed/labels/{self.symbol}_{self.timeframe}/{version}",
                f"results/modular_optimization/{self.symbol}_{self.timeframe}/{version}",
                f"results/models/{self.symbol}_{self.timeframe}/{version}",
                f"logs/optimization/{self.symbol}_{self.timeframe}/{version}"
            ]
            
            deleted_count = 0
            for dir_path in dirs_to_delete:
                if os.path.exists(dir_path):
                    shutil.rmtree(dir_path)
                    deleted_count += 1
                    print(f"âœ… å·²åˆªé™¤: {dir_path}")
            
            print(f"ğŸ—‘ï¸ ç‰ˆæœ¬ {version} å·²å®Œå…¨åˆªé™¤ ({deleted_count} å€‹ç›®éŒ„)")
            return True
            
        except Exception as e:
            print(f"âŒ åˆªé™¤ç‰ˆæœ¬ {version} å¤±æ•—: {e}")
            return False
    
    def run_stage3_only_optimization(self, n_trials: int = 100) -> Dict[str, Any]:
        """åªé‹è¡Œç¬¬ä¸‰éšæ®µæ¨¡å‹å„ªåŒ–ï¼ˆéœ€è¦å·²æœ‰çš„æ¨™ç±¤å’Œç‰¹å¾µæ•¸æ“šï¼‰"""
        print("ğŸ¤– é–‹å§‹åƒ…ç¬¬ä¸‰éšæ®µæ¨¡å‹å„ªåŒ–")
        print(f"äº¤æ˜“å°: {self.symbol}")
        print(f"æ™‚é–“æ¡†æ¶: {self.timeframe}")
        print("="*80)
        
        total_start_time = datetime.now()
        
        try:
            # ğŸ” æª¢æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ¨™ç±¤å’Œç‰¹å¾µæ•¸æ“š
            print("ğŸ” è¼‰å…¥å·²ä¿å­˜çš„æ¨™ç±¤å’Œç‰¹å¾µæ•¸æ“š...")
            
            # è¼‰å…¥æ¨™ç±¤æ•¸æ“š
            labels_path = self.data_paths["labels"]
            if not os.path.exists(labels_path):
                raise ValueError(f"æ¨™ç±¤æ•¸æ“šä¸å­˜åœ¨: {labels_path}")
            
            labels_df = pd.read_parquet(labels_path)
            if 'labels' not in labels_df.columns:
                raise ValueError("æ¨™ç±¤æ•¸æ“šæ ¼å¼éŒ¯èª¤ï¼Œç¼ºå°‘ 'labels' åˆ—")
            
            labels = labels_df['labels']
            print(f"âœ… è¼‰å…¥æ¨™ç±¤æ•¸æ“š: {len(labels)} å€‹æ¨£æœ¬")
            
            # è¼‰å…¥ç‰¹å¾µæ•¸æ“š
            selected_features_path = os.path.join(
                self.data_paths["features_dir"], 
                f"{self.symbol}_{self.timeframe}_selected_features.parquet"
            )
            
            if not os.path.exists(selected_features_path):
                raise ValueError(f"é¸å®šç‰¹å¾µæ•¸æ“šä¸å­˜åœ¨: {selected_features_path}")
            
            selected_features_df = pd.read_parquet(selected_features_path)
            print(f"âœ… è¼‰å…¥é¸å®šç‰¹å¾µæ•¸æ“š: {selected_features_df.shape}")
            
            # å°é½Šæ•¸æ“šé•·åº¦
            min_length = min(len(selected_features_df), len(labels))
            X_aligned = selected_features_df.iloc[:min_length]
            y_aligned = labels.iloc[:min_length]
            
            print(f"ğŸ“Š å°é½Šå¾Œæ•¸æ“š: ç‰¹å¾µ {X_aligned.shape}, æ¨™ç±¤ {len(y_aligned)}")
            
            # åŸ·è¡Œç¬¬ä¸‰éšæ®µï¼šæ¨¡å‹å„ªåŒ–
            print("\n" + "="*60)
            print("ğŸ¯ ç¬¬ä¸‰éšæ®µï¼šæ¨¡å‹è¶…åƒæ•¸å„ªåŒ–")
            print("="*60)
            
            model_results = self.model_optimizer.optimize(X_aligned, y_aligned, n_trials=n_trials)
            
            if not model_results:
                raise ValueError("æ¨¡å‹å„ªåŒ–å¤±æ•—")
            
            # ä¿å­˜çµæœ
            self.optimization_results["model_optimization"] = self._extract_model_results(model_results)
            
            # ä¿å­˜æœ€ä½³åƒæ•¸åˆ°JSON
            best_params = model_results.get('best_params', {})
            if best_params:
                self.save_optuna_params('model', best_params, model_results.get('best_score', 0.0))
            
            # ä¿å­˜æ¨¡å‹åƒæ•¸åˆ°ç‰ˆæœ¬åŒ–ç›®éŒ„
            try:
                model_params_path = os.path.join(
                    self.data_paths["models_dir"], 
                    f"{self.symbol}_{self.timeframe}_model_params.json"
                )
                
                os.makedirs(os.path.dirname(model_params_path), exist_ok=True)
                
                model_info = {
                    "best_params": best_params,
                    "best_score": model_results.get('best_score', 0.0),
                    "final_metrics": model_results.get('final_metrics', {}),
                    "n_trials": model_results.get('n_trials', 0),
                    "optimization_time": model_results.get('optimization_time', 0),
                    "version": self.version,
                    "symbol": self.symbol,
                    "timeframe": self.timeframe,
                    "timestamp": datetime.now().isoformat()
                }
                
                with open(model_params_path, 'w', encoding='utf-8') as f:
                    json.dump(model_info, f, indent=2, ensure_ascii=False, default=str)
                print(f"ğŸ’¾ æ¨¡å‹åƒæ•¸å·²ä¿å­˜åˆ°: {model_params_path}")
                
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åƒæ•¸ä¿å­˜å¤±æ•—: {e}")
            
            # ç”Ÿæˆå ±å‘Š
            final_report = self._generate_stage3_only_report(model_results)
            print(final_report)
            
            total_duration = (datetime.now() - total_start_time).total_seconds()
            self.log_execution("stage3_only_optimization", "success", 
                             f"ç¬¬ä¸‰éšæ®µå„ªåŒ–æˆåŠŸï¼Œè€—æ™‚ {total_duration:.1f} ç§’", total_duration)
            
            return self.optimization_results
            
        except Exception as e:
            total_duration = (datetime.now() - total_start_time).total_seconds()
            error_msg = f"ç¬¬ä¸‰éšæ®µå„ªåŒ–å¤±æ•—: {e}"
            print(f"âŒ {error_msg}")
            self.log_execution("stage3_only_optimization", "failed", error_msg, total_duration)
            return {}
    
    def _generate_stage3_only_report(self, model_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆåƒ…ç¬¬ä¸‰éšæ®µçš„å„ªåŒ–å ±å‘Š"""
        report = []
        report.append("\nğŸ‰ ç¬¬ä¸‰éšæ®µæ¨¡å‹å„ªåŒ–å®Œæˆå ±å‘Š")
        report.append("=" * 60)
        report.append(f"ğŸ“Š äº¤æ˜“å°: {self.symbol}")
        report.append(f"ğŸ“Š æ™‚é–“æ¡†æ¶: {self.timeframe}")
        report.append(f"ğŸ“Š ç‰ˆæœ¬: {self.version}")
        report.append("")
        
        # æ¨¡å‹å„ªåŒ–çµæœ
        if model_results:
            report.append("ğŸ¤– æ¨¡å‹å„ªåŒ–çµæœ:")
            report.append(f"â”œâ”€ æœ€ä½³F1åˆ†æ•¸: {model_results.get('best_score', 0):.6f}")
            report.append(f"â”œâ”€ å„ªåŒ–è©¦é©—æ•¸: {model_results.get('n_trials', 0)}")
            report.append(f"â””â”€ å„ªåŒ–è€—æ™‚: {model_results.get('optimization_time', 0):.1f}ç§’")
            report.append("")
            
            # æœ€ä½³åƒæ•¸
            best_params = model_results.get('best_params', {})
            if best_params:
                report.append("ğŸ† æœ€ä½³æ¨¡å‹åƒæ•¸:")
                for param, value in best_params.items():
                    if isinstance(value, float):
                        report.append(f"â”œâ”€ {param}: {value:.4f}")
                    else:
                        report.append(f"â”œâ”€ {param}: {value}")
                report.append("")
            
            # æœ€çµ‚æ¸¬è©¦æŒ‡æ¨™
            final_metrics = model_results.get('final_metrics', {})
            if final_metrics:
                report.append("ğŸ“ˆ æœ€çµ‚æ¸¬è©¦æŒ‡æ¨™:")
                for metric, value in final_metrics.items():
                    report.append(f"â”œâ”€ {metric}: {value:.4f}")
                report.append("")
        
        report.append("âœ… ç¬¬ä¸‰éšæ®µæ¨¡å‹å„ªåŒ–æˆåŠŸå®Œæˆï¼")
        report.append("ğŸ’¡ æ‚¨ç¾åœ¨å¯ä»¥ä½¿ç”¨å„ªåŒ–å¾Œçš„æ¨¡å‹é€²è¡Œé æ¸¬æˆ–å›æ¸¬ã€‚")
        
        return "\n".join(report)
    
    @classmethod
    def create_version_manager(cls, symbol: str, timeframe: str):
        """å‰µå»ºç‰ˆæœ¬ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆä¸é€²è¡Œå„ªåŒ–ï¼Œåƒ…ç”¨æ–¼ç‰ˆæœ¬ç®¡ç†ï¼‰"""
        return cls(symbol, timeframe, auto_version=False)


# ä¸»å‡½æ•¸ç¤ºä¾‹
if __name__ == "__main__":
    # ç‰ˆæœ¬åŒ–å„ªåŒ–ç¤ºä¾‹
    
    # æ–¹å¼1: è‡ªå‹•ç‰ˆæœ¬ï¼ˆæ¨è–¦ï¼‰
    print("ğŸ”¢ è‡ªå‹•ç‰ˆæœ¬å„ªåŒ–...")
    optimizer = ModularOptunaOptimizer("BTCUSDT", "1h")  # è‡ªå‹•ä½¿ç”¨ä¸‹ä¸€å€‹ç‰ˆæœ¬
    results = optimizer.run_complete_optimization()
    
    # æ–¹å¼2: æŒ‡å®šç‰ˆæœ¬
    # optimizer = ModularOptunaOptimizer("BTCUSDT", "1h", version="v1")
    
    # æ–¹å¼3: ç‰ˆæœ¬ç®¡ç†ç¤ºä¾‹
    # vm = ModularOptunaOptimizer.create_version_manager("BTCUSDT", "1h")
    # print(f"å¯ç”¨ç‰ˆæœ¬: {vm.list_versions()}")
    # print(f"æœ€æ–°ç‰ˆæœ¬: {vm.get_latest_version()}")
    
    # è¼‰å…¥ç‰¹å®šç‰ˆæœ¬çš„æ•¸æ“š
    # features, labels, params = vm.load_processed_data_for_training(version="v1")
