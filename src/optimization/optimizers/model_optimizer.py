#!/usr/bin/env python3
"""
æ¨¡å‹è¨“ç·´å™¨ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œå°ˆæ³¨æ–¼æ¨¡å‹è¨“ç·´é‚è¼¯
åŸ Optuna å„ªåŒ–åŠŸèƒ½å·²é·ç§»è‡³ optuna_system/optimizers/optuna_model.py

è² è²¬ï¼š
- LightGBM æ¨¡å‹è¨“ç·´
- æ¨¡å‹é©—è­‰å’Œè©•ä¼°
- æ¨¡å‹ä¿å­˜å’ŒåŠ è¼‰
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import lightgbm as lgb
from typing import Dict, List, Tuple, Any
import warnings
import pickle
import json
from pathlib import Path
warnings.filterwarnings('ignore')

from .config import get_config


class ModelTrainer:
    """æ¨¡å‹è¨“ç·´å™¨ - å°ˆæ³¨æ–¼æ¨¡å‹è¨“ç·´é‚è¼¯"""

    def __init__(self, symbol: str, timeframe: str):
        self.symbol = symbol
        self.timeframe = timeframe
        self.config = get_config("model", timeframe)
        self.base_config = get_config("base")
        
    def train_model(self, X: pd.DataFrame, y: pd.Series, 
                   model_params: Dict[str, Any] = None) -> lgb.LGBMClassifier:
        """
        è¨“ç·´ LightGBM æ¨¡å‹
        
        Args:
            X: ç‰¹å¾µDataFrame
            y: æ¨™ç±¤Series
            model_params: æ¨¡å‹åƒæ•¸å­—å…¸
        
        Returns:
            è¨“ç·´å¥½çš„æ¨¡å‹
        """
        try:
            print(f"ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹ - {self.symbol} {self.timeframe}")
            
            # ä½¿ç”¨æä¾›çš„åƒæ•¸æˆ–é»˜èªé…ç½®
            if model_params is None:
                model_params = self._get_default_params()
            
            print(f"ğŸ“‹ æ¨¡å‹åƒæ•¸: {model_params}")
            
            # å‰µå»ºä¸¦è¨“ç·´æ¨¡å‹
            model = lgb.LGBMClassifier(**model_params)
            model.fit(X, y)
            
            print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
            
            return model
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            raise e

    def train_with_validation(self, X: pd.DataFrame, y: pd.Series,
                            model_params: Dict[str, Any] = None,
                            validation_split: float = 0.2) -> Tuple[lgb.LGBMClassifier, Dict[str, float]]:
        """
        ä½¿ç”¨é©—è­‰é›†è¨“ç·´æ¨¡å‹
        
        Args:
            X: ç‰¹å¾µDataFrame
            y: æ¨™ç±¤Series
            model_params: æ¨¡å‹åƒæ•¸å­—å…¸
            validation_split: é©—è­‰é›†æ¯”ä¾‹
        
        Returns:
            (æ¨¡å‹, é©—è­‰æŒ‡æ¨™å­—å…¸)
        """
        try:
            print(f"ğŸš€ é–‹å§‹å¸¶é©—è­‰çš„æ¨¡å‹è¨“ç·´ - {self.symbol} {self.timeframe}")
            
            # æ™‚é–“åºåˆ—åˆ†å‰²
            split_point = int(len(X) * (1 - validation_split))
            X_train, X_val = X.iloc[:split_point], X.iloc[split_point:]
            y_train, y_val = y.iloc[:split_point], y.iloc[split_point:]
            
            print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²: è¨“ç·´ {len(X_train)}, é©—è­‰ {len(X_val)}")
            
            # ä½¿ç”¨æä¾›çš„åƒæ•¸æˆ–é»˜èªé…ç½®
            if model_params is None:
                model_params = self._get_default_params()
                
                # è¨“ç·´æ¨¡å‹
            model = lgb.LGBMClassifier(**model_params)
            model.fit(X_train, y_train)
            
            # é©—è­‰æ¨¡å‹
            y_pred = model.predict(X_val)
            metrics = self._calculate_metrics(y_val, y_pred)
            
            print(f"ğŸ“ˆ é©—è­‰çµæœ:")
            for metric, value in metrics.items():
                print(f"   {metric}: {value:.4f}")
            
            return model, metrics
                
        except Exception as e:
            print(f"âŒ å¸¶é©—è­‰çš„æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            raise e

    def cross_validate_model(self, X: pd.DataFrame, y: pd.Series,
                           model_params: Dict[str, Any] = None,
                           cv_folds: int = None) -> Dict[str, List[float]]:
        """
        äº¤å‰é©—è­‰æ¨¡å‹
        
        Args:
            X: ç‰¹å¾µDataFrame
            y: æ¨™ç±¤Series
            model_params: æ¨¡å‹åƒæ•¸å­—å…¸
            cv_folds: äº¤å‰é©—è­‰æŠ˜æ•¸
        
        Returns:
            å„é …æŒ‡æ¨™çš„åˆ†æ•¸åˆ—è¡¨å­—å…¸
        """
        try:
            if cv_folds is None:
                cv_folds = self.base_config.get("cv_folds", 5)
            
            print(f"ğŸ”„ äº¤å‰é©—è­‰ ({cv_folds} æŠ˜) - {self.symbol} {self.timeframe}")
            
            # ä½¿ç”¨æä¾›çš„åƒæ•¸æˆ–é»˜èªé…ç½®
            if model_params is None:
                model_params = self._get_default_params()
            
            # æ™‚é–“åºåˆ—äº¤å‰é©—è­‰
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            
            all_metrics = {'f1': [], 'precision': [], 'recall': [], 'accuracy': []}
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
                print(f"ğŸ“‚ ç¬¬ {fold} æŠ˜...")
                
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # æª¢æŸ¥é¡åˆ¥æ•¸é‡
                if len(y_train.unique()) < 2 or len(y_val.unique()) < 2:
                    print(f"   âš ï¸ è·³éï¼šé¡åˆ¥ä¸è¶³")
                    continue
                
                # è¨“ç·´æ¨¡å‹
                model = lgb.LGBMClassifier(**model_params)
                model.fit(X_train, y_train)
                
                # é æ¸¬å’Œè©•ä¼°
                y_pred = model.predict(X_val)
                metrics = self._calculate_metrics(y_val, y_pred)
                
                # è¨˜éŒ„æŒ‡æ¨™
                for metric, value in metrics.items():
                    all_metrics[metric].append(value)
                
                print(f"   F1: {metrics['f1']:.4f}")
            
            # è¨ˆç®—å¹³å‡å€¼
            avg_metrics = {metric: np.mean(scores) for metric, scores in all_metrics.items()}
            
            print(f"ğŸ“Š äº¤å‰é©—è­‰çµæœ:")
            for metric, value in avg_metrics.items():
                print(f"   å¹³å‡ {metric}: {value:.4f} Â± {np.std(all_metrics[metric]):.4f}")
            
            return all_metrics
        
        except Exception as e:
            print(f"âŒ äº¤å‰é©—è­‰å¤±æ•—: {e}")
            return {}

    def save_model(self, model: lgb.LGBMClassifier, save_dir: str,
                  model_params: Dict[str, Any] = None,
                  feature_names: List[str] = None,
                  metrics: Dict[str, float] = None) -> str:
        """
        ä¿å­˜æ¨¡å‹åŠç›¸é—œä¿¡æ¯
        
        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            save_dir: ä¿å­˜ç›®éŒ„
            model_params: æ¨¡å‹åƒæ•¸
            feature_names: ç‰¹å¾µåç¨±åˆ—è¡¨
            metrics: æ¨¡å‹æŒ‡æ¨™
        
        Returns:
            ä¿å­˜è·¯å¾‘
        """
        try:
            save_path = Path(save_dir)
            save_path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            model_file = save_path / "model.pkl"
            with open(model_file, 'wb') as f:
                pickle.dump(model, f)
            
            # ä¿å­˜é…ç½®ä¿¡æ¯
            config_info = {
                'symbol': self.symbol,
                'timeframe': self.timeframe,
                'model_params': model_params or {},
                'feature_names': feature_names or [],
                'metrics': metrics or {},
                'feature_count': len(feature_names) if feature_names else 0
            }
            
            config_file = save_path / "model_config.json"
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ç‰¹å¾µé‡è¦æ€§
            if hasattr(model, 'feature_importances_') and feature_names:
                importance_dict = dict(zip(feature_names, model.feature_importances_))
                importance_file = save_path / "feature_importance.json"
                with open(importance_file, 'w', encoding='utf-8') as f:
                    json.dump(importance_dict, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
            
            return str(save_path)
        
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
            raise e

    def load_model(self, model_path: str) -> Tuple[lgb.LGBMClassifier, Dict[str, Any]]:
        """
        åŠ è¼‰æ¨¡å‹åŠé…ç½®ä¿¡æ¯
        
        Args:
            model_path: æ¨¡å‹ç›®éŒ„è·¯å¾‘
        
        Returns:
            (æ¨¡å‹, é…ç½®ä¿¡æ¯)
        """
        try:
            model_dir = Path(model_path)
            
            # åŠ è¼‰æ¨¡å‹
            model_file = model_dir / "model.pkl"
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            
            # åŠ è¼‰é…ç½®ä¿¡æ¯
            config_file = model_dir / "model_config.json"
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_info = json.load(f)
            else:
                config_info = {}
            
            print(f"âœ… æ¨¡å‹å·²åŠ è¼‰: {model_path}")
            
            return model, config_info
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            raise e

    def _get_default_params(self) -> Dict[str, Any]:
        """ç²å–é»˜èªæ¨¡å‹åƒæ•¸"""
        return {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'num_leaves': 31,
            'min_child_samples': 20,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'random_state': self.base_config.get("random_state", 42),
            'verbose': -1,
            'num_threads': 1
        }

    def _calculate_metrics(self, y_true: pd.Series, y_pred: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        try:
            metrics = {}
            
            # æ ¹æ“šé¡åˆ¥æ•¸é‡é¸æ“‡å¹³å‡æ–¹æ³•
            if len(y_true.unique()) > 2:
                avg_method = 'weighted'
            else:
                avg_method = 'binary'
            
            metrics['f1'] = f1_score(y_true, y_pred, average=avg_method, zero_division=0)
            metrics['precision'] = precision_score(y_true, y_pred, average=avg_method, zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, average=avg_method, zero_division=0)
            metrics['accuracy'] = accuracy_score(y_true, y_pred)
            
            return metrics
            
        except Exception as e:
            print(f"âš ï¸ æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
            return {'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'accuracy': 0.0}

    def generate_report(self, model: lgb.LGBMClassifier, 
                       metrics: Dict[str, float],
                       feature_names: List[str] = None) -> str:
        """ç”Ÿæˆæ¨¡å‹å ±å‘Š"""
        try:
            report = f"""
ğŸ¤– æ¨¡å‹è¨“ç·´å ±å‘Š - {self.symbol} {self.timeframe}
{'='*50}
ğŸ“Š æ¨¡å‹ä¿¡æ¯:
â”œâ”€ æ¨¡å‹é¡å‹: LightGBM
â”œâ”€ ç‰¹å¾µæ•¸é‡: {len(feature_names) if feature_names else 'æœªçŸ¥'}
â””â”€ è¨“ç·´å®Œæˆ

ğŸ“ˆ æ€§èƒ½æŒ‡æ¨™:"""

            for metric, value in metrics.items():
                report += f"\nâ”œâ”€ {metric.upper()}: {value:.4f}"

            if hasattr(model, 'feature_importances_') and feature_names:
                # ç²å–å‰5é‡è¦ç‰¹å¾µ
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                report += f"\n\nğŸ† å‰5é‡è¦ç‰¹å¾µ:"
                for i, (_, row) in enumerate(importance_df.head(5).iterrows(), 1):
                    report += f"\nâ”œâ”€ {i}. {row['feature']:<20} {row['importance']:.4f}"

            return report
                
        except Exception as e:
            return f"æ¨¡å‹å ±å‘Šç”Ÿæˆå¤±æ•—: {e}"


# å‘å¾Œå…¼å®¹æ€§
ModelOptimizer = ModelTrainer  # åˆ¥åï¼Œä¿æŒå‘å¾Œå…¼å®¹