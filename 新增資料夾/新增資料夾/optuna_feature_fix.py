# ä¿®å¾© optuna_feature.py ä¸­çš„é—œéµå•é¡Œ

## 1. ä¿®å¾©é‡æ¡æ¨£æ–¹æ³•
```python
def resample_ohlcv(self, ohlcv: pd.DataFrame, rule) -> pd.DataFrame:
    """ä¿®å¾©ç‰ˆæœ¬ - æ­£ç¢ºæå–é »ç‡å­—ç¬¦ä¸²ä¸¦è™•ç†å¤šæ™‚æ¡†é…ç½®"""
    agg = {
        'open': 'first',
        'high': 'max', 
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }
    
    try:
        # ä¿®å¾©ï¼šæ­£ç¢ºæå–é »ç‡å­—ç¬¦ä¸²
        if isinstance(rule, dict):
            frequency = rule.get('rule', '1H')  # å¾å­—å…¸ä¸­æå–é »ç‡
        else:
            frequency = str(rule)  # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²å‰‡ç›´æ¥ä½¿ç”¨
            
        # é©—è­‰é »ç‡æ ¼å¼
        valid_frequencies = ['15T', '1H', '4H', '1D', '15m', '1h', '4h', '1d']
        if frequency not in valid_frequencies:
            self.logger.warning(f"âš ï¸ ç„¡æ•ˆé »ç‡ {frequency}ï¼Œä½¿ç”¨é»˜èª 1H")
            frequency = '1H'
            
        resampled = ohlcv.resample(frequency).agg(agg)
        self.logger.info(f"âœ… æˆåŠŸé‡æ¡æ¨£åˆ° {frequency}: {resampled.shape}")
        return resampled.dropna()
        
    except Exception as e:
        self.logger.warning(f"âš ï¸ é‡æ¡æ¨£å¤±æ•— rule={rule}: {e}")
        return pd.DataFrame(columns=ohlcv.columns)
```

## 2. ä¿®å¾©å¤šæ™‚æ¡†ç‰¹å¾µç”Ÿæˆé‚è¼¯
```python
def generate_technical_features(self, ohlcv_data: pd.DataFrame, params: Dict) -> pd.DataFrame:
    """ä¿®å¾©ç‰ˆæœ¬ - æ­£ç¢ºè™•ç†å¤šæ™‚æ¡†ç‰¹å¾µç”Ÿæˆ"""
    flags_tech = self.flags.get('tech', {})
    base_features = self._calc_base_indicators(ohlcv_data, self.timeframe, flags_tech)
    features = base_features.copy()
    
    # å¤šæ™‚æ¡†é…ç½®
    mtf_cfg = flags_tech.get('multi_timeframes', {})
    if isinstance(mtf_cfg, dict) and mtf_cfg.get('enabled'):
        rules = mtf_cfg.get('rules', {})
        tf_list = mtf_cfg.get('timeframes', [])
        
        for tf_key in tf_list:
            rule_config = rules.get(tf_key)
            if not rule_config:
                continue
                
            # ä¿®å¾©ï¼šæ­£ç¢ºæå–é »ç‡å­—ç¬¦ä¸² 
            frequency = rule_config.get('rule') if isinstance(rule_config, dict) else str(rule_config)
            
            resampled = self.resample_ohlcv(ohlcv_data, frequency)  # å‚³éå­—ç¬¦ä¸²è€Œä¸æ˜¯å­—å…¸
            if resampled.empty:
                continue
                
            # ä½¿ç”¨ç‰¹å®šæ™‚æ¡†çš„åƒæ•¸ç”ŸæˆæŒ‡æ¨™
            tf_features = self._calc_base_indicators(
                resampled, tf_key, flags_tech, 
                base_index=ohlcv_data.index,
                tf_overrides=rule_config  # å‚³éæ™‚æ¡†ç‰¹å®šé…ç½®
            )
            
            features = self.safe_merge(features, tf_features, prefix=f'{tf_key}_')
            
    return features
```

## 3. èª¿æ•´ Layer1 è¯å‹•åƒæ•¸ç¯„åœ
```python
def objective(self, trial) -> float:
    """ä¿®å¾©ç‰ˆæœ¬ - æ“´å¤§ Layer1 è¯å‹•æœç´¢ç¯„åœ"""
    # ... å‰é¢ä»£ç¢¼ä¸è®Š ...
    
    # ä¿®å¾©ï¼šæ“´å¤§ Layer1 è¯å‹•æœç´¢ç¯„åœ
    if layer1_params:
        layer1_lag = layer1_params.get('lag', 15)
        layer1_buy_q = layer1_params.get('buy_quantile', 0.7)
        layer1_sell_q = layer1_params.get('sell_quantile', 0.3)
        
        # æ“´å¤§æœç´¢ç¯„åœï¼Œå¢åŠ éˆæ´»æ€§
        lag_range = max(3, layer1_lag - 5), min(30, layer1_lag + 5)  # å¾ Â±3 æ“´å¤§åˆ° Â±5
        profit_q_range = max(0.55, layer1_buy_q - 0.1), min(0.95, layer1_buy_q + 0.1)  # å¾ Â±0.05 æ“´å¤§åˆ° Â±0.1
        loss_q_range = max(0.05, layer1_sell_q - 0.1), min(0.45, layer1_sell_q + 0.1)
        
        lag = trial.suggest_int('lag', *lag_range)
        profit_quantile = trial.suggest_float('profit_quantile', *profit_q_range)
        loss_quantile = trial.suggest_float('loss_quantile', *loss_q_range)
        
        self.logger.info(f"ğŸ”— Layer1è¯å‹•æ“´å¤§ç¯„åœ: lag={layer1_lag} â†’ æœç´¢ç¯„åœ{lag_range}")
        self.logger.info(f"ğŸ”— Layer1è¯å‹•æ“´å¤§ç¯„åœ: buy_q={layer1_buy_q:.3f} â†’ profit_q{profit_q_range}")
        self.logger.info(f"ğŸ”— Layer1è¯å‹•æ“´å¤§ç¯„åœ: sell_q={layer1_sell_q:.3f} â†’ loss_q{loss_q_range}")
    else:
        # é»˜èªç¯„åœä¿æŒä¸è®Š
        lag = trial.suggest_int('lag', 3, 20)
        profit_quantile = trial.suggest_float('profit_quantile', 0.6, 0.9)
        loss_quantile = trial.suggest_float('loss_quantile', 0.1, 0.4)
    
    # ... å¾ŒçºŒä»£ç¢¼ä¸è®Š ...
```

## 4. å¢åŠ ç‰¹å¾µè³ªé‡éæ¿¾
```python
def nested_cv_evaluation(self, X: pd.DataFrame, y: pd.Series, coarse_k: int, fine_k: int) -> Dict:
    """ä¿®å¾©ç‰ˆæœ¬ - å¢åŠ ç‰¹å¾µè³ªé‡éæ¿¾"""
    # ... å‰é¢ä»£ç¢¼ä¸è®Š ...
    
    # åœ¨ç‰¹å¾µé¸æ“‡å‰å¢åŠ è³ªé‡éæ¿¾
    X_filtered = self._filter_low_quality_features(X)
    self.logger.info(f"ğŸ”§ ç‰¹å¾µè³ªé‡éæ¿¾: {X.shape[1]} â†’ {X_filtered.shape[1]}")
    
    # å¾ŒçºŒä½¿ç”¨ X_filtered é€²è¡Œç‰¹å¾µé¸æ“‡
    # ... å‰©é¤˜ä»£ç¢¼ä½¿ç”¨ X_filtered æ›¿ä»£ X
    
def _filter_low_quality_features(self, X: pd.DataFrame) -> pd.DataFrame:
    """éæ¿¾ä½è³ªé‡ç‰¹å¾µ"""
    # 1. ç§»é™¤å¸¸é‡ç‰¹å¾µ
    constant_features = X.columns[X.nunique() <= 1]
    X = X.drop(columns=constant_features)
    
    # 2. ç§»é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾µ (>50%)
    missing_rate = X.isnull().mean()
    high_missing_features = missing_rate[missing_rate > 0.5].index
    X = X.drop(columns=high_missing_features)
    
    # 3. ç§»é™¤é«˜ç›¸é—œæ€§ç‰¹å¾µ (>0.95)
    corr_matrix = X.corr().abs()
    upper_triangle = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    high_corr_features = [
        column for column in upper_triangle.columns 
        if any(upper_triangle[column] > 0.95)
    ]
    X = X.drop(columns=high_corr_features)
    
    self.logger.info(f"ğŸ“Š éæ¿¾çµ±è¨ˆ: å¸¸é‡={len(constant_features)}, é«˜ç¼ºå¤±={len(high_missing_features)}, é«˜ç›¸é—œ={len(high_corr_features)}")
    
    return X
```

## 5. å„ªåŒ–æ¨™ç±¤å¹³è¡¡
```python
def generate_rolling_labels(self, returns: pd.Series, lag: int, profit_quantile: float, 
                          loss_quantile: float, lookback_window: int) -> pd.Series:
    """ä¿®å¾©ç‰ˆæœ¬ - å„ªåŒ–æ¨™ç±¤å¹³è¡¡"""
    # ... å‰é¢ä»£ç¢¼ä¸è®Š ...
    
    # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒä¸¦èª¿æ•´
    label_counts = labels.value_counts().sort_index()
    total = len(labels.dropna())
    
    # è¨ˆç®—é¡åˆ¥æ¯”ä¾‹
    if len(label_counts) == 3:
        ratios = [count/total for count in label_counts.values]
        min_ratio = min(ratios)
        
        # å¦‚æœæœ€å°é¡åˆ¥æ¯”ä¾‹ < 25%ï¼Œèª¿æ•´ quantile
        if min_ratio < 0.25:
            self.logger.warning(f"âš ï¸ æ¨™ç±¤ä¸å¹³è¡¡åš´é‡: {dict(label_counts)}, æœ€å°æ¯”ä¾‹={min_ratio:.3f}")
            
            # å‹•æ…‹èª¿æ•´ quantile ä»¥æ”¹å–„å¹³è¡¡
            if ratios[2] < 0.25:  # é¡åˆ¥2åå°‘ï¼Œé™ä½ profit_quantile
                profit_quantile = max(0.65, profit_quantile - 0.05)
            if ratios[0] < 0.25:  # é¡åˆ¥0åå°‘ï¼Œæé«˜ loss_quantile  
                loss_quantile = min(0.35, loss_quantile + 0.05)
            
            self.logger.info(f"ğŸ“Š èª¿æ•´å¾Œ quantile: profit={profit_quantile:.3f}, loss={loss_quantile:.3f}")
            
            # é‡æ–°ç”Ÿæˆæ¨™ç±¤
            return self.generate_rolling_labels(returns, lag, profit_quantile, loss_quantile, lookback_window)
    
    self.logger.info(f"ğŸ“Š æ¨™ç±¤åˆ†å¸ƒ: {dict(label_counts)} (ç¸½è¨ˆ{total})")
    return labels.dropna().astype(int)
```

## ä¿®å¾©ç¸½çµ
1. **é‡æ¡æ¨£å•é¡Œ**ï¼šä¿®å¾©é »ç‡å­—ç¬¦ä¸²æå–é‚è¼¯
2. **Layer1 è¯å‹•**ï¼šæ“´å¤§æœç´¢ç¯„åœï¼Œå¢åŠ éˆæ´»æ€§
3. **ç‰¹å¾µè³ªé‡**ï¼šå¢åŠ éæ¿¾æ©Ÿåˆ¶ï¼Œæå‡ç‰¹å¾µæœ‰æ•ˆæ€§
4. **æ¨™ç±¤å¹³è¡¡**ï¼šå‹•æ…‹èª¿æ•´ quantileï¼Œæ”¹å–„é¡åˆ¥åˆ†å¸ƒ
5. **éŒ¯èª¤è™•ç†**ï¼šå¢å¼·ç•°å¸¸è™•ç†å’Œæ—¥èªŒè¨˜éŒ„

é€™äº›ä¿®å¾©æ‡‰è©²èƒ½é¡¯è‘—æ”¹å–„ Layer2 çš„å„ªåŒ–æ•ˆæœï¼Œé æœŸ F1 åˆ†æ•¸å¾ 0.24 æå‡åˆ° 0.4+ ã€‚